\documentclass[12pt, a4paper]{memoir}
%\documentclass[12pt, a4paper, oldfontcommands]{memoir}
%\documentclass[12pt, a4paper]{report}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{graphicx}
\usepackage[outdir=./]{epstopdf}
\usepackage[round]{natbib} 
\usepackage{url}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{hyphenat}
\usepackage{pdfpages}

\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{2}

\let\footruleskip\undefined
\usepackage{fancyhdr}
\fancypagestyle{plain}{%
\fancyhf{}% clears all header and footer fields
\fancyhead[LE,RO]{\thepage}%
\renewcommand{\headrulewidth}{0pt}%
\renewcommand{\footrulewidth}{0pt}}

\DeclareMathOperator{\expfamily}{ExpFamily}
\DeclareMathOperator{\expectation}{\mathbb{E}}
\DeclareMathOperator{\variance}{\mathbb{V}ar}
\DeclareMathOperator{\cov}{\mathbb{C}ov}
\DeclareMathOperator{\corr}{\mathbb{C}orr}
\DeclareMathOperator{\bernoulli}{Bernoulli}
\DeclareMathOperator{\betaDist}{Beta}
\DeclareMathOperator{\dirichlet}{Dir}
\DeclareMathOperator{\bin}{Bin}
\DeclareMathOperator{\MN}{Multinomial}
\DeclareMathOperator{\prob}{\mathbb{P}}
\DeclareMathOperator{\trace}{Tr}
\DeclareMathOperator{\normal}{N}
\DeclareMathOperator{\gammaDist}{Gamma}
\DeclareMathOperator{\poisson}{Poisson}
\DeclareMathOperator{\CPoisson}{CP\Gamma}
\DeclareMathOperator{\normDist}{N}

\newcommand{\RSS}{\mathrm{RSS}}
\newcommand{\euler}{\mathrm{e}}
\newcommand{\diff}{\mathrm{d}}
\newcommand{\T}{^\textup{T}}
\newcommand{\dotdotdot}{_{\phantom{.}\cdots}}
\newcommand{\BIC}{\textup{BIC}}

\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vectGreek}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\mathsf{#1}}

\begin{document}\sloppy

\includepdf{title.pdf}

\frontmatter
\section*{Abstract}
X-ray CT can be used for defect detection and quality control of 3D printing. This report will outline statistical methods for comparing x-ray scans of a 3D printed sample with its computer model under the face of uncertainty. Shading correction methods and the mean and variance relationship were investigated. Lastly the compound Poisson was reviewed.

\newpage

\section*{Acknowledgements}
\begin{itemize}
	\item Supervisors: Julia Brettschneider and Tom Nichols
	\item Inside Out Team: Wilfrid Kendall, Audrey Kueh, Jay Warnett and Clair Barnes
	\item EPSRC Funding: EP/L016710/1
\end{itemize}

\newpage
\tableofcontents*

\mainmatter

\chapter{Introduction}


\chapter{Literature Review}

3D printing, formally additive manufacturing \citep{gibson2010additive} \citep{wong2012review}, has recently become state of the art technology which manufactures objects of complicated shapes with very high precision. It was developed in the 1980's \citep{kodama1981automatic} but recently it has been commercialised and is used in medical science \citep{kang20163d} and engineering \citep{wong2012review}. Because of such a wide range of applications, there has been a need for quality control and this can be done by scanning the 3D printed sample using x-ray computed tomography.

\section{X-ray Imaging}

\subsection{X-Ray Production}
Photons in CT scanning are produced in an X-ray tube. In an X-ray tube, a cathode, consisting of a heated filament, fires projectile electrons through an electric potential to a target which forms the anode \citep{michael2001x}, as shown in Figure \ref{fig:x_ray_tube}. Most of the kinetic energy of the projectile electrons is converted into heat however some is converted into electromagnetic radiation. This depends on how the projectile electrons interact with the atoms in the anode \citep{cantatore2011introduction}.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figures/x_ray_tube.png}
\caption{An X-ray tube produces photons by firing projectile electrons from a cathode to an anode. \emph{Source: G.~Michael (2001) \citep{michael2001x}}}
\label{fig:x_ray_tube}
\end{figure}

Bremsstrahlung radiation is the result of projectile electrons deaccelerating due to the electrostatic field produced by nucleus of the target. The kinetic energy of the projectile electrons is then converted to electromagnetic radiation to produce X-ray radiation. As a result, the photon energies in bremsstrahlung radiation is  a continuous spectrum and can range up to the maximum kinetic energy of the projectile electrons \citep{michael2001x}.

Characteristic radiation is due to projectile electrons colliding with electrons in the target atom and ionizing them. This produce vacancies in the electron shell and emits photons when the electrons in the target atom drops down back to the ground state. The energy of the emitted radiation is monoenergetic and depends on the binding energy of the target's atoms \citep{michael2001x}.

A typical energy spectrum of photons emitted from an X-ray tube is as shown in Figure \ref{fig:x_ray_spectrum}. The energy spectrum consist of both bremsstrahlung and characteristic radiation \citep{michael2001x}.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figures/x_ray_spectrum.png}
\caption{A typical energy spectrum of photons emitted from an X-ray tube. The continuous spectrum is the result of Bremmsstrahlung radiation. The peaks are the result of characteristic radiation. \emph{Source: G.~Michael (2001) \citep{michael2001x}}}
\label{fig:x_ray_spectrum}
\end{figure}

The voltage and current can be varied in the X-ray tube to produce different energy spectrums and rate of photon production. This can vary the results produced when collecting CT data \citep{cantatore2011introduction}. Another important factor is the focal spot size because smaller spot sizes produce sharper edges. Larger spot sizes produce unsharp results and this is know as the penumbra effect, as shown in Figure \ref{fig:x_ray_penumbra}. However spot sizes too small can produce concentrated heat \citep{welkenhuyzen2009industrial} and can damage the X-ray tube.

\begin{figure}
\centering
\includegraphics[width=1\textwidth]{figures/x_ray_penumbra.png}
\caption{Larger focal spot sizes produces unsharp results. This is know as the penumbra effect. \emph{Source: F.~Welkenhuyzen et al.~(2009)\citep{welkenhuyzen2009industrial}}}
\label{fig:x_ray_penumbra}
\end{figure}

\subsection{Photon Interactions}
Photons emitted by the X-ray tube are projected onto the sample and interacts with it in a number of ways \citep{cantatore2011introduction}.

The sample can effectively absorb photons via the photoelectric effect or pair production \citep{cantatore2011introduction}. In the photoelectric effect, the photons transfers all its energy to a bounded electron and ejects it from the sample's atom \citep{millikan1916direct}. In pair production, the photons convert into electron-position pairs by interacting with the Coulomb field of the sample's atomic nucleus \citep{hubbell2006electron}. In addition to getting absorbed, photons can be scattered by the sample. This happens when photons collide inelastically with and transfers its energy to the sample's electrons. This process is known as Compton scattering \citep{compton1923quantum}.

Suppose a mono-energetic X-ray pencil beam attenuating through an object with varying attenuation coefficient in position $\mu=\mu(x)$. The beam starts at $x=0$ and is detected at $x=L$, then the attenuation (decrease in X-ray intensity from $I_0$ to $I_1$) is given as \citep{cantatore2011introduction}
\begin{equation}
I_1 = I_0\exp\left[\int_0^L-\mu(x)\diff x\right].
\label{eq:beerLaw}
\end{equation}
By comparing the intensity before and after attenuation, the integral of the attenuation coefficient along the path of the X-ray can be calculated. Because of the discrete nature of pixels, the integral is usually replaced by a sum \citep{michael2001x}. 

However it was shown that the attenuation coefficient does depend on the energy of the photons \citep{elbakri2002statistical}. Thus $\mu=\mu(x,E)$ should be made dependent on the energy of the photons \citep{cantatore2011introduction}. In general low energy photons are more likely to be absorbed than high energy photons, this increases the average energy of the attenuated photons and can be a source of error in CT scanning. This is referred to beam hardening and can cause inaccuracies in Equation \eqref{eq:beerLaw} \citep{michael2001x}. This can be reduced by placing a thin sheet of filter to absorb low energy photons \citep{welkenhuyzen2009industrial} or by correcting it in the data analysis stage \citep{michael2001x}.

\subsection{Detection and Reconstruction}
Most X-ray detectors are scintillator-photodiode detectors. The photons interact with the scintiallator material and produce visible light. The visible light is then detected by photodiodes and coverts it into electricical current \citep{michael2001x}. The detectors used in CT scanning are flat bed scanner which consist of an array of panels of photodiodes \citep{cantatore2011introduction}.

\section{X-ray Computed Tomography}
Computed tomography (CT) scanning is a 3D imaging technique. It does this by reconstructing the geometry of the sample through a series of 2D X-ray images of the sample. The sample rotates after each image taken \citep{cantatore2011introduction}.

Figure \ref{fig:x_ray_ct} shows a diagram on how CT scanning works. A 2D image is taken by projecting X-ray photons onto the stationary sample. The photons are then scattered or absobred by the sample.  Some of these photons are then detected by an X-ray detector on the other side of the sample, which produces an image. After an image has been taken, the object rotates and another image is taken. Finally after a number of images, a 3D reconstruction of the object can be estimated \citep{cantatore2011introduction}.

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{figures/x_ray_ct.png}
\caption{X-ray computed tomography reconstructs the sample by projecting photons onto a rotating sample. The photons are then detected by the detector. \emph{Source: \url{http://www.phoenix-xray.com/}}}
\label{fig:x_ray_ct}
\end{figure}

CT scanning was invented by G. Hounsfield \citep{hounsfield1980computed} in the 1980's and it was mainly used for medical imaging. The setup for CT scanning is different when scanning patients because the detector and X-ray source rotates around the patient \citep{cantatore2011introduction}. Recently CT has been used industrially for non-destructive testing in manufacturing \citep{cantatore2011introduction}. One possible application would be inspecting 3D printed samples \citep{villarraga2015assessing}. Other uses in science include the investigation of batteries \citep{o2017investigating} and materials \citep{wang2017x} \citep{zhang2016x}.

Methods used to reconstruct the 3D surface of the sample are available such as filtered back-projection \citep{brooks1976principles} and the FDK algorithm \citep{feldkamp1984practical}. There exist software such as \emph{VGStudio MAX} \citep{reinhart2008industrial} which automatically aligns and scales the 3D reconstruction with the computed aided design and compares them.

Studies have been done to use x-ray CT to do defection on 3D printing \citep{kim2016inspection} \citep{villarraga2015assessing}. This was done by investigating the variation of the material thickness and the distribution of the volume of voids \citep{villarraga2015assessing}. Voids then can be classified as defects if the voids are larger than some volume threshold. This threshold controls the probability of the detection of defects \citep{amrhein2014characterization} \citep{gandossi2010probability}.

Full 3D x-ray CT scans can be slow and may not generalised well in production lines. However there is potential to bring in methods such as those used in airport luggage inspection to speed up to process \citep{warnett2016towards}.

There are many sources of error in CT scanning \citep{cantatore2011introduction} and this can cause problems when reconstructing the geometry of the sample. Sources of error include: dead pixels in the detector \citep{brettschneider2014spatial}, use of cone beams \citep{sun2016applications}, beam hardening and the orientation of the sample \citep{corcoran2016observations}.

The scale of the 2D images can be calibrated with the use of reference standards \citep{bartscher2007enhancement} \citep{lifton2013application}.

\section{Compound Poisson}

The grey values of each pixel in the detector can be modelled as a random variable, due to the random behaviour of photons being produced, interacting with the phantom and interacting with the scintillator in the detector. When the photons interact with the scintillator, they are converted into visible light. The visible light photons are then detected and converted into a grey value. How the detected visible light is converted is important in modelling the grey value.

Should the grey value be proportional to the number of photons detected, the detector is known as a quantum counter and the grey value is a Poisson random variable, up to a constant \citep{whiting2006properties}. However if the grey value records the signal strength, proportional to the energy detected, then the detector is an energy integrating detector. The grey value has the compound Poisson distribution, again up to a constant \citep{whiting2006properties}. There are other types of detection schemes \citep{whiting2006properties} but it shall be not be considered here.

Many reconstruction algorithms have been proposed considering the compound Poisson nature of the detector \citep{elbakri2002statistical} \citep{elbakri2003statistical} \citep{lasio2007statistical} \citep{xie2008x} and with beam hardening correction \citep{elbakri2003efficient}. These will not be discussed here as they are beyond the scope of this thesis.

Experiments have been done to verify the compound Poisson nature of the detector. This was done by investigating the variance of radiographs of air \citep{hsieh2015compound} and a polyethylene cylinder \citep{yang2009evaluation} \citep{yang2010noise} at different voltages and powers. It was found there were 2 components in the noise, one was signal dependent and comes from the compound Poisson, the other was signal independent and may be electronic noise. The electronic noise can be modelled as Normally distributed \citep{xu2009electronic}.

The compound Poisson can be formally written. Let $Y$ be the number of photons detected in a pixel for some time exposure $\tau$,
\begin{equation*}
Y\sim\poisson(\lambda)
\end{equation*}
and
\begin{equation*}
\prob(Y=y)=\euler^{-\lambda}\frac{\lambda^y}{y!} \quad \text{for }y=1,2,3,\dotdotdot
\end{equation*}
where $\lambda>0$ is the Poisson rate parameter. Let $U_i$ be the energy of the $i$th photon in a pixel for i.i.d.~$i=1,2,3,\dotdotdot$ with p.d.f.~$p_U(u)$. Let $X$ be the greyvalue of a pixel and is an energy integrating detector, then
\begin{equation}
X|Y = \sum_{i=1}^{Y}U_i+\kappa
\label{eq:compoundPoisson_X|Y}
\end{equation}
where $\kappa$ is some offset. For now let $\kappa=0$, then the marginal $X$ has the compound Poisson distribution. The p.d.f.~of $X$ can be obtained by marginalising the joint p.d.f.
\begin{equation*}
p_X(x)=\sum_{y=0}^\infty p_{X|Y}(x|y)\prob(Y=y) \quad\text{for }x\geqslant 0\ .
\end{equation*}
However it should be noted that $X=0$ if and only if $Y=0$ with probability
\begin{equation*}
\prob(X=0)=\euler^{-\lambda} \ .
\end{equation*}
Formally $X$ has probability mass at $X=0$ and probability density at $X>0$, then 
\begin{equation}
p_X(x) = 
\begin{cases}
\delta(x) \euler^{-\lambda}  & \text{ for } x=0 \\ 
\sum_{y=1}^\infty p_{X|Y}(x|y)\euler^{-\lambda}\frac{\lambda^y}{y!} \quad\text{for } & \text{ for } x>0
\end{cases}
\end{equation}
where $\delta(x)$ is the Dirac delta function. It is quite often the case this cannot be written in closed form.

The m.g.f.~of $X$ has a nicer form however. Let the m.g.f.~of $X$ be
\begin{equation*}
M_X(\theta)=\expectation\left[\euler^{X\theta}\right] \ .
\end{equation*}
This can be computed using the result for conditional expectations
\begin{equation*}
M_X(\theta)=\expectation\expectation\left[\euler^{X\theta}|Y\right] \ .
\end{equation*}
Using the definition of $X|Y$ in Equation \eqref{eq:compoundPoisson_X|Y}, then
\begin{equation*}
M_X(\theta)=\expectation\expectation\left[\exp\left(\theta U_1 + \theta U_2 + \dotdotdot + \theta U_Y\right)|Y\right]
\end{equation*}
\begin{equation*}
M_X(\theta)=\expectation\expectation\left[\euler^{\theta U_1}\cdot\euler^{\theta U_2}\cdot\dotdotdot\cdot\euler^{\theta U_Y}|Y\right] \ .
\end{equation*}
But because $U_i$ for $i=1,2,3,\dotdotdot$ are i.i.d., then each $U_i$ has a common m.g.f.
\begin{equation*}
M_U(\theta)=\expectation\left[\euler^{U\theta}\right] \ ,
\end{equation*}
then
\begin{equation*}
M_X(\theta)=\expectation\left(
\expectation\left[\euler^{\theta U_1}|Y\right]\cdot
\expectation\left[\euler^{\theta U_2}|Y\right]\cdot
\dotdotdot \cdot
\expectation\left[\euler^{\theta U_Y}|Y\right]
\right)
\end{equation*}
\begin{equation*}
M_X(\theta)=\expectation\left[\left(M_U(\theta)\right)^Y\right]
\end{equation*}
\begin{equation*}
M_X(\theta)=\expectation\left[\euler^{Y\ln(M_U(\theta))}\right]
\end{equation*}
\begin{equation*}
M_X(\theta) = M_Y\left(\ln(M_U(\theta)\right)
\end{equation*}
where $M_Y(\theta)$ is the m.g.f.~of $Y$. It can be shown that the m.g.f.~of $Y$ is
\begin{equation}
M_Y(\theta)=\expectation\left[\euler^{Y\theta}\right]
=\exp\left[
\lambda\left(
\euler^\theta-1
\right)
\right]
\end{equation}
then
\begin{equation}
M_X(\theta)
=
\exp\left[
\lambda\left(
M_U(\theta)-1
\right)
\right] \ .
\end{equation}
As long as the m.g.f.~of $U$ is known, then m.g.f.~of $X$ can be written in closed form.

Moments of $X$ can be obtained from the m.g.f.~by differentiating it and setting it to zero
\begin{align*}
M_X'(\theta)&=\exp\left[\lambda\left(M_U(\theta)-1\right)\right]\cdot\lambda M_U'(\theta) \\
M_X'(\theta)&=M_X(\theta)\lambda M_U'(\theta)
\end{align*}
then
\begin{equation}
\expectation\left[X\right]=\lambda\expectation\left[U\right] \ .
\end{equation}
Conducting the same procedure
\begin{equation*}
M_X''(\theta)=M_X'(\theta)\lambda M_U'(\theta)+M_X(\theta)\lambda M_U''(\theta) \ ,
\end{equation*}
the variance can be obtained
\begin{equation*}
\variance\left[X\right]=M_X''(0)-\left[M_X'(0)\right]^2
\end{equation*}
\begin{equation*}
\variance\left[X\right]=M_X'(0)\lambda M_U'(0)+M_X(0)\lambda M_U''(0)-\left[\lambda\expectation\left[U\right]\right]^2
\end{equation*}
\begin{equation*}
\variance\left[X\right]=\lambda^2 \left(\expectation\left[U\right]\right)^2+\lambda \expectation\left[U^2\right]-\left[\lambda\expectation\left[U\right]\right]^2
\end{equation*}
resulting in
\begin{equation}
\variance\left[X\right] = \lambda\expectation\left[U^2\right] \ .
\end{equation}
This can be extended for higher moments.

Because the m.g.f.~can be written down in closed form, so is the characterisation function or Fourier transform of the p.d.f.~of $X$. By obtaining an empirical version of $p_U(u)$, moments of $X$ can be estimated using the m.g.f.~and the p.d.f.~can be estimated by using fast Fourier transform on the empirical characteristic function \citep{whiting2006properties}.

If the distribution of $U$ cannot be obtained or estimated, then one can guess that distribution. \cite{xu2009electronic} for example used a Gamma distribution
\begin{equation}
U\sim\gammaDist\left(\alpha,\beta\right)
\end{equation}
where $\alpha>0$ is the Gamma shape parameter and $\beta>0$ is the Gamma rate parameter. This is a special case of the compound Poisson called the compound Poisson-Gamma model and one can write
\begin{equation}
X\sim\CPoisson(\lambda,\alpha,\beta) \ .
\end{equation}
The conditional distribution can be shown to be
\begin{equation}
X|Y\sim\gammaDist\left(Y\alpha,\lambda\right) \ .
\end{equation}

Using results derived earlier, the m.g.f.~of the of $X$ is
\begin{equation}
M(\theta)=\exp\left[\lambda\left(\left(\frac{\beta}{\beta-\theta}\right)^{\alpha}-1\right)\right]
\end{equation}
and moments can be obtained from it such as
\begin{equation}
\expectation\left[X\right]=\frac{\alpha\lambda}{\beta}
\end{equation}
\begin{equation}
\variance\left[X\right]=\frac{\alpha(\alpha+1)\lambda}{\beta^2}
\label{eq:compoundPoisson_variance}
\end{equation}
and
\begin{equation}
\expectation\left[(X-\expectation[X])^3\right] = \frac{\alpha(\alpha+1)(\alpha+2)\lambda}{\beta^3} \ .
\end{equation}

The p.d.f.~can be shown to be
\begin{equation}
p_X(x) = 
\begin{cases}
\delta(x) \euler^{-\lambda} & \text{ for } x=x \\ 
\sum_{y=1}^{\infty}\frac{\beta^{y\alpha}}{\Gamma(y\alpha)}x^{y\alpha-1}\euler^{-\beta x}\euler^{-\lambda}\frac{\lambda^y}{y!} & \text{ for } x>0
\end{cases}
\label{eq:compoundPoisson_pdf}
\end{equation}
and it is well known the infinite sum cannot be written in closed form. There are a number of approximations or computational methods to evaluate the p.d.f.~such as Fourier inverting the characteristic function \citep{dunn2008evaluation}, using the saddlepoint approximation \citep{daniels1954saddlepoint} or cleverly sum over certain terms in the infinite sum \citep{dunn2005series}. Of course direction simulation of a compound Poisson-Gamma random variable is incredibly easy if $U$ can be simulated. The p.d.f.~then can be estimated using these simulated samples.

For a given i.i.d.~random sample of a compound Poisson-Gamma random variable, parameter estimation can be difficult. Maximum likelihood estimation is very complicated \citep{withers2011compound}. It can be made simpler by fixing $\alpha$ to be known \citep{withers2011compound} but this is not useful in this context.

Because the moments of a compound Poisson-Gamma random variable can be obtained easily, method of moments estimators can be obtained. Suppose $\widehat{\mu}$ is an estimator of $\expectation[X]$ and $\widehat{\mu}_j$ is an estimator of $\expectation\left[\left(X-\expectation[X]\right)^j\right]$ for $j=2,3$. Then the estimators
\begin{equation}
\widehat{\lambda}=\frac{\widehat{\mu}^2\widehat{\mu}_2}{\left(2\widehat{\mu}_2^2-\widehat{\mu}_3\widehat{\mu}\right)}
\end{equation}
\begin{equation}
\widehat{\alpha}=\frac{\widehat{\mu}_3\widehat{\mu}-2\widehat{\mu}_2^2}{\widehat{\mu}_2^2-\widehat{\mu}\widehat{\mu}_3}
\end{equation}
\begin{equation}
\widehat{\beta}=\frac{\widehat{\mu}\widehat{\mu}_2}{\widehat{\mu}\widehat{\mu}_3-\widehat{\mu}_2^2}
\end{equation}
are method of moments estimators of $\lambda$, $\alpha$ and $\beta$ respectively \citep{withers2011compound}. These estimators suffer because estimation is not done through the sufficient statistics and can be negative, this is a problem because the parameters do not take non-positive values.

However, parameter estimation can be made easier once it can be realised that the compound Poisson-Gamma distribution is in the exponential family for fixed $\alpha$ \citep{jorgensen1987exponential}. To show this, it is particular useful to reparametrize the compound Poisson-Gamma distribution using the following:
\begin{equation}
p=\frac{2+\alpha}{1+\alpha} \ ,
\end{equation}
\begin{equation}
\mu=\frac{\lambda\alpha}{\beta} \ ,
\end{equation}
\begin{equation}
\phi = \frac{\alpha+1}{\beta^{2-p}(\lambda\alpha)^{p-1}} \ .
\end{equation}
The parameters $p$, $\mu$ and $\phi$ are called the index, mean and dispersion parameters respectively. It can be shown that $1<p<2$, $\mu>0$ and $\phi>0$.

By rearranging the parameters to get
\begin{equation}
\lambda=\frac{\mu^{2-p}}{\phi(2-p)}
\end{equation}
\begin{equation}
\alpha=\frac{2-p}{p-1}
\end{equation}
\begin{equation}
\beta=\frac{1}{\phi(p-1)\mu^{p-1}}
\end{equation}
and substituting it into Equation \eqref{eq:compoundPoisson_pdf}, the p.m.f.~at zero can be shown to be
\begin{equation}
\prob(X=0) = \exp\left[
-\frac{\mu^{2-p}}{\phi(2-p)}
\right]
\end{equation}
and the p.d.f.~for $x>0$ is
\begin{multline*}
p_X(x) = \sum_{y=1}^{\infty}
\left[\frac{1}{\phi(p-1)\mu^{p-1}}\right]^{y\alpha}
\frac{1}{\Gamma(y\alpha)}
x^{y\alpha-1}
\exp\left[-\frac{x}{\phi(p-1)\mu^{p-1}}\right]
\\
\exp\left[-\frac{\mu^{2-p}}{\phi(2-p)}\right]
\left[\frac{\mu^{2-p}}{\phi(2-p)}\right]^y
\frac{1}{y!} \ .
\end{multline*}
Tidying up the equation
\begin{multline*}
p_X(x) = 
\exp\left[\frac{1}{\phi}\left(x\frac{\mu^{1-p}}{1-p}-\frac{\mu^{2-p}}{2-p}\right)\right]\frac{1}{x}
\\
\sum_{y=1}^{\infty}\frac{x^{y\alpha}\mu^{y[2-p-\alpha(p-1)]}}{\phi^{y(1+\alpha)}(p-1)^{y\alpha}(2-p)^yy!\Gamma(y\alpha)} \ .
\end{multline*}
To simplify further, it should be noted that
\begin{align*}
2-p-\alpha(p-1) &= 2-p - \frac{2-p}{p-1}(p-1)
\\&=0
\end{align*}
so that
\begin{equation}
p_X(x) = 
\exp\left[\frac{1}{\phi}\left(x\frac{\mu^{1-p}}{1-p}-\frac{\mu^{2-p}}{2-p}\right)\right]\frac{1}{x}
\sum_{y=1}^{\infty}W_y(x,p,\phi)
\end{equation}
where
\begin{equation}
W_y(x,p,\phi)=\frac{x^{y\alpha}}{\phi^{y(1+\alpha)}(p-1)^{y\alpha}(2-p)^yy!\Gamma(y\alpha)} \ .
\end{equation}

Those familiar with generalised linear models \citep{nelder1972generalized} \citep{mccullagh1984generalized} will notice that for fixed $p$ or $\alpha$, the above is in the form of a distribution in the dispersive exponential family. Parameter estimation then can be done via the generalised linear model framework and can be extended to include linear mixed models \citep{zhang2013likelihood}. These has applications in for example insurance claim data \citep{jorgensen1994fitting} \citep{smyth2002fitting}.

Estimating $p$ is difficult and various methods were discussed by \cite{zhang2013likelihood}. One easy way is to estimate $\mu$ and $\phi$ on a grid of $p$'s and then select the $p$ which maximises the likelihood \citep{dunn2005series}.

One special property of the compound Poisson-Gamma distribution is that it is in the Tweedie dispersion exponential family \citep{jorgensen1987exponential}. It has a special variance mean relationship
\begin{equation}
\variance[X] = \phi \mu^p
\end{equation}
where, as a reminder, $1<p<2$. This can be derived using Equation \eqref{eq:compoundPoisson_variance} or via the partition function $Z$. That is let
\begin{equation}
\theta = \frac{\mu^{1-p}}{1-p}
\end{equation}
and
\begin{equation}
\ln Z = \frac{1}{\phi(2-p)\theta^\alpha(1-p)^\alpha} \ ,
\end{equation}
then
\begin{equation}
\variance[X] = \phi^2 \frac{\partial^2\ln Z}{\partial\theta^2} \ .
\end{equation}

\chapter{Compound Poisson}
In this chapter, the compound Poisson-Gamma distribution is reviewed and studied. Firstly methods for evaluating the density of the compound Poisson-Gamma distribution are discussed. Approximations such as the Normal approximation or saddlepoint approximation \citep{daniels1954saddlepoint} can be used. These approximate densities can be written in closed form. Exact methods can be computationally challenging when dealing with the infinite sum but can be done when summing over important terms \citep{dunn2005series}. Next, a method for parameter estimation will be proposed here using the EM algorithm \citep{dempster1977maximum}. However in the end of the chapter, it will be shown that no unique maximum likelihood estimator exist when certain parameters of the compound Poisson-Gamma distribution gets too large.

\section{Density Evaluation}
Suppose $X\sim\CPoisson(\lambda,\alpha,\beta)$ and $\lambda,\alpha,\beta>0$ then the p.d.f.~of $X$ is
\begin{equation}
p_X(x) = 
\begin{cases}
\delta(x) \euler^{-\lambda} & \text{ for } x=0 \\ 
\euler^{-\beta x-\lambda}\frac{1}{x}\sum_{y=1}^{\infty}W_y & \text{ for } x>0 
\end{cases}
\end{equation}
where
\begin{equation}
W_y = \frac{\beta^{y\alpha}\lambda^yx^{y\alpha}}{\Gamma(y\alpha)y!} \ .
\label{eq:compoundPoisson_w}
\end{equation}
The evaluation of this p.d.f.~is difficult because the sum of $W_y$ cannot be simplified. However the m.g.f.~of $X$ can be written down in closed form
\begin{equation}
M(\theta)=\exp\left[\lambda\left(\left(\frac{\beta}{\beta-\theta}\right)^{\alpha}-1\right)\right] \ .
\end{equation}
The m.g.f.~allows the study of limiting distributions of the compound Poisson-Gamma distribution. These limiting distributions then can be used to approximate the p.d.f.~of a compound Poisson-Gamma random variable.

The saddlepoint approximation \citep{daniels1954saddlepoint} gives an approximate solution to inverting the Laplace transformation  of the m.g.f.~giving the p.d.f. However inverting the Fourier transformation of the characteristic function also gives the p.d.f.~using computational methods \citep{dunn2008evaluation}. The saddlepoint approximation will be studied here.

Lastly the infinite sum can be computationally summed in a clever way. This was done by summing only large terms in the sum and ignores small terms. By using Stirling's approximation, the largest term in the sum can be approximately found \citep{dunn2005series}.

\subsection{Normal Approxmation}
The m.g.f.~provides a starting point to what limiting distributions the compound Poisson-Gamma distribution converges to for large parameters. However the m.g.f.~of $X$ itself it not useful because when considering $\lambda\rightarrow\infty$ or $\alpha\rightarrow\infty$ then $M_X(\theta)\rightarrow\infty$. Also for $\beta\rightarrow\infty$, then $M_X(\theta)\rightarrow 1$ which is not useful either.

The compound Poisson-Gamma random variable $X$ should be standardised to obtain useful limiting results from the m.g.f. Let
\begin{equation}
Z = \frac{X-\expectation[X]}{\sqrt{\variance[X]}} \ ,
\end{equation}
then using previous results
\begin{equation*}
Z = \frac{\beta}{\sqrt{\alpha(\alpha+1)\lambda}}\left(
X-\frac{\alpha\lambda}{\beta}
\right)
\end{equation*}
\begin{equation}
Z = bX+a
\end{equation}
where
\begin{equation}
b = \frac{\beta}{\sqrt{\alpha(\alpha+1)\lambda}}
\end{equation}
and
\begin{equation}
a = -\sqrt{\frac{\alpha\lambda}{\alpha+1}} \ .
\end{equation}

The m.g.f.~of $Z$ is then
\begin{equation*}
M_Z(\theta)=\expectation\left[\euler^{Z\theta}\right]
\end{equation*}
\begin{equation*}
M_Z(\theta)=\expectation\left[\euler^{(bX+a)\theta}\right]
\end{equation*}
\begin{equation*}
M_Z(\theta)=\euler^{a\theta}M_X(b\theta) \ .
\end{equation*}
Substituting in the values
\begin{equation*}
M_Z(\theta)=\exp\left(-\theta\sqrt{\frac{\alpha\lambda}{\alpha+1}}\right)
\exp\left[
\lambda\left(
\left(
\frac{\beta}{\beta-\frac{\beta\theta}{\sqrt{\alpha(\alpha+1)\lambda}}}
\right)^\alpha-1
\right)
\right]
\end{equation*}
\begin{equation*}
M_Z(\theta)=\exp\left(-\theta\sqrt{\frac{\alpha\lambda}{\alpha+1}}\right)
\exp\left[
\lambda\left(
\left(
\frac{\sqrt{\alpha(\alpha+1)\lambda}}{\sqrt{\alpha(\alpha+1)\lambda}-\theta}
\right)^\alpha
-1\right)
\right]
\end{equation*}
or in a different form
\begin{equation}
M_Z(\theta)=\exp\left(-\theta\sqrt{\frac{\alpha\lambda}{\alpha+1}}\right)
\exp\left[
\lambda\left(
\left(
1-\frac{\theta}{\sqrt{\alpha(\alpha+1)\lambda}}
\right)^{-\alpha}
-1\right)
\right] \ .
\end{equation}

The above form is useful so that the binomial expansion can be conducted, that is
\begin{multline}
\left(
1-\frac{\theta}{\sqrt{\alpha(\alpha+1)\lambda}}
\right)^{-\alpha}
=
1+
\sum_{r=1}^\infty \frac{\theta^r\prod_{s=1}^r(\alpha+s-1)}{(\alpha(\alpha+1)\lambda)^{r/2}r!}
\\
\text{for }\frac{\|\theta\|}{\sqrt{\alpha(\alpha+1)\lambda}}<1 \ .
\end{multline}
Substituting in the binomial expansion
\begin{equation*}
M_Z(\theta)=\exp\left(-\theta\sqrt{\frac{\alpha\lambda}{\alpha+1}}\right)
\exp\left[
\lambda
\sum_{r=1}^\infty \frac{\theta^r\prod_{s=1}^r(\alpha+s-1)}{(\alpha(\alpha+1)\lambda)^{r/2}r!}
\right] \ .
\end{equation*}
Writing in full the $r=1,2$ terms
\begin{multline*}
M_Z(\theta)=\exp\left(-\theta\sqrt{\frac{\alpha\lambda}{\alpha+1}}\right)
\exp\left[
\theta\sqrt{\frac{\alpha\lambda}{\alpha+1}}
+\frac{\theta^2}{2}
\right.\\\left.
+\lambda\sum_{r=3}^\infty \frac{\theta^r\prod_{s=1}^r(\alpha+s-1)}{(\alpha(\alpha+1)\lambda)^{r/2}r!}
\right]
\end{multline*}
and a term cancels out to get
\begin{equation}
M_Z(\theta)=
\exp\left[
	\frac{\theta^2}{2}
	+\sum_{r=3}^\infty
		\frac{\theta^r
			\prod_{s=1}^r(\alpha+s-1)
		}
		{(\alpha(\alpha+1))^{r/2}r!}
		\lambda^{1-r/2}
\right] \ .
\end{equation}

For large $\lambda$
\begin{equation}
\lim_{\lambda\rightarrow\infty}M_Z(\theta)=\exp\left[\frac{\theta^2}{2}\right]
\end{equation}
which is the same as the m.g.f.~of a standard Normal variable, therefore
\begin{equation}
\lim_{\lambda\rightarrow\infty}Z\sim\normal(0,1) \ .
\end{equation}
This should make sense as for high $\lambda$, the Poisson random variable has a high expectation, increasing the number of Gamma random variables in a summation. Increasing the number of terms in a summation will trigger the central limit theorem.

Next for high $\alpha$, it should be noted that
\begin{equation*}
\lim_{\alpha\rightarrow\infty}
	\frac{
		\prod_{s=1}^r(\alpha+s-1)
	}
	{(\alpha(\alpha+1))^{r/2}}
=
\frac{
		\prod_{s=1}^r\alpha
	}
	{\alpha^{r}}
\end{equation*}
so that
\begin{equation}
\lim_{\alpha\rightarrow\infty}
	\frac{
		\prod_{s=1}^r(\alpha+s-1)
	}
	{(\alpha(\alpha+1))^{r/2}} = 1 \ .
\end{equation}
As a result
\begin{equation}
\lim_{\alpha\rightarrow\infty}
M_Z(\theta)=
\exp\left[
	\frac{\theta^2}{2}
	+\sum_{r=3}^\infty
		\frac{\theta^r}
		{r!}
		\lambda^{1-r/2}
\right] \ .
\end{equation}
However this shows that taking the limit $\alpha\rightarrow\infty$ is not enough to get a Normal limiting distribution. For a Normal limiting distribution, the limit must be accompanied with the limit $\lambda\rightarrow\infty$, that is
\begin{equation}
\lim_{\lambda\rightarrow\infty}\lim_{\alpha\rightarrow\infty}Z\sim\normal(0,1) \ .
\end{equation}

Finally it should be noted that $M_Z(\theta)$ is independent of $\beta$. Thus $\beta$ will have no effect on the convergence to a Normal limiting distribution.

The above results justify the use of the approximation
\begin{equation}
X\sim\normal\left(\frac{\lambda\alpha}{\beta},\frac{\lambda\alpha(\alpha+1)}{\beta^2}\right)
\end{equation}
for large $\lambda$. The limiting case where $\lambda\rightarrow 0$, $\alpha\rightarrow 0$ and $\beta\rightarrow 0$ will not be discussed here.

\subsection{Normal Approximation - General Case}
It was shown in certain situations, the compound Poisson-Gamma distribution can be approximated with the Normal distribution. This can be extended to the general compound Poisson.

Let $Y\sim\poisson(\lambda)$ where $\lambda>0$ is the Poisson rate parameter. Let $U_i$ be some random variable for i.i.d.~$i=1,2,3,\dotdotdot$ with m.g.f.~$M_U(\theta)$. Let
\begin{equation}
X|Y = \sum_{i=1}^{Y}U_i \ .
\end{equation}

It can be shown that the m.g.f.~of $X$ is
\begin{equation}
M_X(\theta)=\exp\left[
\lambda(M_U(\theta)-1)
\right] \ .
\end{equation}
As in the compound Poisson-Gamma case, it is useful to standardise $X$ using
\begin{equation*}
Z = \frac{X-\lambda\expectation[U]}{\sqrt{\lambda\expectation[U^2]}}
\end{equation*}
\begin{equation}
Z = \frac{X}{\sqrt{\lambda\expectation[U^2]}}
-\frac{\sqrt{\lambda}\expectation[U]}{\sqrt{\expectation[U^2]}} \ .
\end{equation}
The m.g.f.~of $Z$ is then
\begin{equation*}
M_Z(\theta)=\expectation\left[\euler^{Z\theta}\right]
\end{equation*}
\begin{equation*}
M_Z(\theta)=\exp\left[-\theta\frac{\sqrt{\lambda}\expectation[U]}{\sqrt{\expectation[U^2]}}\right]
\exp\left[
\lambda\left(
M_U\left(\frac{\theta}{\sqrt{\lambda\expectation[U^2]}}\right)-1
\right)
\right]
\end{equation*}

Taking the Maclaurin series of $M_U(\theta)$
\begin{equation}
M_U(\theta) = 1 + \sum_{r=1}^\infty\frac{\theta^r}{r!}M_U^{(r)}(0)
\quad\text{for }\lim_{r\rightarrow\infty}
\left\|
\frac{\theta}{r+1}\frac{M_U^{(r+1)}(0)}{M_U^{(r)}(0)}
\right\|<1
\end{equation}
and substituting it into $M_Z(\theta)$
\begin{equation*}
M_Z(\theta)=\exp\left[-\theta\frac{\sqrt{\lambda}\expectation[U]}{\sqrt{\expectation[U^2]}}\right]
\exp\left[
\lambda\sum_{r=1}^\infty\frac{\theta^r}{(\lambda\expectation[U^2])^{r/2}r!}M_U^{(r)}(0)
\right] \ .
\end{equation*}
Writing out in full the $r=1,2$ terms
\begin{multline*}
M_Z(\theta)=\exp\left[-\theta\frac{\sqrt{\lambda}\expectation[U]}{\sqrt{\expectation[U^2]}}\right]
\exp\left[
\theta\frac{\sqrt{\lambda}M_U'(0)}{\sqrt{\expectation[U^2]}}
+
\frac{\theta^2M_U''(0)}{2\expectation[U^2]}
\right.\\+\left.
\lambda\sum_{r=3}^\infty\frac{\theta^r}{(\lambda\expectation[U^2])^{r/2}r!}M_U^{(r)}(0)
\right] \ .
\end{multline*}
By using the fact that $\expectation[U^r]=M_U^{(r)}(0)$ for $r=0,1,2\dotdotdot$, a few terms cancel out to get
\begin{equation}
M_Z(\theta)=\exp\left[
\frac{\theta^2}{2}
+
\sum_{r=3}^\infty\frac{\theta^r \expectation[U^r]}{(\expectation[U^2])^{r/2}r!}\lambda^{1-r/2}
\right] \ .
\end{equation}

For the limit $\lambda\rightarrow\infty$
\begin{equation}
\lim_{\lambda\rightarrow\infty}M_Z(\theta) = \exp\left[\frac{\theta^2}{2}\right]
\end{equation}
then it can be concluded that
\begin{equation}
\lim_{\lambda\rightarrow\infty}Z\sim\normal(0,1) \ .
\end{equation}
The convergence can be aided if the ratio $\dfrac{\expectation[U^r]}{(\expectation[U^2])^{r/2}}$ for $r=3,4,5,\dotdotdot$ are small.

\subsection{Saddlepoint approximation}
The m.g.f.~of a random variable $X$ is defined to be
\begin{equation}
M_X(\theta)=\expectation[\euler^{\theta X}]
=
\int_{-\infty}^{\infty}\euler^{x\theta} p_X(x) \diff x \ .
\end{equation}
For a given m.g.f., the saddlepoint approximation \citep{daniels1954saddlepoint} \citep{butler2007saddlepoint} finds an approximate $p_X(x)$. The saddlepoint approximation is given as
\begin{equation}
p_X(x)\approx\left(2\pi K_X''(s)\right)^{-1/2}\exp\left[K_X(s)-sx\right]
\label{eq:saddlePoint:generalSaddlePoint}
\end{equation}
where $K_X(\theta) = \ln\left(M_X(\theta)\right)$, and $s=s(x)$ is the solution to the saddle point equation $K_X'(s)=x$.

For the compound Poisson-Gamma distribution, the saddle point approximation is given as 
\begin{multline}
p_X(x)\approx
\frac{\left(\lambda\alpha\beta^\alpha\right)^{\frac{1}{2(\alpha+1)}}\euler^{-\lambda}}{\sqrt{2\pi(\alpha+1)}}x^{-\frac{\alpha+2}{2(\alpha+1)}}
\euler^{-x\beta}
\exp\left[x^{\frac{\alpha}{\alpha+1}}
	\frac{(\lambda\beta^\alpha)^{\frac{1}{\alpha+1}}(\alpha+1)}{\alpha^{\frac{\alpha}{\alpha+1}}}
\right]
\\
\text{for }x>0 \ .
\label{eq:saddle_point_approx}
\end{multline}
It appears the approximation is not well defined for $x=0$.

The integral of the density approximation over the support may not equal to one, it can be numerically re-normalised if necessary. Thus it may be more sensible to write the approximation up to a constant
\begin{equation}
p_X(x)\propto x^{-\frac{\alpha+2}{2(\alpha+1)}}\euler^{-x\beta}\exp\left[x^{\frac{\alpha}{\alpha+1}}
	\frac{(\lambda\beta^\alpha)^{\frac{1}{\alpha+1}}(\alpha+1)}{\alpha^{\frac{\alpha}{\alpha+1}}}
\right] \ .
\end{equation}

For the remainder of this section, it will be shown how the saddlepoint approximation can be algebraically derived. First of all the cumulant generating function $K_X(\theta)$ can be obtained from the m.g.f.
\begin{equation}
K_X(\theta) = \lambda\left[
\left(\frac{\beta}{\beta-\theta}\right)^\alpha-1
\right] \ .
\end{equation}
Taking the derivative with respect to $\theta$
\begin{equation}
K_X'(\theta)=\frac{\lambda\alpha\beta^\alpha}{(\beta-\theta)^{\alpha+1}}
\end{equation}
and this is known as the saddlepoint equation. The quantity $s=s(x)$ is the solution to the equation $K_X'(s)=x$, that is
\begin{equation*}
\frac{\lambda\alpha\beta^\alpha}{(\beta-s)^{\alpha+1}} = x
\end{equation*}
with solution
\begin{equation}
s = \beta - \left(\frac{\lambda\alpha\beta^\alpha}{x}\right)^{\frac{1}{\alpha+1}} \ .
\end{equation}

The second derivative of the cumulant generating function is
\begin{equation}
K_X''(\theta)=\frac{\lambda\alpha(\alpha+1)\beta^\alpha}{(\beta-\theta)^{\alpha+2}} \ .
\end{equation}
Substituting this and $K_X(\theta)$ into Equation \eqref{eq:saddlePoint:generalSaddlePoint}
\begin{equation*}
p_X(x)\approx
\frac{1}{\sqrt{2\pi}}\left[\frac{(\beta-s)^{\alpha+2}}{\lambda\alpha(\alpha+1)\beta^\alpha}\right]^{1/2}
\exp\left[
	\lambda\left(\left(\frac{\beta}{\beta-s}\right)^\alpha-1\right)-sx
\right] \ .
\end{equation*}
Substituting in $s=s(x)$
\begin{multline*}
p_X(x)\approx
\frac{1}{\sqrt{2\pi}}
\left[
\frac{\left(\beta-\beta+\left(\frac{\lambda\alpha\beta^\alpha}{x}\right)^{\frac{1}{\alpha+1}}\right)^{\alpha+2}}{\lambda\alpha(\alpha+1)\beta^\alpha}
\right]^{1/2}
\\
\exp\left[
	\lambda\left(
		\left(
			\frac{\beta}{\beta-\beta+\left(\frac{\lambda\alpha\beta^\alpha}{x}\right)^{\frac{1}{\alpha+1}}}
		\right)^\alpha
		-1
	\right)
	-x\left(
		\beta-\left(\frac{\lambda\alpha\beta^\alpha}{x}\right)^{\frac{1}{\alpha+1}}
	\right)
\right]
\end{multline*}
simplifying further
\begin{multline*}
p_X(x)\approx
\frac{1}{\sqrt{2\pi}}
\left[
\frac{\left(\frac{\lambda\alpha\beta^\alpha}{x}\right)^{\frac{\alpha+2}{\alpha+1}}}{\lambda\alpha(\alpha+1)\beta^\alpha}
\right]^{1/2}
\\
\exp\left[
	\lambda\left(
		\beta^\alpha
		\left(\frac{x}{\lambda\alpha\beta^\alpha}\right)^{\frac{\alpha}{\alpha+1}}
		-1
	\right)
	-x\beta
	+(\lambda\alpha\beta^\alpha)^{\frac{1}{\alpha+1}}x^{1-\frac{1}{\alpha+1}}
\right]
\end{multline*}
\begin{multline*}
p_X(x)\approx
\frac{1}{\sqrt{2\pi(\alpha+1)}}x^{-\frac{\alpha+2}{2(\alpha+1)}}
\left(\lambda\alpha\beta^\alpha\right)^{\left(\frac{\alpha+2}{\alpha+1}-1\right)/2}
\\
\exp\left[
	\lambda\left(
		\beta^\alpha
		\left(\frac{x}{\lambda\alpha\beta^\alpha}\right)^{\frac{\alpha}{\alpha+1}}
		-1
	\right)
	-x\beta
	+(\lambda\alpha\beta^\alpha)^{\frac{1}{\alpha+1}}x^{\frac{\alpha}{\alpha+1}}
\right]
\end{multline*}
\begin{multline*}
p_X(x)\approx
\frac{\left(\lambda\alpha\beta^\alpha\right)^{\frac{1}{2(\alpha+1)}}}{\sqrt{2\pi(\alpha+1)}}x^{-\frac{\alpha+2}{2(\alpha+1)}}
\\
\exp\left[
	-\lambda
	-x\beta
	+x^{\frac{\alpha}{\alpha+1}}
	\left(
		\frac{\lambda\beta^\alpha}{\left(\lambda\alpha\beta^\alpha\right)^{\frac{\alpha}{\alpha+1}}}+\left(\lambda\alpha\beta^\alpha\right)^{\frac{1}{\alpha+1}}
	\right)
\right]
\end{multline*}
\begin{multline*}
p_X(x)\approx
\frac{\left(\lambda\alpha\beta^\alpha\right)^{\frac{1}{2(\alpha+1)}}\euler^{-\lambda}}{\sqrt{2\pi(\alpha+1)}}x^{-\frac{\alpha+2}{2(\alpha+1)}}
\euler^{-x\beta}
\\
\exp\left[x^{\frac{\alpha}{\alpha+1}}
	\left(
		(\lambda\beta^\alpha)^{1-\frac{\alpha}{\alpha+1}}\alpha^{-\frac{\alpha}{\alpha+1}}
		+\left(\lambda\alpha\beta^\alpha\right)^{\frac{1}{\alpha+1}}
	\right)
\right]
\end{multline*}
\begin{multline*}
p_X(x)\approx
\frac{\left(\lambda\alpha\beta^\alpha\right)^{\frac{1}{2(\alpha+1)}}\euler^{-\lambda}}{\sqrt{2\pi(\alpha+1)}}x^{-\frac{\alpha+2}{2(\alpha+1)}}\euler^{-x\beta}
\\
\exp\left[x^{\frac{\alpha}{\alpha+1}}
	\left(
		(\lambda\beta^\alpha)^{\frac{1}{\alpha+1}}\alpha^{-\frac{\alpha}{\alpha+1}}
		+\left(\lambda\alpha\beta^\alpha\right)^{\frac{1}{\alpha+1}}
	\right)
\right]
\end{multline*}
\begin{multline*}
p_X(x)\approx
\frac{\left(\lambda\alpha\beta^\alpha\right)^{\frac{1}{2(\alpha+1)}}\euler^{-\lambda}}{\sqrt{2\pi(\alpha+1)}}x^{-\frac{\alpha+2}{2(\alpha+1)}}\euler^{-x\beta}
\\
\exp\left[x^{\frac{\alpha}{\alpha+1}}(\lambda\beta^\alpha)^{\frac{1}{\alpha+1}}
	\left(
		\alpha^{-\frac{\alpha}{\alpha+1}}+\alpha^{\frac{1}{\alpha+1}}
	\right)
\right] \ .
\end{multline*}

The expression $\alpha^{-\frac{\alpha}{\alpha+1}}+\alpha^{\frac{1}{\alpha+1}}$ can be simplified by putting the two terms over a common denominator
\begin{align}
\alpha^{-\frac{\alpha}{\alpha+1}}+\alpha^{\frac{1}{\alpha+1}} & = \alpha^{\frac{1}{\alpha+1}}+\frac{1}{\alpha^{\frac{\alpha}{\alpha+1}}}
\nonumber\\
& = \frac{\alpha^{\frac{1}{\alpha+1}}\alpha^{\frac{\alpha}{\alpha+1}}+1}{\alpha^{\frac{\alpha}{\alpha+1}}}
\nonumber\\
& = \frac{\alpha+1}{\alpha^{\frac{\alpha}{\alpha+1}}}
\end{align}
so that
\begin{equation*}
p_X(x)\approx
\frac{\left(\lambda\alpha\beta^\alpha\right)^{\frac{1}{2(\alpha+1)}}\euler^{-\lambda}}{\sqrt{2\pi(\alpha+1)}}x^{-\frac{\alpha+2}{2(\alpha+1)}}
\euler^{-x\beta}
\exp\left[x^{\frac{\alpha}{\alpha+1}}
	\frac{(\lambda\beta^\alpha)^{\frac{1}{\alpha+1}}(\alpha+1)}{\alpha^{\frac{\alpha}{\alpha+1}}}
\right] \ .
\end{equation*}

\subsection{Series Evaluation}
Evaluation of the p.d.f.~of the compound Poisson-Gamma distribution is difficult because of the infinite sum
\begin{equation}
p_X(x) = 
\delta(x) \euler^{-\lambda}
+
\euler^{-\beta x-\lambda}\frac{1}{x}\sum_{y=1}^{\infty}W_y
\quad\text{for }x\geqslant 0
\end{equation}
where
\begin{equation}
W_y = \frac{\beta^{y\alpha}\lambda^yx^{y\alpha}}{\Gamma(y\alpha)y!} \ .
\label{eq:compoundPoisson_Wy}
\end{equation}
$W_y$ can be expressed in a different way using different parameters
\begin{equation}
W_y(x,p,\phi)=\frac{x^{y\alpha}}{\phi^{y(1+\alpha)}(p-1)^{y\alpha}(2-p)^yy!\Gamma(y\alpha)}
\end{equation}
where
\begin{equation}
p=\frac{2+\alpha}{1+\alpha}
\end{equation}
and
\begin{equation}
\phi = \frac{\alpha+1}{\beta^{2-p}(\lambda\alpha)^{p-1}} \ .
\end{equation}

\cite{dunn2005series} approximated the sum by truncation
\begin{equation}
\sum_{y=1}^\infty W_y \approx \sum_{y=y_\text{l}}^{y_\text{u}}W_y
\end{equation}
where $y_\text{l}<y_{\text{max}}<y_\text{u}$ and $y_{\text{max}}$ is the value of $y$ which maximises $W_y$. \cite{dunn2005series} used Stirling's approximation to find $y_{\text{max}}$. This was done by treating $W_y$ as a continuous function of $y$ and is differentiable with respect to $y$.

It is easier to differentiate $\ln(W_y)$ where
\begin{multline*}
\ln(W_y) = y\alpha\ln(x)-y(1+\alpha)\ln(\phi)-y\alpha\ln(p-1)\\-y\ln(2-p)-\ln(y!)-\ln\Gamma(y\alpha)
\end{multline*}
\begin{equation}
\ln(W_y) =
y\ln\left(
	\frac{x^\alpha}{\phi^{1+\alpha}(p-1)^\alpha(2-p)}
\right)
-\ln(y!)-\ln\Gamma(y\alpha) \ .
\end{equation}
Using Stirling's approximation
\begin{equation}
\ln(n!)\approx\ln\Gamma(n)\approx n\ln(n!)-n
\end{equation}
then
\begin{equation}
\ln(W_y) \approx
y\ln\left(
	\frac{x^\alpha}{\phi^{1+\alpha}(p-1)^\alpha(2-p)}
\right)
-y\ln(y)+y-y\alpha\ln(y\alpha) + y\alpha \ .
\end{equation}
Taking the derivative with respect to $y$
\begin{multline*}
\frac{\partial \ln(W_y)}{\partial y} \approx
\ln\left(
	\frac{x^\alpha}{\phi^{1+\alpha}(p-1)^\alpha(2-p)}
\right)
-\ln(y)-1+1
\\-\alpha\ln(y\alpha)-\alpha+\alpha
\end{multline*}
\begin{equation}
\frac{\partial \ln(W_y)}{\partial y} \approx
\ln\left(
	\frac{x^\alpha}{\phi^{1+\alpha}(p-1)^\alpha(2-p)}
\right)
-\ln(y)
-\alpha\ln(y\alpha) \ .
\end{equation}

Setting the derivative to zero
\begin{equation*}
0 \approx \ln\left(
\frac{x^\alpha}{\phi^{1+\alpha}(p-1)^\alpha(2-p)y_{\text{max}}^{1+\alpha}\alpha^\alpha}
\right)
\end{equation*}
\begin{equation*}
1 \approx 
\frac{x^\alpha}{\phi^{1+\alpha}(p-1)^\alpha(2-p)y_{\text{max}}^{1+\alpha}\alpha^\alpha}
\end{equation*}
\begin{equation*}
y_{\text{max}}^{1+\alpha} \approx 
\frac{x^\alpha}{\phi^{1+\alpha}(p-1)^\alpha(2-p)\alpha^\alpha}
\end{equation*}
\begin{equation*}
y_{\text{max}} \approx 
\frac{1}{\phi}
\left(
	\frac{x}{(p-1)\alpha}
\right)^{\frac{\alpha}{1+\alpha}}
(2-p)^{\frac{-1}{1+\alpha}} \ .
\end{equation*}
This can be simplified using the fact that 
\begin{equation*}
\alpha=\frac{2-p}{p-1} \ ,
\end{equation*}
\begin{equation*}
\frac{1}{1+\alpha} = p-1
\end{equation*}
and
\begin{equation*}
\frac{\alpha}{1+\alpha} = 2-p
\end{equation*}
then
\begin{equation*}
y_{\text{max}} \approx 
\frac{1}{\phi}
\left(
	\frac{x}{2-p}
\right)^{2-p}
(2-p)^{1-p}
\end{equation*}
and finally
\begin{equation}
y_{\text{max}} \approx \frac{x^{2-p}}{\phi(2-p)} \ .
\end{equation}
Because values of $y$ should be a positive integer, it would be appropriate to round the solution to $y_\text{max}$ accordingly
\begin{equation}
y_{\text{max}} = \text{max}\left[
1,\text{round}\left(\frac{x^{2-p}}{\phi(2-p)}\right)
\right] \ .
\end{equation}

To verify that $y_\text{max}$ is a maximum, the second derivative can be investigated
\begin{equation}
\frac{\partial^2\ln(W_y)}{\partial y^2}
\approx
-\frac{1}{y}(\alpha+1)
\end{equation}
to see that
\begin{equation}
\frac{\partial^2\ln(W_y)}{\partial y^2} < 0 \quad \text{for }y=1,2,3,\dotdotdot
\end{equation}
therefore $y_\text{max}$ is a maximum.

Returning back to the truncation of the infinite sum
\begin{equation}
\sum_{y=1}^\infty W_y \approx \sum_{y=y_\text{l}}^{y_\text{u}}W_y \ ,
\end{equation}
$y_\text{l}$ and $y_\text{u}$ can be chosen such that $W_{y_\text{l}}$ and $W_{y_\text{u}}$ are less than $\epsilon W_{y_\text{max}}$ where $\epsilon$ is some small constant, for example $\epsilon=\euler^{-37}$ will be better than machine precision in 64 bits \citep{dunn2005series}. To prevent overflow problems, it is advised to calculate each term in the summation in log scale \citep{dunn2005series} and using the following equation
\begin{equation}
\ln\left[\sum_{y=y_\text{l}}^{y_\text{u}}W_y\right]
= \ln\left(W_{y_\text{max}}\right)+\ln\sum_{y=y_\text{l}}^{y_\text{u}}\exp\left[
\ln\left(W_y\right)-\ln\left(W_{y_\text{max}}\right)
\right] \ .
\end{equation}

\subsection{Simulation Studies}

Simulations of a compound Poisson-Gamma random variable were conducted with the aim to compare how well these density evaluations methods perform. This was done by comparing the evaluated densities with the histogram of simulations and a Q-Q plot. The following compound Poisson-Gamma distributions were used in the simulations to capture the variety in the compound Poisson-Gamma family: $\CPoisson(1,1,1)$, $\CPoisson(10,1,1)$, $\CPoisson(1,100,1)$, $\CPoisson(100,100,1)$.

A few technical details will be stated here. The Normal approximation does not support probability mass at $x=0$, but instead has a support of $-\infty<x<\infty$. The Normal approximation with $-\infty<x<\infty$ support was used here.

Suppose realizations of $X$ were simulated $\left\{x_1,x_2,x_3,\dotdotdot,x_n\right\}$. The saddlepoint approximation was computed up to a constant using
\begin{equation}
p_X(x) \propto \exp\left[
-\frac{\alpha+2}{2(\alpha+1)}\ln(x)
-x\beta
+\left(
\frac{x\beta}{\alpha}
\right)^{\frac{\alpha}{\alpha+1}}\lambda^{\frac{1}{\alpha+1}}(\alpha+1) - k
\right]
\end{equation}
for 500 equally spaced points from and including the minimum non-zero simulated value to the maximum simulated value. $k$ is some constant to control over/underflow and it was chosen to be
\begin{equation}
k = \max_{i\in\left\{1,2,3,\dotdotdot,n\right\}}\left[
-\frac{\alpha+2}{2(\alpha+1)}\ln(x_i)
-x_i\beta
+\left(
\frac{x_i\beta}{\alpha}
\right)^{\frac{\alpha}{\alpha+1}}\lambda^{\frac{1}{\alpha+1}}(\alpha+1)
\right] \ .
\end{equation}
The density was then was normalised by numerically integrating it using the trapezium rule using the 500 evaluated points.

A Q-Q plot is a plot which compares the empirical with the theoretical c.d.f. Let
\begin{equation}
F_X(x) = \prob(X\leqslant x)
\end{equation}
\begin{equation}
\widehat{F}_X(x) = \frac{1}{n}\cdot\text{max}\left[
\left(\sum_{i=1}^n\mathbb{I}(x_i\leqslant x)\right)-0.5,0
\right] \ ,
\end{equation}
then a Q-Q plot plot is a parametric plot which plots $\widehat{F}_X^{-1}(p)$ against $F_X^{-1}(p)$ for $p=\frac{0.5}{n},\frac{1.5}{n},\frac{2.5}{n},\dotdotdot,\frac{n-0.5}{n}$. If $F_X(x)$ and $\widehat{F}_X(x)$ are similar, a Q-Q plot should be a straight line with gradient 1, intercepting the origin. However there is no closed form for $F_X(x)$. When using the exact or saddlepoint method, $F_X(x)$ was found numerically by evaluating the p.d.f.~at $10\,000$ equally spaced points from and including the minimum to the maximum of the simulated samples, and then summing the required trapeziums. $\widehat{F}_X^{-1}(p)$ was then calculated by interpolation. For the saddlepoint approximation where $x=0$ cannot be evaluated and their exist a zero value in the sample, the lower limit of the numerical integral was chosen, arbitrary, to be
\begin{equation}
\frac{\euler^{-\lambda}\lambda\cdot\text{min}\left(\text{non-zero samples}\right)}{\euler^{-\lambda}(1+\lambda)}\times 0.1 \ .
\end{equation}

For low $\lambda$ (Figures \ref{fig:cp_simulation_1} and \ref{fig:cp_simulation_3}) there is a chance of simulating zeros. The Normal approximation failed to capture the probability mass at zero because the Normal distribution is symmetric. The saddlepoint approximation did capture probability mass at zero as shown by an increase in probability density towards zero. However in Figure \ref{fig:cp_simulation_3}, the compound Poisson-Gamma distribution looked like a mixture model and the saddlepoint approximation was not flexible enough to capture each individual peak. In addition, the Q-Q plots looked quite sensitive at the tails of each peak, perhaps there were a few inaccuracies in the exact evaluation of the p.d.f.~at low densities.

As $\lambda$ increased, all 3 density evaluation methods performed quite well. In Figure \ref{fig:cp_simulation_2}, the Normal approximation did not quite capture the skewness of the histogram. However in Figure \ref{fig:cp_simulation_4} for the Normal looking compound Poisson-Gamma random variable, all methods performed well.

\begin{figure}
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_hist1.png}
        \caption{Histogram - Exact method}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_qq1.png}
        \caption{Q-Q plot - Exact method}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_norm_hist1.png}
        \caption{Histogram - Normal approx.}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_norm_qq1.png}
        \caption{Q-Q plot - Normal approx.}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_saddle_hist1.png}
        \caption{Histogram - Saddlepoint approx.}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_saddle_qq1.png}
        \caption{Q-Q plot - Saddlepoint approx.}
    \end{subfigure}
    }
    \caption{Simulations of $\CPoisson(1,1,1)$ compared to different density evaluation methods}
    \label{fig:cp_simulation_1}
\end{figure}

\begin{figure}
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_hist2.png}
        \caption{Histogram - Exact method}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_qq2.png}
        \caption{Q-Q plot - Exact method}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_norm_hist2.png}
        \caption{Histogram - Normal approx.}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_norm_qq2.png}
        \caption{Q-Q plot - Normal approx.}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_saddle_hist2.png}
        \caption{Histogram - Saddlepoint approx.}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_saddle_qq2.png}
        \caption{Q-Q plot - Saddlepoint approx.}
    \end{subfigure}
    }
    \caption{Simulations of $\CPoisson(10,1,1)$ compared to different density evaluation methods}
    \label{fig:cp_simulation_2}
\end{figure}

\begin{figure}
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_hist3.png}
        \caption{Histogram - Exact method}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_qq3.png}
        \caption{Q-Q plot - Exact method}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_norm_hist3.png}
        \caption{Histogram - Normal approx.}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_norm_qq3.png}
        \caption{Q-Q plot - Normal approx.}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_saddle_hist3.png}
        \caption{Histogram - Saddlepoint approx.}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_saddle_qq3.png}
        \caption{Q-Q plot - Saddlepoint approx.}
    \end{subfigure}
    }
    \caption{Simulations of $\CPoisson(1,100,1)$ compared to different density evaluation methods}
    \label{fig:cp_simulation_3}
\end{figure}

\begin{figure}
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_hist4.png}
        \caption{Histogram - Exact method}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_qq4.png}
        \caption{Q-Q plot - Exact method}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_norm_hist4.png}
        \caption{Histogram - Normal approx.}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_norm_qq4.png}
        \caption{Q-Q plot - Normal approx.}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_saddle_hist4.png}
        \caption{Histogram - Saddlepoint approx.}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/cp_saddle_qq4.png}
        \caption{Q-Q plot - Saddlepoint approx.}
    \end{subfigure}
    }
    \caption{Simulations of $\CPoisson(100,100,1)$ compared to different density evaluation methods}
    \label{fig:cp_simulation_4}
\end{figure}

\section{EM Algorithm}

The EM algorithm \citep{dempster1977maximum} will be proposed here as a method to estimate the parameters of a $X\sim\CPoisson(\lambda,\alpha,\beta)$ random variable given a sample of measurements of it $\left\{x_1,x_2,x_3,\dotdotdot,x_n\right\}$.

A popular method for parameter estimation is the maximum log likelihood method. Let $\widehat{\lambda}$, $\widehat{\alpha}$ and $\widehat{\beta}$ be estimators of $\lambda$, $\alpha$ and $\beta$ respectively. The log likelihood is defined to be
\begin{equation*}
\ln L(\lambda,\alpha,\beta;X) = \sum_{i=1}^n\left[
\mathbb{I}(x_i=0)\ln \prob(X=x_i)
+\mathbb{I}(x_i>0)\ln p_X(x_i)
\right]
\end{equation*}
\begin{multline}
\ln L(\lambda,\alpha,\beta;X) = \sum_{i=1}^n\left[
-\mathbb{I}(x_i=0)\lambda
\vphantom{\sum_i^i}\right.\\\left.
+\mathbb{I}(x_i>0)\left(
	-\beta x_i-\lambda-\ln x -\ln\sum_{y=1}^\infty W_y
\right)
\right]
\end{multline}
where $W_y=W_y(\lambda,\alpha,\beta)$ was defined in Equation \eqref{eq:compoundPoisson_w}. $\widehat{\lambda}$, $\widehat{\alpha}$ and $\widehat{\beta}$ are values of $\lambda$, $\alpha$ and $\beta$ which jointly maximises the log likelihood. The gradient of the log likelihood can be evaluated \citep{dunn2005series} and optimisation can be done using gradient methods.

The EM algorithm \citep{dempster1977maximum} instead treats $Y\sim\poisson(\lambda)$ as a latent variable. Let $\left\{Y_1,Y_2,Y_3,\dotdotdot, Y_n\right\}$ be realizations of $Y$ and $\left\{x_1,x_2,x_3,\dotdotdot, x_n\right\}$ be measurements of $X|Y$. Define the joint log likelihood to be
\begin{multline*}
\ln L(\lambda,\alpha,\beta;X,Y)
=
\sum_{i=1}^n
\left[
	\mathbb{I}(x_i=0)
	\ln
	\prob(Y=0)
	\right.\\\left.
	+
	\mathbb{I}(x_i>0)
	\ln
	\left[
	p_{X|Y}(x_i|Y_i)\prob(Y=Y_i)
	\right]
\right]
\end{multline*}
so that
\begin{multline}
\ln L(\lambda,\alpha,\beta;X,Y)
=
\sum_{i=1}^n
\left[
	-\mathbb{I}(x_i=0)
	\lambda
	\right.\\\left.+
	\mathbb{I}(x_i>0)
	\left(
	Y_i\alpha\ln\beta-\ln\Gamma(Y_i\alpha)+(Y_i\alpha-1)\ln x_i - \beta x_i
	\right.\right.\\\left.\left.	
	- \lambda + Y_i \ln \lambda - \ln(Y_i!)
	\right)
\right] \ .
\end{multline}
The estimators are found by optimising the joint log likelihood. This is done iteratively by estimating the $Y$s given the $X$s and parameters (E step), followed by estimating the parameters given the $Y$s and $X$s (M step) until some convergence conditions are met.

It will be shown that using some approximations, the E step and M step can be implemented. The Cram\'er-Rao lower bound \citep{rao1945information} \citep{cramer1946mathematical} can also be found by using a few approximations. However simulations show that these estimators struggle for high $\lambda$ and it will be shown that this will be the case for all estimators.

\subsection{E Step}

In the E step, the realizations of $Y$ are estimate using
\begin{equation}
y_i = \expectation\left[
Y|X=x_i
\right]
\end{equation}
for given parameters $\lambda$, $\alpha$ and $\beta$. The conditional expectation is calculated using
\begin{equation}
y_i = 
\begin{cases}
0 & \text{ for } x=0 \\ 
\dfrac{\sum_{y=1}^\infty y \prob(Y=y|X=x_i)}{\sum_{y=1}^\infty \prob(Y=y|X=x_i)} & \text{ for } x>0
\end{cases}
\end{equation}
where
\begin{equation*}
\prob(Y=y|X=x) = \frac{p_{X|Y}(x|y)\prob(Y=y)}{p_X(x)} \ .
\end{equation*}
Focussing on the $x>0$ case for now
\begin{equation*}
\prob(Y=y|X=x) = \frac{1}{p_X(x)}\frac{\beta^{y\alpha}}{\Gamma(y\alpha)}x^{y\alpha-1}\euler^{-\beta x}\frac{\euler^{-\lambda}\lambda^y}{y!}
\end{equation*}
and
\begin{equation}
\prob(Y=y|X=x) = W_y \frac{\euler^{-\lambda-\beta x}}{x}
\end{equation}
where $W_y$ is defined in Equation \eqref{eq:compoundPoisson_Wy}.
Then the conditional expectation is
\begin{equation}
y_i = \frac{\sum_{y=1}^\infty y W_y}{\sum_{y=1}^\infty W_y} \ .
\end{equation}
It was already established how the denominator can be evaluated \citep{dunn2005series}. A similar method for evaluating the numerator can be obtained by truncating the sum and summing large terms.

Let
\begin{equation}
W_y^{(r)} = y^r W_y
\end{equation}
so that
\begin{equation}
y_i = \frac{\sum_{y=1}^\infty W^{(1)}_y}{\sum_{y=1}^\infty W_y}
\end{equation}
and
\begin{equation}
\variance[Y|X=x_i] = \frac{\sum_{y=1}^\infty W^{(2)}_y}{\sum_{y=1}^\infty W_y} - \left(y_i\right)^2 \ .
\end{equation}
The exponent $r=0,1,2,\dotdotdot$ will be useful in the M step where higher conditional moments are needed.
The sum can be truncated
\begin{equation}
\sum_{y=1}^\infty W^{(r)}_y \approx \sum_{y=y_\text{l}}^{y_\text{u}} W^{(r)}_y
\end{equation}
where $y_\text{l}<y_\text{max}<y_\text{u}$ and $y_\text{max}$ is the value of $y$ which maximises $W_y^{(r)}$. By expressing $\ln(W^{(r)}_y)$ as
\begin{equation}
\ln\left(W_y^{(r)}\right)=r\ln(y)+\ln(W_y)
\end{equation}
and then taking the derivative with respect to $y$
\begin{equation}
\frac{\partial \ln(W_y^{(r)})}{\partial y} = \frac{r}{y } + \frac{\partial \ln(W_y)}{\partial y} \ .
\end{equation}
Keep in mind that $y=1,2,3,\dotdotdot$ so for large $y$, an approximation can be made $r/y\approx 0$ so that
\begin{equation}
\frac{\partial \ln(W_y^{(r)})}{\partial y} \approx \frac{\partial \ln(W_y)}{\partial y} \ .
\end{equation}
By using such an approximation, the method for evaluating the sum $\sum_{y=1}^\infty W^{(r)}_y$ is almost exactly the same as evaluating the sum $\sum_{y=1}^\infty W_y$, in particular $y_\text{max}$ are the same. For a given $r=0,1,2,\dotdotdot$, the limit of the sum $\sum_{y=y_\text{l}}^{y_\text{u}} W^{(r)}_y$ are chosen such that $W_{y_\text{l}}$ and $W_{y_\text{u}}$ are less than $\epsilon W_{y_\text{max}}^{(r)}$ where $\epsilon$ is some small constant. The limits will be different for different values of $r$.

\subsection{M Step}

In the M step, the conditional expected joint log likelihood is maximised with respect to the parameters $\lambda$, $\alpha$ and $\beta$. The objective function is then
\begin{multline*}
T(\lambda,\alpha,\beta)
=
\sum_{i=1}^n
\expectation\left[
	-\mathbb{I}(x_i=0)
	\lambda
	\right.\\\left.+
	\mathbb{I}(x_i>0)
	\left(
	Y_i\alpha\ln\beta-\ln\Gamma(Y_i\alpha)+(Y_i\alpha-1)\ln x_i - \beta x_i
	\right.\right.\\\left.\left.	
	- \lambda + Y_i \ln \lambda - \ln(Y_i!)
	\right)
|X_i=x_i\right]
\end{multline*}
\begin{multline*}
T(\lambda,\alpha,\beta)
=
-n\lambda
\\+
	\sum_{i=1}^n
	\mathbb{I}(x_i>0)
	\left[
	\expectation[Y_i|X_i=x_i]\alpha\ln\beta-\expectation[\ln\Gamma(Y_i\alpha)|X_i=x_i]
	\right.\\\left.
	+\expectation[Y_i|X_i=x_i]\alpha\ln x_i - \beta x_i
	+ \expectation[Y_i|X_i=x_i] \ln \lambda
	\right] + c
\end{multline*}
where $c$ is some constant not dependent on $\lambda$, $\alpha$ or $\beta$.

The conditional expectation $y_i = \expectation[Y_i|X_i=x_i]$ was calculated in the E step. The quantity $\expectation[\ln\Gamma(Y_i\alpha)|X_i=x_i]$ can be calculated using the approximation
\begin{equation}
\expectation[\ln\Gamma(Y_i\alpha)|X_i=x_i] \approx
\ln\Gamma(\alpha y_i) + \frac{1}{2}\zeta_i\alpha^2\psi'(y_i\alpha)
\end{equation}
where
\begin{equation}
\zeta_i = \variance[Y_i|X_i=x_i]
\end{equation}
calculated in the E step and $\psi$ is the digamma function. The objective function is then
\begin{multline}
T(\lambda,\alpha,\beta)
=
-n\lambda+
	\sum_{i=1}^n
	\mathbb{I}(x_i>0)
	\left[
	y_i\alpha\ln\beta-\ln\Gamma(\alpha y_i) - \frac{1}{2}\zeta_i\alpha^2\psi'(y_i\alpha)
	\right.\\\left.
	+y_i\alpha\ln x_i - \beta x_i
	+ y_i \ln \lambda
	\vphantom{\frac{1}{1}}\right] + c \ .
\end{multline}

Taking the derivative with respect to $\lambda$
\begin{equation}
\frac{\partial T}{\partial\lambda} = -n + \frac{\sum_{i=1}^ny_i}{\lambda}
\end{equation}
and setting it to zero
\begin{equation}
\widehat{\lambda} = \frac{\sum_{i=1}^n y_i}{n}
\end{equation}
obtains a M step estimator, and a familiar one, for $\lambda$. Taking the second derivative with respect to $\lambda$
\begin{equation}
\frac{\partial^2 T}{\partial \lambda^2} = -\frac{\sum_{i=1}^ny_i}{\lambda^2} < 0
\end{equation}
verifies $\widehat{\lambda}$ maximises T. In addition
\begin{equation}
\frac{\partial^2 T}{\partial \alpha \partial \lambda } = 0
\end{equation}
and
\begin{equation}
\frac{\partial^2 T}{\partial \beta \partial \lambda } = 0 \ .
\end{equation}

Maximising $T$ with respect to $\alpha$ and $\beta$ can be done numerically using the Newton-Raphson method since derivatives up to the second order can be obtained. For the first order these are:
\begin{multline}
\frac{\partial T}{\partial \alpha} = \sum_{i=1}^n\mathbb{I}(x_i>0)\left[
	y_i\ln\beta -\psi(\alpha y_i)y_i-\zeta_i\alpha\psi'(\alpha y_i)
	\vphantom{\frac{1}{1}}\right.\\\left.
	- \frac{1}{2}\zeta_i\alpha^2\psi''(\alpha y_i)y_i + y_i\ln x_i
\right]
\end{multline}
and
\begin{equation}
\frac{\partial T}{\partial \beta} = \sum_{i=1}^n\mathbb{I}(x_i>0)\left[
\frac{\alpha y_i}{\beta}-x_i
\right] \ .
\end{equation}
The second orders are:
\begin{equation}
\frac{\partial^2 T}{\partial \alpha \partial \beta} =
\sum_{i=1}^n \mathbb{I}(x_i>0)\left[\frac{y_i}{\beta}\right] \ ,
\end{equation}
\begin{equation}
\frac{\partial^2 T}{\partial \beta^2} = \sum_{i=1}^n\mathbb{I}(x_i>0)\left[-\frac{\alpha y_i}{\beta^2}
\right]
\end{equation}
and
\begin{multline*}
\frac{\partial^2 T}{\partial \alpha^2} =  \sum_{i=1}^n\mathbb{I}(x_i>0)\left[
	-y_i^2\psi'(\alpha y_i) - \zeta_i\psi'(\alpha y_i) - \zeta_i\alpha y_i\psi''(\alpha y_i)
	\vphantom{\frac{1}{2}}\right.\\\left.	
	- \zeta_i\alpha\psi''(\alpha y_i)y_i
	-\frac{1}{2}\zeta_i\alpha^2\psi'''(\alpha y_i)y_i^2
\right]
\end{multline*}
simplifying to
\begin{multline}
\frac{\partial^2 T}{\partial \alpha^2} =  \sum_{i=1}^n\mathbb{I}(x_i>0)\left[
	-(y_i^2+\zeta_i)\psi'(\alpha y_i) - 2\zeta_i\alpha y_i\psi''(\alpha y_i)
	\vphantom{\frac{1}{2}}\right.\\\left.	
	-\frac{1}{2}\zeta_i\alpha^2y_i^2\psi'''(\alpha y_i)
\right] \ .
\end{multline}

All the derivatives can be used in the Newton-Raphson iterative update, which is
\begin{equation}
\begin{pmatrix}
\alpha \\ \beta
\end{pmatrix}
\leftarrow
\begin{pmatrix}
\alpha \\ \beta
\end{pmatrix}
\left[
	\nabla_{\alpha,\beta}\nabla_{\alpha,\beta}\T T
\right]^{-1}
\left[\nabla_{\alpha,\beta} T\right]
\end{equation}
where
\begin{equation}
\nabla_{\alpha,\beta}=
\begin{pmatrix}
	{\partial}/{\partial \alpha}
	\\
	{\partial}/{\partial \beta}
\end{pmatrix} \ .
\end{equation}
Since increasing $T$ is sufficient for the EM algorithm \citep{dempster1977maximum}, one step of the Newton-Raphson iterative update was chosen in the M step to avoid implementing a convergence condition.

\subsection{Estimation Error}

The variance of the M step estimators can be obtained by using the Cram\'er-Rao lower bound \citep{rao1945information} \citep{cramer1946mathematical} from the Fisher's information matrix defined as
\begin{equation}
\matr{I} = -\expectation\left[
\nabla_{\lambda,\alpha,\beta}\nabla_{\lambda,\alpha,\beta}\T \ln L(\lambda,\alpha,\beta;X,Y)
\right] \ .
\end{equation}
The calculation is similar to the derivatives of $T$, so using the same approximations and the that fact $\expectation[Y]=\lambda$, then
\begin{equation}
\matr{I}
=
\begin{pmatrix}
\dfrac{n}{\lambda} & 0 & 0 \\
0 & (\lambda^2+\lambda)\psi'(\alpha\lambda)+2\alpha\lambda^2\psi''(\alpha\lambda)+\frac{1}{2}\alpha^2\lambda^3\psi'''(\alpha\lambda) & -\dfrac{n\lambda}{\beta}\\
0 & -\dfrac{n\lambda}{\beta} & \dfrac{n\alpha\lambda}{\beta^2}
\end{pmatrix} \ .
\end{equation}
The Cram\'er-Rao lower bound is then
\begin{equation}
\cov\left[
\begin{pmatrix}
\widehat{\lambda}\\\widehat{\alpha}\\\widehat{\beta}
\end{pmatrix}
\right]
= \matr{I}^{-1} \ .
\end{equation}

\subsection{Simulations}

\begin{figure}[p]
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_1_lnL.eps}
        \caption{Log likelihood}
    \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_1_lambda.eps}
        \caption{$\lambda$}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_1_alpha.eps}
        \caption{$\alpha$}
    \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_1_beta.eps}
        \caption{$\beta$}
    \end{subfigure}
    }
    \caption{EM algorithm was used to estimate the parameters of a $\CPoisson(1,1,1)$ random variable using 1\,000 simulated samples, repeated 10 times. The graphs show the log likelihood and the estimated parameters at each EM step for each repeat of the experiment. The dashed lines show the standard deviation of the Cram\'er-Rao lower bound.}
    \label{fig:compoundPoisson_convergence_1}
\end{figure}

\begin{figure}[p]
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_2_lnL.eps}
        \caption{Log likelihood}
    \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_2_lambda.eps}
        \caption{$\lambda$}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_2_alpha.eps}
        \caption{$\alpha$}
    \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_2_beta.eps}
        \caption{$\beta$}
    \end{subfigure}
    }
    \caption{EM algorithm was used to estimate the parameters of a $\CPoisson(10,1,1)$ random variable using 1\,000 simulated samples, repeated 10 times. The graphs show the log likelihood and the estimated parameters at each EM step for each repeat of the experiment. The dashed lines show the standard deviation of the Cram\'er-Rao lower bound.}
    \label{fig:compoundPoisson_convergence_2}
\end{figure}

\begin{figure}[p]
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_3_lnL.eps}
        \caption{Log likelihood}
    \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_3_lambda.eps}
        \caption{$\lambda$}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_3_alpha.eps}
        \caption{$\alpha$}
    \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_3_beta.eps}
        \caption{$\beta$}
    \end{subfigure}
    }
    \caption{EM algorithm was used to estimate the parameters of a $\CPoisson(1,100,1)$ random variable using 1\,000 simulated samples, repeated 10 times. The graphs show the log likelihood and the estimated parameters at each EM step for each repeat of the experiment. The dashed lines show the standard deviation of the Cram\'er-Rao lower bound.}
    \label{fig:compoundPoisson_convergence_3}
\end{figure}

\begin{figure}[p]
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_4_lnL.eps}
        \caption{Log likelihood}
    \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_4_lambda.eps}
        \caption{$\lambda$}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_4_alpha.eps}
        \caption{$\alpha$}
    \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_4_beta.eps}
        \caption{$\beta$}
    \end{subfigure}
    }
    \caption{EM algorithm was used to estimate the parameters of a $\CPoisson(100,100,1)$ random variable using 1\,000 simulated samples, repeated 10 times. The graphs show the log likelihood and the estimated parameters at each EM step for each repeat of the experiment. The dashed lines show the standard deviation of the Cram\'er-Rao lower bound.}
    \label{fig:compoundPoisson_convergence_4}
\end{figure}

\subsection{Failure Cases}

\chapter{Inference}

\bibliographystyle{apalike}
\bibliography{../bib}

\end{document}
