In the previous chapter, it was found that it was hard to fit a compound Poisson random variable onto the grey values directly itself. Results did show that the variance of the grey value has a linear relationship with the mean grey value. As a result, it would be possible to predict the variance of the grey value, given the grey value. This opens up new ways to predict the uncertainty of each individual pixel in an x-ray projection.

In this chapter, generalised linear models \citep{nelder1972generalized, nelder1972generalized_2, mccullagh1984generalized} with different link functions and polynomial explanatory variables were selected using forward step-wise selection. These selected models were compared using cross validation to find the best model.

\section{Generalised Linear Models}

The sample variance-mean data were obtained from the replicated projections. Let $x_{x,y}^{(r)}$ be the grey value of the pixel at $(x,y)$ from the $r$th replicate projection for $r=1,2,3,\dotdotdot,n$. The pixel sample mean is
\begin{equation}
    \overline{x}_{x,y}=\frac{1}{n}\sum_{r=1}^n x_{x,y}^{(r)}
\end{equation}
and the pixel sample variance is
\begin{equation}
    y_{x,y} =
    \frac{1}{n-1}
    \sum_{r=1}^n
        \left(
            x_{x,y}^{(r)} - \overline{x}_{x,y}
        \right)^2
    \ .
\end{equation}
Only pixels in the region of interest (ROI) were considered here, that is pixels which represent the test sample.

The aim is to model and predict the pixel variance given the pixel grey value by fitting a model onto the sample variance-mean data. No spatial information is used, thus the variance-mean data is denoted as $(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m)$ from now on, where $m$ is the area of the ROI or the size of the dataset. Let $Y(x)$ be a random variable and the sample variance given a grey value mean $x$. It was assumed the standard error from estimating the mean was negligible so that the uncertainty is captured by the random variable $Y$. It was assumed that for a given pixel, the grey values are Normal and i.i.d. Let $\sigma^2(x)$ be the variance given a grey value $x$, then it can be shown that
\begin{equation}
\dfrac{(n-1)Y(x)}{\sigma^2(x)}\sim\chi^2_{n-1}
\end{equation}
which results in
\begin{equation}
Y(x)\sim\gammaDist\left(\alpha,\dfrac{\alpha}{\sigma^2(x)}\right)
\end{equation}
where $\alpha=(n-1)/2$ is the shape parameter. The expectation and variance are
\begin{equation}
\expectation\left[Y(x)\right]=\sigma^2(x)
\end{equation}
and
\begin{equation}
\variance\left[Y(x)\right]=\dfrac{\sigma^2(x)}{\alpha}
\end{equation}
respectively.

This framework allows the use of generalised linear models (GLM) \citep{nelder1972generalized,nelder1972generalized_2, mccullagh1984generalized}. In the gamma distribution case, GLM can be used to model
\begin{equation}
Y(x)\sim\gammaDist\left(\alpha,\dfrac{\alpha}{g^{-1}(\eta(x))}\right)
\end{equation}
where $g(y)$ is the link function and $\eta(x)$ is a linear function called the systematic component. It should be noted that
\begin{equation}
  \expectation\left[Y(x)\right]=g^{-1}(\eta(x))
\end{equation}
which shows how the link function and systematic component work together. Examples of link functions are the identity link
\begin{equation}
g(y)=g^{-1}(y)=y
\end{equation}
and, for the gamma distribution case, the canonical link
\begin{equation}
g(y)=g^{-1}(y)=1/y \ .
\end{equation}
An example of a systematic component are polynomials $\eta(x)=\beta_0+\sum_{j=1}^{p-1}\beta_j x^j$ so that when used with the identity link, for example, $\expectation\left[Y(x)\right] = \beta_0+\sum_{j=1}^{p-1}\beta_j x^j$. Iterative reweighted least squares \citep{friedman2001elements} can be used to estimate the parameters $\beta_0, \beta_1, \cdots, \beta_{p-1}$ given data for the model to fit onto.

Once the parameters have been estimated, prediction of the variance given a grey value $x$ is done using the expectation $\widehat{y}(x) = \expectation\left[Y(x)\right] = g^{-1}(\eta(x))$.

\section{Model Selection}

This section describes how forward stepwise selection \citep{friedman2001elements} was used to select which polynomials to use in the systematic component.

In summary, forward stepwise selection fits a basic model to the data initially. A term is added to make the model more complicated at each step to improve the fit onto the data. This is continued until the model cannot be improved subject to overfitting. The Akaike information criterion (AIC) \citep{friedman2001elements} and the Bayesian information criterion (BIC) \citep{friedman2001elements} are criteria which can be used to assess the fit of the model at each step without overfitting to the data too much.

The AIC and BIC are given as
\begin{equation}
\AIC = 2p-2\ln L
\end{equation}
and
\begin{equation}
\BIC = p\ln m - 2\ln L
\end{equation}
respectively where $p$ is the number of terms in the systematic component and $\ln L$ is the log likelihood of the GLM. The model with the lowest AIC or BIC is preferred. GLM aims to maximise the log likelihood but the additional terms in the criteria penalise models with too many terms. The log likelihood is given as
\begin{equation}
  \ln L = \sum_{i=1}^m \left[
    \alpha\ln\alpha
    -\ln\Gamma(\alpha)
    -\alpha\ln \widehat{y}_i
    +(\alpha-1)\ln y_i
    -\frac{\alpha y_i}{\widehat{y}_i}
  \right]
  \ .
\end{equation}
where $\widehat{y}_i=\widehat{y}(x_i)$. It should be reminded that $\alpha=(n-1)/2$ was assumed to be known and does not need to be estimated.

The procedure is as follows. A criterion and a link function is chosen beforehand. In the initial step, a GLM with systematic component $\eta(x)=\beta_0$ is fitted and the criterion is recorded. In the next step, a polynomial term with one order higher is added to the systematic component $\eta(x)=\beta_0+\beta_1 x$, fitted and the criterion recorded. In addition, a polynomial term with one order lower is added $\beta(x)=\beta_{-1}x^{-1}+\beta_0$ and fitted separately with the criterion recorded. The model which decreases the criterion the most is accepted. Adding higher and lower order polynomials to the systematic component is repeated, for example after accepting $\eta(x)=\beta_0+\beta_1 x$, the following systematic components $\eta(x)=\beta_0+\beta_1 x+\beta_2x^2$ and $\eta(x)=\beta_{-1}x^{-1}+\beta_0+\beta_1 x$ are fitted and assessed. This is repeated until the criterion cannot be decreased and the procedure is left with the final model.

Forward stepwise selection was conducted on the datasets \texttt{AbsNoFilterDeg120} and \texttt{AbsFilterDeg120}. The procedure was repeated 100 times by using a random permutation with replacement of the replicated projections to obtain a different sample variance-mean data which introduces some variation to the data. The procedure was also repeated using various shading corrections to investigate the effects of shading correction on the variance-mean relationship.

\begin{sidewaystable}
\centering
\begin{tabular}{cc|ccc}
\multicolumn{2}{c|}{Identity Link}& null & bw & linear \\ \hline
\multirow{3}{*}{AIC} & order     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterNull_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterBw_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterLinear_identityorder.txt}     \\
                     & votes     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterNull_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterBw_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterLinear_identityvote.txt}      \\
                     & criterion & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterNull_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterBw_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterLinear_identitycriterion.txt} \\ \hline
\multirow{3}{*}{BIC} & order     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterNull_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterBw_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterLinear_identityorder.txt}     \\
                     & votes     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterNull_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterBw_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterLinear_identityvote.txt}      \\
                     & criterion & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterNull_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterBw_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterLinear_identitycriterion.txt} \\
\\
\multicolumn{2}{c|}{Canonical Link}& null & bw & linear \\ \hline
\multirow{3}{*}{AIC} & order      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterNull_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterBw_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterLinear_reciprocalorder.txt}     \\
                     & votes      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterNull_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterBw_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterLinear_reciprocalvote.txt}      \\
                     & criterion  & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterNull_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterBw_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterLinear_reciprocalcriterion.txt} \\ \hline
\multirow{3}{*}{BIC} & order      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterNull_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterBw_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterLinear_reciprocalorder.txt}     \\
                     & votes      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterNull_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterBw_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterLinear_reciprocalvote.txt}      \\
                     & criterion  & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterNull_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterBw_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterLinear_reciprocalcriterion.txt}     
\end{tabular}
\caption{Forward stepwise selection was used to find suitable polynomial orders when fitting a GLM onto the sample variance-mean data of the grey values from the projections in \texttt{AbsNoFilterDeg120}. The columns of the table represent different shading corrections used on the projections. Forward stepwise selection was repeated 100 times by using a different random permutation with replacement to obtain a different sample variance-mean data. The row labelled order shows the most common selected polynomial orders. The error bars are the standard deviation from the 100 repeats.}
\label{table:meanVar_glmselect_absnofilter}
\end{sidewaystable}

\begin{sidewaystable}
\centering
\begin{tabular}{cc|ccc}
\multicolumn{2}{c|}{Identity Link}& null & bw & linear \\ \hline
\multirow{3}{*}{AIC} & order     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterNull_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterBw_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterLinear_identityorder.txt}     \\
                     & votes     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterNull_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterBw_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterLinear_identityvote.txt}      \\
                     & criterion & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterNull_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterBw_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterLinear_identitycriterion.txt} \\ \hline
\multirow{3}{*}{BIC} & order     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterNull_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterBw_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterLinear_identityorder.txt}     \\
                     & votes     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterNull_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterBw_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterLinear_identityvote.txt}      \\
                     & criterion & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterNull_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterBw_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterLinear_identitycriterion.txt} \\
\\
\multicolumn{2}{c|}{Canonical Link}& null & bw & linear \\ \hline
\multirow{3}{*}{AIC} & order      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterNull_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterBw_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterLinear_reciprocalorder.txt}     \\
                     & votes      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterNull_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterBw_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterLinear_reciprocalvote.txt}      \\
                     & criterion  & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterNull_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterBw_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterLinear_reciprocalcriterion.txt} \\ \hline
\multirow{3}{*}{BIC} & order      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterNull_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterBw_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterLinear_reciprocalorder.txt}     \\
                     & votes      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterNull_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterBw_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterLinear_reciprocalvote.txt}      \\
                     & criterion  & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterNull_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterBw_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterLinear_reciprocalcriterion.txt}     
\end{tabular}
\caption{Forward stepwise selection was used to find suitable polynomial orders when fitting a GLM onto the sample variance-mean data of the grey values from the projections in \texttt{AbsFilterDeg120}. The columns of the table represent different shading corrections used on the projections. Forward stepwise selection was repeated 100 times by using a different random permutation with replacement to obtain a different sample variance-mean data. The row labelled order shows the most common selected polynomial orders. The error bars are the standard deviation from the 100 repeats.}
\label{table:meanVar_glmselect_absfilter}
\end{sidewaystable}

The results are shown in Tables \ref{table:meanVar_glmselect_absnofilter} and \ref{table:meanVar_glmselect_absfilter}. The models selected all have only two terms and are quite simple. Different shading correction or different criteria had no effect on the selected model. The method is quite robust to the variation introduced to the dataset when repeating the experiment because all 100 repeats consistently selected the same model.

There was some variation to the selected models between datasets. For example when using the identity link, \texttt{AbsNoFilterDeg120} prefers $\eta(x)=\beta_0+\beta_1 x$ whereas \texttt{AbsFilterDeg120} prefers $\eta(x)=\beta_{-1}x^{-1}+\beta_0$. Using the canonical link, both datasets selected $\eta(x)=\beta_{-1}x^{-1}+\beta_0$ which correspond to $\widehat{y}(x)=\left(\beta_{-1}x^{-1}+\beta_0\right)^{-1}$. Figures \ref{fig:meanVar_varMeanExample_AbsNoFilterDeg120} and \ref{fig:meanVar_varMeanExample_AbsFilterDeg120} shows the GLM fits. There is a leverage towards the low grey values because there are more low grey values in the dataset. As a result, the GLM does not capture the curvature towards the high grey values.

\begin{figure}
  \centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/varmean/varMeanExample_AbsNoFilterDeg120_identity000010.eps}
        \caption{$\widehat{y}(x)=\beta_0+\beta_1 x$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/varmean/varMeanExample_AbsNoFilterDeg120_reciprocal000100.eps}
        \caption{$\widehat{y}(x)=(\beta_{-1}x^{-1}\beta_0)^{-1}$}
    \end{subfigure}
    }
    \caption{Log frequency density histogram of the sample variance-mean data from \texttt{AbsNoFilterDeg120} with linear shading correction. The red solid line shows the GLM fit along with $\Phi(\pm 1)$ quantile error bars as dashed lines.}
    \label{fig:meanVar_varMeanExample_AbsNoFilterDeg120}
\end{figure}

\begin{figure}
  \centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/varmean/varMeanExample_AbsFilterDeg120_identity000100.eps}
        \caption{$\widehat{y}(x)=\beta_{-1}x^{-1}+\beta_0$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/varmean/varMeanExample_AbsFilterDeg120_reciprocal000100.eps}
        \caption{$\widehat{y}(x)=(\beta_{-1}x^{-1}+\beta_0)^{-1}$}
    \end{subfigure}
    }
    \caption{Log frequency density histogram of the sample variance-mean data from \texttt{AbsFilterDeg120} with linear shading correction. The red solid line shows the GLM fit along with $\Phi(\pm 1)$ quantile error bars as dashed lines.}
    \label{fig:meanVar_varMeanExample_AbsFilterDeg120}
\end{figure}

\section{Cross Validation}

Instead of assessing the model fit using the AIC and BIC, the performance on predicting the variance given a grey value can be assessed using cross validation \citep{friedman2001elements}. Cross validation assess the model to make predictions on data it has not seen before. This is done by randomly splitting the $m$ data points into two disjoint sets, the training set and the test set. It was chosen that the training and test set are of the same size. The model is fitted onto the training set. Afterwards, the fitted model predict the variances given the grey values in the test set which are compared to the actual variances. 

The mean scaled deviance was used to assess the performance of variance prediction. The deviance is defined to be
\begin{equation}
  D = 2\left(
    \ln L_\mathrm{s} - \ln L    
  \right)
\end{equation}
where $L$ and $L_\mathrm{s}$ are the likelihood and saturated likelihood respectively.  The saturated likelihood is obtained by replacing all $\widehat{y}_i$ with $y_i$ in the likelihood so that
\begin{equation}
  \ln L_\mathrm{s} = \sum_{i=1}^m \left[
    \alpha\ln\alpha
    -\ln\Gamma(\alpha)
    -\alpha\ln{y_i}
    +(\alpha-1)\ln y_i
    -\alpha
  \right]
  \ .
\end{equation}
Following from this, the deviance is
\begin{equation}
  D = 2\alpha
  \sum_{i=1}^m\left[
      \dfrac{
          y_i-\widehat{y}_i
      }
      {
          \widehat{y}_i
      }
      - \ln\left(\dfrac{y_i}{\widehat{y}_i}\right)
  \right]
  \ .
\end{equation}
The mean scaled deviance is obtained by removing the factor of $\alpha$ and dividing by $m$ to get
\begin{equation}
    D_\mathrm{s} = \dfrac{2}{m}
    \sum_{i=1}^m\left[
        \dfrac{
            y_i-\widehat{y}_i
        }
        {
            \widehat{y}_i
        }
        - \ln\left(\dfrac{y_i}{\widehat{y}_i}\right)
    \right]
    \ .
\end{equation}

\begin{figure}
  \centering
  \includegraphics[width=0.49\textwidth]{../figures/varmean/devianceGraph.eps}
  \caption{Scaled deviance loss function}
  \label{fig:meanVar_deviance}
\end{figure}

The mean scaled deviance is a loss function which increases as $y_i/\widehat{y}_i$ deviates from one, this is shown in Figure \ref{fig:meanVar_deviance} and it should be noted the $x$-axis is in log scale. Another way to show this is by letting $r_i = y_i/\widehat{y}_i$ and $d_i = 2\left[r-1-\ln r\right]$ be an element from the sum in the deviance. For $r_i\approx 1$, $\ln(r)\approx(r-1)-(r-1)^2/2$ which implies $d_i\approx (r-1)^2$ with minimum at one. For $r_i$ deviate greatly from one, the loss function is asymmetric. For example a ratio of $r_i=10^1$ has a greater penalty than $r_i=10^{-1}$. This means in extreme cases, overestimates are penalised less than underestimates relative to $y_i$. 

Assuming the model is correct, it is given that $D\sim\chi_{m-p}^2$ which implies that for large $m$,
\begin{equation}
\expectation\left[D_\mathrm{s}\right] = \dfrac{1}{\alpha}
\end{equation}
and
\begin{equation}
\variance\left[D_\mathrm{s}\right] = \dfrac{2}{\alpha^2 m} \ .
\end{equation}
Because $\alpha=(n-1)/2$, this shows that the number of replicated projections used to obtain the sample variance-mean data has an influence on the mean scaled deviance. This result can be used to estimate $\alpha$ if it is unknown.

Cross validation was performed on the datasets \texttt{AbsNoFilterDeg120} and \texttt{AbsFilterDeg120} with various different shading corrections. The models selected from forward step-wise selection in the previous section were assessed. They are $\widehat{y}(x)=\beta_0+\beta_1 x$ and $\widehat{y}(x)=\beta_{-1}x^{-1}+\beta_0$ using the identity link and $\widehat{y}(x)=(\beta_{-1}x^{-1}+\beta_0)^{-1}$ using the canonical link. The analysis was repeated 100 times by using a random permutation with replacement of the replicated projections to obtain a different sample variance-mean data which introduces some variation to the data.

\begin{figure}
  \centering
  \centerline{
  \begin{subfigure}[b]{0.49\textwidth}
      \includegraphics[width=\textwidth]{../figures/varmean/varMeanCv_AbsNoFilterDeg120_testmeanscaleddeviance.eps}
      \caption{\texttt{AbsNoFilterDeg120}}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\textwidth}
      \includegraphics[width=\textwidth]{../figures/varmean/varMeanCv_AbsFilterDeg120_testmeanscaleddeviance.eps}
      \caption{\texttt{AbsFilterDeg120}}
  \end{subfigure}
  }
  \caption{Test mean scaled deviance from predicting variances in a test set using a GLM fitted onto a training set. The box plot represent the 100 repeats of the analysis by by using a random permutation with replacement of the replicated projections to work out a different sample variance-mean data.}
  \label{fig:meanVar_varMeanCv}
\end{figure}

The results from the cross validation is shown in Figure \ref{fig:meanVar_varMeanCv}. The performance of the three candidate models were very similar. One exception is the model $\widehat{y}(x)=\beta_{-1}x^{-1}+\beta_0$ fitted onto the \texttt{AbsNoFilterDeg120} dataset, the mean scaled deviance was significantly larger and this is expected from a model not favoured in the forward step-wise selection in the previous section.

Shading correction, again, did not had an effect in the analysis.

From these results, it is recommended that the relationship $\widehat{y}(x)=\beta_0+\beta_1 x$ should be used for its simple form, similar performance to other candidate models and connections to the compound Poisson.

\section{Conclusion}

It was found that a gamma GLM with relationship $\widehat{y}(x)=\beta_0+\beta_1 x$ using the identity link was a good model. The simple linear relationship is attractive because it is justified by the compound Poisson model of x-ray photon behaviour. This model is also preferred by those who follows Occam’s razor.

Non-parametric and machine learning methods may be used in the prediction of variance for more flexible models. These models are flexible, however, they are slow and are unnecessary in a low dimensional problem with a large number of data points.