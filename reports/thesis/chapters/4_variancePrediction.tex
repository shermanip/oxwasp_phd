In the previous chapter, the grey values were modelled using a compound Poisson distribution to reflect the science of x-rays. This model has a linear relationship between the variance and the expectation. As a result, it would be possible to predict the variance of the grey value, given the grey value. This opens up new ways to predict the uncertainty of each pixel in an x-ray projection.

In this chapter, generalised linear models \citep{nelder1972generalized, nelder1972generalized_2, mccullagh1984generalized} with different link functions and polynomial features were selected using forward stepwise selection. These selected models were compared using cross-validation to find the best model and verified using residual analysis.

\section{Generalised Linear Models}

The sample variance-mean data were obtained from the replicate projections. Let $x_{i,j}$ be the grey value of the $i$th pixel, ignoring any spatial information, from the $j$th replicate projection for $i=1,2,3,\ldots,N$ and $j=1,2,3,\ldots,m$. The sample mean and sample variance grey value for the $i$th pixel are
\begin{equation}
    x_{i}=\frac{1}{m}\sum_{j=1}^m x_{i,j}
\end{equation}
and
\begin{equation}
    y_{i} =
    \frac{1}{m-1}
    \sum_{j=1}^m
        \left(
            x_{i,j} - \overline{x}_{i}
        \right)^2
\end{equation}
respectively. The symbol $y$ should not be confused with the latent variable in the previous chapter. Only pixels in the region of interest (ROI) were considered here, that is, pixels which represent the test sample. The ROI was created by manually segmenting the test sample from the projection.

The aim is to model and predict the grey value variance of a pixel given its grey value by using a model fitted onto the sample variance-mean data. The variance-mean data is denoted as $(x_1,y_1),(x_2,y_2),\ldots,(x_N,y_N)$ where $N$ is the area of the ROI or the size of the dataset. Let $Y(x)$ be a random variable and the sample variance given a grey value mean $x$. It was assumed that the standard error from estimating the mean was negligible so that the uncertainty is captured by the random variable $Y$. It was assumed that for a given pixel, the grey values are Normal and i.i.d. Let $\sigma^2(x)$ be the variance given a grey value $x$, then it can be shown that
\begin{equation}
\dfrac{(m-1)Y(x)}{\sigma^2(x)}\sim\chi^2_{m-1}
\end{equation}
which results in
\begin{equation}
Y(x)\sim\gammaDist\left(\alpha,\dfrac{\alpha}{\sigma^2(x)}\right)
\end{equation}
where $\alpha=(m-1)/2$ is the shape parameter. The expectation and variance are
\begin{equation}
\expectation\left[Y(x)\right]=\sigma^2(x)
\end{equation}
and
\begin{equation}
\variance\left[Y(x)\right]=\left(\dfrac{\sigma^2(x)}{\alpha}\right)^2
\end{equation}
respectively.

This framework allows the use of generalised linear models (GLM) \citep{nelder1972generalized,nelder1972generalized_2, mccullagh1984generalized}. In the gamma distribution case, a GLM can be used to model
\begin{equation}
Y(x)\sim\gammaDist\left(\alpha,\dfrac{\alpha}{g^{-1}(\eta(x))}\right)
\end{equation}
where $g(y)$ is the link function and $\eta(x)$ is a linear function called the systematic component. It should be noted that
\begin{equation}
  \expectation\left[Y(x)\right]=g^{-1}(\eta(x))
\end{equation}
which shows how the link function and systematic component work together. Examples of link functions are the identity link
\begin{equation}
g(y)=g^{-1}(y)=y
\end{equation}
and, for the gamma distribution case, the canonical link
\begin{equation}
g(y)=g^{-1}(y)=1/y \ .
\end{equation}

An example of a systematic component are polynomial features $\eta(x)=\sum_{r=0}^{p}\beta_r x^{r}$ so that when used with the identity link, for example, $\expectation\left[Y(x)\right] = \sum_{r=0}^{p}\beta_r x^{r}$. Iterative reweighted least squares \citep{friedman2001elements} can be used to estimate the parameters $\beta_0, \beta_1, \ldots, \beta_{p}$ given data for the model to fit onto.

Once the parameters have been estimated, prediction of the variance given a grey value $x$ is done by using $\widehat{y}(x) = g^{-1}(\widehat{\eta}(x))$ where $\widehat{\eta}(x)$ is the systematic component using the estimated parameters.

\section{Model Selection}

This section describes how forward stepwise selection \citep{efroymson1960multiple, friedman2001elements} was used to select which polynomial features to use in the systematic component.

In summary, forward stepwise selection fits a basic model to the data initially. A feature is added to make the model more complicated at each step to improve the fit onto the data. This is continued until the model cannot be improved subject to overfitting. The Akaike information criterion (AIC) \citep{akaike1974new, friedman2001elements} and the Bayesian information criterion (BIC) \citep{schwarz1978estimating, friedman2001elements} are criteria which can be used to assess the fit of the model at each step without overfitting to the data too much.

The AIC and BIC are given as
\begin{equation}
\AIC = 2k-2\ln L
\end{equation}
and
\begin{equation}
\BIC = k\ln N - 2\ln L
\end{equation}
respectively where $k$ is the number of parameters in the systematic component and $\ln L$ is the log-likelihood of the GLM. The model with the lowest AIC or BIC is preferred. GLM aims to maximise the log-likelihood but the additional terms in the criteria penalise models with too many terms. The log-likelihood is given as
\begin{equation}
  \ln L = \sum_{i=1}^N \left[
    \alpha\ln\alpha
    -\ln\Gamma(\alpha)
    -\alpha\ln \widehat{y}_i
    +(\alpha-1)\ln y_i
    -\frac{\alpha y_i}{\widehat{y}_i}
  \right]
  \ .
\end{equation}
where $\widehat{y}_i=\widehat{y}(x_i)$. $\alpha=(m-1)/2$ was assumed to be known so does not need to be estimated.

The procedure is as follows. A criterion and a link function were chosen beforehand. In the initial step, a GLM with systematic component $\eta(x)=\beta_0$ was fitted and the criterion was recorded. In the next step, a polynomial feature with one order higher was added to the systematic component $\eta(x)=\beta_0+\beta_1 x$, fitted and the criterion recorded. Also, a polynomial feature with one order lower was added $\beta(x)=\beta_{-1}x^{-1}+\beta_0$ and fitted separately with the criterion recorded. The model which decreased the criterion the most was accepted. Adding higher and lower order polynomials to the systematic component was repeated, for example, after accepting $\eta(x)=\beta_0+\beta_1 x$, the following systematic components $\eta(x)=\beta_0+\beta_1 x+\beta_2x^2$ and $\eta(x)=\beta_{-1}x^{-1}+\beta_0+\beta_1 x$ were fitted and assessed. The fitted parameters may change when adding more terms. This is repeated until the criterion cannot be decreased and the procedure is left with the final model.

Forward stepwise selection was conducted on the datasets \texttt{AbsNoFilter} and \texttt{AbsFilter}. The procedure was repeated 10 times by using a random permutation, with replacement, of the replicate projections to obtain a different sample variance-mean data which introduced some variation to the data \citep{efron1979bootstrap}. The procedure was also repeated using various shading corrections to investigate the effects of shading correction on the variance-mean relationship.

\begin{sidewaystable}
\centering
\begin{tabular}{ll|ccc}
\multicolumn{2}{c|}{Identity Link}& null & bw & linear \\ \hline
\multirow{3}{*}{AIC} & order     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg30null_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg30bw_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg30linear_identityorder.txt}     \\
                     & votes     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg30null_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg30bw_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg30linear_identityvote.txt}      \\
                     & criterion & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg30null_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg30bw_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg30linear_identitycriterion.txt} \\ \hline
\multirow{3}{*}{BIC} & order     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg30null_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg30bw_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg30linear_identityorder.txt}     \\
                     & votes     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg30null_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg30bw_identityvote.txt}       & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg30linear_identityvote.txt}      \\
                     & criterion & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg30null_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg30bw_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg30linear_identitycriterion.txt} \\
\\
\multicolumn{2}{c|}{Canonical Link}& null & bw & linear \\ \hline
\multirow{3}{*}{AIC} & order      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg30null_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg30bw_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg30linear_reciprocalorder.txt}     \\
                     & votes      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg30null_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg30bw_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg30linear_reciprocalvote.txt}      \\
                     & criterion  & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg30null_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg30bw_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg30linear_reciprocalcriterion.txt} \\ \hline
\multirow{3}{*}{BIC} & order      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg30null_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg30bw_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg30linear_reciprocalorder.txt}     \\
                     & votes      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg30null_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg30bw_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg30linear_reciprocalvote.txt}      \\
                     & criterion  & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg30null_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg30bw_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg30linear_reciprocalcriterion.txt}
\end{tabular}
\caption{Forward stepwise selection was used to find suitable polynomial features when fitting a gamma GLM onto the sample variance-mean data from the projections in \texttt{AbsNoFilter} at \ang{30}. The columns of the table represent different shading corrections. Forward stepwise selection was repeated 10 times by bootstrapping the replicate projections. `Order' shows the most commonly selected polynomial orders and `votes' shows the number of times it was selected out of the 10 repeats. The error bars are the standard deviation from the 10 repeats.}
\label{table:meanVar_glmselect_absnofilterdeg30}
\end{sidewaystable}

\begin{sidewaystable}
\centering
\begin{tabular}{ll|ccc}
\multicolumn{2}{c|}{Identity Link}& null & bw & linear \\ \hline
\multirow{3}{*}{AIC} & order     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg120null_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg120bw_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg120linear_identityorder.txt}     \\
                     & votes     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg120null_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg120bw_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg120linear_identityvote.txt}      \\
                     & criterion & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg120null_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg120bw_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg120linear_identitycriterion.txt} \\ \hline
\multirow{3}{*}{BIC} & order     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg120null_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg120bw_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg120linear_identityorder.txt}     \\
                     & votes     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg120null_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg120bw_identityvote.txt}       & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg120linear_identityvote.txt}      \\
                     & criterion & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg120null_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg120bw_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg120linear_identitycriterion.txt} \\
\\
\multicolumn{2}{c|}{Canonical Link}& null & bw & linear \\ \hline
\multirow{3}{*}{AIC} & order      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg120null_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg120bw_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg120linear_reciprocalorder.txt}     \\
                     & votes      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg120null_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg120bw_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg120linear_reciprocalvote.txt}      \\
                     & criterion  & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg120null_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg120bw_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsNoFilterDeg120linear_reciprocalcriterion.txt} \\ \hline
\multirow{3}{*}{BIC} & order      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg120null_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg120bw_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg120linear_reciprocalorder.txt}     \\
                     & votes      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg120null_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg120bw_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg120linear_reciprocalvote.txt}      \\
                     & criterion  & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg120null_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg120bw_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsNoFilterDeg120linear_reciprocalcriterion.txt}
\end{tabular}
\caption{Forward stepwise selection was used to find suitable polynomial features when fitting a gamma GLM onto the sample variance-mean data from the projections in \texttt{AbsNoFilter} at \ang{120}. The columns of the table represent different shading corrections. Forward stepwise selection was repeated 10 times by bootstrapping the replicate projections. `Order' shows the most commonly selected polynomial orders and `votes' shows the number of times it was selected out of the 10 repeats. The error bars are the standard deviation from the 10 repeats.}
\label{table:meanVar_glmselect_absnofilterdeg120}
\end{sidewaystable}

\begin{sidewaystable}
\centering
\begin{tabular}{ll|ccc}
\multicolumn{2}{c|}{Identity Link}& null & bw & linear \\ \hline
\multirow{3}{*}{AIC} & order     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg30null_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg30bw_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg30linear_identityorder.txt}     \\
                     & votes     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg30null_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg30bw_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg30linear_identityvote.txt}      \\
                     & criterion & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg30null_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg30bw_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg30linear_identitycriterion.txt} \\ \hline
\multirow{3}{*}{BIC} & order     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg30null_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg30bw_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg30linear_identityorder.txt}     \\
                     & votes     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg30null_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg30bw_identityvote.txt}       & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg30linear_identityvote.txt}      \\
                     & criterion & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg30null_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg30bw_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg30linear_identitycriterion.txt} \\
\\
\multicolumn{2}{c|}{Canonical Link}& null & bw & linear \\ \hline
\multirow{3}{*}{AIC} & order      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg30null_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg30bw_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg30linear_reciprocalorder.txt}     \\
                     & votes      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg30null_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg30bw_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg30linear_reciprocalvote.txt}      \\
                     & criterion  & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg30null_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg30bw_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg30linear_reciprocalcriterion.txt} \\ \hline
\multirow{3}{*}{BIC} & order      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg30null_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg30bw_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg30linear_reciprocalorder.txt}     \\
                     & votes      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg30null_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg30bw_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg30linear_reciprocalvote.txt}      \\
                     & criterion  & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg30null_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg30bw_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg30linear_reciprocalcriterion.txt}
\end{tabular}
\caption{Forward stepwise selection was used to find suitable polynomial features when fitting a gamma GLM onto the sample variance-mean data from the projections in \texttt{AbsFilter} at \ang{30}. The columns of the table represent different shading corrections. Forward stepwise selection was repeated 10 times by bootstrapping the replicate projections. `Order' shows the most commonly selected polynomial orders and `votes' shows the number of times it was selected out of the 10 repeats. The error bars are the standard deviation from the 10 repeats.}
\label{table:meanVar_glmselect_absfilterdeg30}
\end{sidewaystable}

\begin{sidewaystable}
\centering
\begin{tabular}{ll|ccc}
\multicolumn{2}{c|}{Identity Link}& null & bw & linear \\ \hline
\multirow{3}{*}{AIC} & order     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg120null_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg120bw_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg120linear_identityorder.txt}     \\
                     & votes     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg120null_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg120bw_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg120linear_identityvote.txt}      \\
                     & criterion & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg120null_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg120bw_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg120linear_identitycriterion.txt} \\ \hline
\multirow{3}{*}{BIC} & order     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg120null_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg120bw_identityorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg120linear_identityorder.txt}     \\
                     & votes     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg120null_identityvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg120bw_identityvote.txt}       & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg120linear_identityvote.txt}      \\
                     & criterion & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg120null_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg120bw_identitycriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg120linear_identitycriterion.txt} \\
\\
\multicolumn{2}{c|}{Canonical Link}& null & bw & linear \\ \hline
\multirow{3}{*}{AIC} & order      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg120null_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg120bw_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg120linear_reciprocalorder.txt}     \\
                     & votes      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg120null_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg120bw_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg120linear_reciprocalvote.txt}      \\
                     & criterion  & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg120null_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg120bw_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectAicAbsFilterDeg120linear_reciprocalcriterion.txt} \\ \hline
\multirow{3}{*}{BIC} & order      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg120null_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg120bw_reciprocalorder.txt}     & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg120linear_reciprocalorder.txt}     \\
                     & votes      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg120null_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg120bw_reciprocalvote.txt}      & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg120linear_reciprocalvote.txt}      \\
                     & criterion  & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg120null_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg120bw_reciprocalcriterion.txt} & \inputNumber{../figures/varmean/GlmSelectBicAbsFilterDeg120linear_reciprocalcriterion.txt}
\end{tabular}
\caption{Forward stepwise selection was used to find suitable polynomial features when fitting a gamma GLM onto the sample variance-mean data from the projections in \texttt{AbsFilter} at \ang{120}. The columns of the table represent different shading corrections. Forward stepwise selection was repeated 10 times by bootstrapping the replicate projections. `Order' shows the most commonly selected polynomial orders and `votes' shows the number of times it was selected out of the 10 repeats. The error bars are the standard deviation from the 10 repeats.}
\label{table:meanVar_glmselect_absfilterdeg120}
\end{sidewaystable}

The results are shown in Tables \ref{table:meanVar_glmselect_absnofilterdeg30} and \ref{table:meanVar_glmselect_absnofilterdeg120} for \texttt{AbsNoFilter} and Tables \ref{table:meanVar_glmselect_absfilterdeg30} and \ref{table:meanVar_glmselect_absfilterdeg120} for \texttt{AbsFilter}. The models selected are quite simple and all have two features. Different shading corrections or different criteria had no effect on the selected model. The method was quite robust to the variation introduced to the dataset when repeating the experiment because all 10 repeats consistently selected the same model.

There was some variation to the selected models between datasets. For example, when using the identity link, \texttt{AbsNoFilter} preferred $\eta(x)=\beta_0+\beta_1 x$ whereas \texttt{AbsFilter} preferred $\eta(x)=\beta_{-1}x^{-1}+\beta_0$. Using the canonical link, both datasets selected $\eta(x)=\beta_{-1}x^{-1}+\beta_0$ which correspond to $\widehat{y}(x)=\left(\beta_{-1}x^{-1}+\beta_0\right)^{-1}$.

Figures \ref{fig:meanVar_varMeanExample_AbsNoFilter} and \ref{fig:meanVar_varMeanExample_AbsFilter} shows the GLM fits for the datasets \texttt{AbsNoFilter} and \texttt{AbsFilter} respectively. The prediction intervals were obtained using the distribution $\widehat{Y}(x)\sim\gammaDist\left(\alpha, \dfrac{\alpha}{g^{-1}\left(\widehat{\eta}(x)\right)}\right)$. The fits all looked reasonable except for \texttt{AbsNoFilter} at \ang{120} where the fit did not capture the inflection. The forward stepwise selection may not picked this up because there were a lot of low grey values, causing a leverage towards these low grey values. As a result, the inflection did not stand out to the method.

\begin{figure}
  \centering
    \centerline{
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/varmean/varMeanExample_AbsNoFilterDeg30_identity000010.eps}
        \caption{${y}(x)=\beta_0+\beta_1 x$ at \ang{30}}
    \end{subfigure}
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/varmean/varMeanExample_AbsNoFilterDeg30_reciprocal000100.eps}
        \caption{${y}(x)=(\beta_{-1}x^{-1}\beta_0)^{-1}$ at \ang{30}}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/varmean/varMeanExample_AbsNoFilterDeg120_identity000010.eps}
        \caption{${y}(x)=\beta_0+\beta_1 x$ at \ang{120}}
    \end{subfigure}
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/varmean/varMeanExample_AbsNoFilterDeg120_reciprocal000100.eps}
        \caption{${y}(x)=(\beta_{-1}x^{-1}\beta_0)^{-1}$ at \ang{120}}
    \end{subfigure}
    }
    \caption{Log frequency density histogram of the sample variance-mean data from \texttt{AbsNoFilter} with linear shading correction. The solid red line shows the GLM fit along with the 68\% prediction interval as dashed lines. The colour scales are in units of $\log\SI{}{\adu^{-3}}$.}
    \label{fig:meanVar_varMeanExample_AbsNoFilter}
\end{figure}

\begin{figure}
  \centering
    \centerline{
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/varmean/varMeanExample_AbsFilterDeg30_identity000100.eps}
        \caption{${y}(x)=\beta_{-1}x^{-1}+\beta_0$ at \ang{30}}
    \end{subfigure}
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/varmean/varMeanExample_AbsFilterDeg30_reciprocal000100.eps}
        \caption{${y}(x)=(\beta_{-1}x^{-1}+\beta_0)^{-1}$ at \ang{30}}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/varmean/varMeanExample_AbsFilterDeg120_identity000100.eps}
        \caption{${y}(x)=\beta_{-1}x^{-1}+\beta_0$ at \ang{120}}
    \end{subfigure}
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/varmean/varMeanExample_AbsFilterDeg120_reciprocal000100.eps}
        \caption{${y}(x)=(\beta_{-1}x^{-1}+\beta_0)^{-1}$ at \ang{120}}
    \end{subfigure}
    }
    \caption{Log frequency density histogram of the sample variance-mean data from \texttt{AbsFilter} with linear shading correction. The solid red line shows the GLM fit along with the 68\% prediction interval as dashed lines. The colour scales are in units of $\log\SI{}{\adu^{-3}}$.}
    \label{fig:meanVar_varMeanExample_AbsFilter}
\end{figure}

\section{Cross-Validation}

Instead of assessing the model fit using the AIC and BIC, the performance on predicting the variance given a grey value was assessed using cross-validation \citep{allen1974relationship, stone1974cross, stone1977asymptotic, friedman2001elements}. Cross-validation assesses the model to make predictions on data it has not seen before. This was done by randomly splitting the $N$ data points into two disjoint sets, the training set and the test set $\tau$. The model was fitted onto the training set. Afterwards, the fitted model predicts the variances given the grey values in the test set which were then compared to the actual variances.

It was chosen that the training and test set are of the same size, a 50:50 spilt. The sizes of the sets can be different, for example, a 75:25 spilt. A 50:50 spilt was chosen because a large training set is an overkill for a high data and low dimensional problem. Here, a model with a few parameters is fitted onto millions of pixels.

The mean scaled deviance was used to assess the performance of variance prediction of the test set. The deviance is defined to be
\begin{equation}
  D = 2\left(
    \ln L_\mathrm{s} - \ln L_\tau
  \right)
\end{equation}
where
\begin{equation}
\ln L_\tau = \sum_{i\in\tau} \left[
    \alpha\ln\alpha
    -\ln\Gamma(\alpha)
    -\alpha\ln \widehat{y}_i
    +(\alpha-1)\ln y_i
    -\frac{\alpha y_i}{\widehat{y}_i}
  \right]
\end{equation}
is the log-likelihood of the test set given the fitted model and $\ln L_\mathrm{s}$ is the saturated log-likelihood. This is obtained by replacing all $\widehat{y}_i$ with $y_i$ in $\ln L_\tau$ so that
\begin{equation}
  \ln L_\mathrm{s} = \sum_{i\in\tau}\left[
    \alpha\ln\alpha
    -\ln\Gamma(\alpha)
    -\ln y_i
    -\alpha
  \right]
  \ .
\end{equation}
Following from this, the deviance is
\begin{equation}
  D = 2\alpha
  \sum_{i\in\tau}\left[
      \dfrac{
          y_i-\widehat{y}_i
      }
      {
          \widehat{y}_i
      }
      - \ln\left(\dfrac{y_i}{\widehat{y}_i}\right)
  \right]
  \ .
\end{equation}
The mean scaled deviance is obtained by removing the factor of $\alpha$ and dividing by $N_\tau$ to get
\begin{equation}
    D_\mathrm{s} = \dfrac{2}{N_\tau}
    \sum_{i\in\tau}\left[
        \dfrac{
            y_i-\widehat{y}_i
        }
        {
            \widehat{y}_i
        }
        - \ln\left(\dfrac{y_i}{\widehat{y}_i}\right)
    \right]
    \ .
\end{equation}

\begin{figure}
  \centering
  \includegraphics[width=\subSize]{../figures/varmean/devianceGraph.eps}
  \caption{The scaled deviance loss function}
  \label{fig:meanVar_deviance}
\end{figure}

The mean scaled deviance is a loss function which increases as $y_i/\widehat{y}_i$ deviates from one, this is shown in Figure \ref{fig:meanVar_deviance} and it should be noted that the $x$-axis is in log scale. Another way to show this is by letting $r_i = y_i/\widehat{y}_i$ and $d_i = 2\left[r_i-1-\ln r_i\right]$ be an element from the sum in the deviance. For $r_i\approx 1$, $\ln(r_i)\approx(r_i-1)-(r_i-1)^2/2$ which implies that $d_i\approx (r_i-1)^2$ with minimum at one. For $r_i$ deviate greatly from one, the loss function is asymmetric. For example a ratio of $r_i=10^1$ has a greater penalty than $r_i=10^{-1}$. This means that in extreme cases, overestimates are penalised less than underestimates relative to $y_i$.

Assuming the model is correct, it is given that $D\sim\chi_{N_\tau-k}^2$ which implies that for large $N_\tau$,
\begin{equation}
\expectation\left[D_\mathrm{s}\right] = \dfrac{1}{\alpha}
\end{equation}
and
\begin{equation}
\variance\left[D_\mathrm{s}\right] = \dfrac{2}{\alpha^2 N_\tau} \ .
\end{equation}
Because $\alpha=(m-1)/2$, this shows that the number of replicated projections used to obtain the sample variance-mean data has an influence on the mean scaled deviance. This result can be used to estimate $\alpha$ if it is unknown.

Cross validation was performed on the datasets \texttt{AbsNoFilter} and \texttt{AbsFilter} with various shading corrections. The models selected from forward stepwise selection in the previous section were assessed. They are ${y}(x)=\beta_0+\beta_1 x$ and ${y}(x)=\beta_{-1}x^{-1}+\beta_0$ using the identity link and ${y}(x)=(\beta_{-1}x^{-1}+\beta_0)^{-1}$ using the canonical link. The analysis was repeated 100 times by using a random permutation with replacement of the replicate projections to obtain a different sample variance-mean data which introduced some variation to the data \citep{efron1979bootstrap}.

\begin{figure}
  \centering
  \centerline{
  \begin{subfigure}[b]{\subSize}
      \includegraphics[width=\textwidth]{../figures/varmean/VarMeanCvAbsNoFilterDeg30_testmeanscaleddeviance.eps}
      \caption{\texttt{AbsNoFilter} at \ang{30}}
  \end{subfigure}
  \begin{subfigure}[b]{\subSize}
      \includegraphics[width=\textwidth]{../figures/varmean/VarMeanCvAbsNoFilterDeg120_testmeanscaleddeviance.eps}
      \caption{\texttt{AbsNoFilter} at \ang{120}}
  \end{subfigure}
  }
    \centerline{
  \begin{subfigure}[b]{\subSize}
      \includegraphics[width=\textwidth]{../figures/varmean/VarMeanCvAbsFilterDeg30_testmeanscaleddeviance.eps}
      \caption{\texttt{AbsFilter} at \ang{30}}
  \end{subfigure}
  \begin{subfigure}[b]{\subSize}
      \includegraphics[width=\textwidth]{../figures/varmean/VarMeanCvAbsFilterDeg120_testmeanscaleddeviance.eps}
      \caption{\texttt{AbsFilter} at \ang{120}}
  \end{subfigure}
  }
  \caption{Test mean scaled deviance from predicting variances in a test set using a GLM fitted onto a training set. The different colours represent different shading corrections. The boxplots represent the 100 repeats of the analysis by bootstrapping the replicate projections.}
  \label{fig:meanVar_varMeanCv}
\end{figure}

The results from the cross-validation is shown in Figure \ref{fig:meanVar_varMeanCv}. The performances of the three candidate models were very similar. One exception is the model ${y}(x)=\beta_{-1}x^{-1}+\beta_0$ fitted onto the \texttt{AbsNoFilter} dataset because the mean scaled deviance was significantly larger. This is expected from a model not favoured in the forward stepwise selection in the previous section.

Shading correction did not have a significant effect on the analysis.

From these results, it is recommended that the relationship ${y}(x)=\beta_0+\beta_1 x$ should be used for its simple form, similar performance to other candidate models and connections to the compound Poisson.

\section{Residual Analysis}

Residual analysis was conducted to study for anything overlooked. The model ${y}(x)=\beta_0+\beta_1 x$ was fitted onto the entire sample variance-mean data. A residual plot plots $r_i=y_i-\widehat{y}(x_i)$ for all $x_i$ in the dataset which was done using a 2D histogram. Due to the gamma GLM, higher grey values typically have bigger residuals in magnitude. A prediction residual interval is included in the histogram to aid judging the residuals. This interval was acquired by obtaining the prediction interval and subtracting it from the fit.

\begin{figure}
  \centering
  \centerline{
  \begin{subfigure}[b]{\subSize}
      \includegraphics[width=\textwidth]{../figures/varmean/varMeanResidualAbsNoFilterDeg30_vsgreyvalue.eps}
      \caption{\texttt{AbsNoFilter} at \ang{30}}
  \end{subfigure}
  \begin{subfigure}[b]{\subSize}
      \includegraphics[width=\textwidth]{../figures/varmean/varMeanResidualAbsNoFilterDeg120_vsgreyvalue.eps}
      \caption{\texttt{AbsNoFilter} at \ang{120}}
  \end{subfigure}
  }
  \centerline{
  \begin{subfigure}[b]{\subSize}
      \includegraphics[width=\textwidth]{../figures/varmean/varMeanResidualAbsFilterDeg30_vsgreyvalue.eps}
      \caption{\texttt{AbsFilter} at \ang{30}}
  \end{subfigure}
  \begin{subfigure}[b]{\subSize}
      \includegraphics[width=\textwidth]{../figures/varmean/varMeanResidualAbsFilterDeg120_vsgreyvalue.eps}
      \caption{\texttt{AbsFilter} at \ang{120}}
  \end{subfigure}
  }
  \caption{Log frequency density histogram of the residuals given the mean grey value. The residuals are from fitting a gamma GLM of the form ${y}(x)=\beta_0+\beta_1 x$ onto the sample variance-mean data obtained from the replicate projections. The dashed red line shows the 68\% prediction residual interval. The colour scales are in units of $\log\SI{}{\adu^{-3}}$.}
  \label{fig:meanVar_residualGreyvalue}
\end{figure}

The residual plots are shown in Figure \ref{fig:meanVar_residualGreyvalue}. The residuals are all sensible and increased in magnitude with the grey value which is captured by the prediction residual interval. The inflection in \texttt{AbsNoFilter} at \ang{120}, as discussed previously, can be seen more clearly here.

\begin{figure}
  \centering
  \centerline{
  \begin{subfigure}[b]{\subSize}
      \includegraphics[width=\textwidth]{../figures/varmean/varMeanResidualAbsNoFilterDeg30_spatial.eps}
      \caption{\texttt{AbsNoFilter} at \ang{30}}
  \end{subfigure}
  \begin{subfigure}[b]{\subSize}
      \includegraphics[width=\textwidth]{../figures/varmean/varMeanResidualAbsNoFilterDeg120_spatial.eps}
      \caption{\texttt{AbsNoFilter} at \ang{120}}
  \end{subfigure}
  }
  \centerline{
  \begin{subfigure}[b]{\subSize}
      \includegraphics[width=\textwidth]{../figures/varmean/varMeanResidualAbsFilterDeg30_spatial.eps}
      \caption{\texttt{AbsFilter} at \ang{30}}
  \end{subfigure}
  \begin{subfigure}[b]{\subSize}
      \includegraphics[width=\textwidth]{../figures/varmean/varMeanResidualAbsFilterDeg120_spatial.eps}
      \caption{\texttt{AbsFilter} at \ang{120}}
  \end{subfigure}
  }
  \caption{Absolute value residuals from fitting a gamma GLM of the form ${y}(x)=\beta_0+\beta_1 x$ onto the sample variance-mean data obtained from the replicate projections. Highlighted in a dashed red box is an example of an inflection. The colour scales are in units of \SI{}{\adu^2}.}
  \label{fig:meanVar_spatialResidual}
\end{figure}

Because no spatial information was used in modelling the variance, the residuals were plotted on top of the projections to look for any spatial structure, this is shown in Figure \ref{fig:meanVar_spatialResidual}. Because of the geometry of the test sample, lower grey values, thus smaller residuals, were found in the middle of the projection. Similarly, higher grey values, thus bigger residuals, were found on the edges of the test sample. Considering that, the residuals do not show any striking spatial structure. Again, the inflection in \texttt{AbsNoFilter} at \ang{120} can be seen more clearly here.

\section{Conclusion}

Various parametric models were investigated. A gamma GLM with identity and canonical link functions were looked at with different polynomial features. In terms of cross-validation, most of the models looked at have similar prediction performance when predicting the variance given a grey value. Thus, the relationship $\widehat{y}(x)=\beta_0+\beta_1 x$ using the identity link is attractive for its simplicity and links with the compound Poisson model.

It was found that shading correction had little effect on the variance-mean relationship.

As discussed in the compound Poisson chapter, a linear relationship can be derived by assuming no beam hardening, then it should be expected that a linear relationship is observed in the dataset \texttt{AbsFilter}. In the forward stepwise selection, \texttt{AbsNoFilter} consistently selected the linear model, given the identity link function. \texttt{AbsFilter} did not select the linear model but in terms of cross-validation, the linear model was just as good as the other models.

It was unusual to see an inflection in the sample variance-mean data in \texttt{AbsNoFilter} at \ang{120} which was not captured by the GLM. A good explanation was not found but perhaps the absence of an x-ray filter may have contributed to this. As a result, the \texttt{AbsNoFilter} dataset was discarded in favour of the \texttt{AbsFilter} dataset from the next chapter.

There exist model selection methods such as lasso and elastic net \citep{tibshirani1996regression, zou2005regularization, friedman2010regularization} but they are catered for high dimensional problems involving hundreds of parameters. In this problem, only a few parameters were needed to find a good model thus forward stepwise selection was sufficient. More flexible models such as non-parametric and machine learning methods may be used, however, they are slow and are unnecessary in a low dimensional problem with a large number of data points.

With the ability to predict the variance given a grey value of a pixel, the uncertainty can be quantified. In the next chapter, the projection is compared to a simulated projection in the face of that uncertainty.
