The grey value of each pixel in the detector can be modelled as a random variable, due to the random behaviour of photons being produced, interacting with the test sample and the scintillator in the detector. By modelling using a random variable, the uncertainty can be quantified and considered when conducting inference about any detected defects.

The compound Poisson distribution is studied here because of the compound Poission-like behaviour from the detection of photons. It is defined by defining a latent variable $Y\sim\poisson(\lambda)$ with probability mass function (p.m.f.)
\begin{equation}
  \prob(Y=y)=\euler^{-\lambda}\frac{\lambda^y}{y!} \quad \text{for }y=0,1,2,\cdots
\end{equation}
where $\lambda>0$ is the Poisson rate parameter. Let $U_i$ be some independent and identically distributed (i.i.d.) latent random variables with probability density function (p.d.f.) $p_U(u)$ for $i=1,2,3,\dotdotdot$. Let $X$ be a compound Poisson random variable where
\begin{equation}
  X|Y = \sum_{i=1}^{Y}U_i \ .
  \label{eq:compoundPoisson_X|Y}
\end{equation}
The p.d.f.~of $X$ can be obtained by marginalising the joint p.d.f.
\begin{equation}
  p_X(x)=\sum_{y=0}^\infty p_{X|Y}(x|y)\prob(Y=y) \quad\text{for }x\geqslant 0
  \ .
\end{equation}
It should be noted that $X=0$ if and only if $Y=0$ with probability $\prob(X=0)=\euler^{-\lambda}$. Then $X$ has probability mass at $X=0$ and probability density at $X>0$ which results in the p.d.f.
\begin{equation}
  p_X(x) = 
  \begin{cases}
    \delta(x) \euler^{-\lambda}  & \text{ for } x=0 \\ 
    \sum_{y=1}^\infty p_{X|Y}(x|y)\euler^{-\lambda}\frac{\lambda^y}{y!} \quad\text{for } & \text{ for } x>0
  \end{cases}
\end{equation}
where $\delta(x)$ is the Dirac delta function.

This chapter starts with a literature review on the compound Poisson distribution, how it is derived from the behaviour of photons, how its likelihood is evaluated and methods for fitting onto data. A model was proposed for the pixel's grey values and the expectation-maximization (EM) algorithm was implemented to fit the model onto data. It was found that for high photon rate, there were identifiability issues. The chapter is concluded on a discussion on why the EM algorithm failed.

\section{Literature Review}

\subsection{Compound Poisson in X-ray Detection}

In an x-ray tube, photons are emitted as a Poisson process and each photon has some random energy due to bremsstrahlung and characteristic radiation. This shares similarities to the compound Poisson. Let $Y$ be the number of photons emitted for some time exposure $\tau$, then $Y\sim\poisson(\lambda)$. Each photon is assumed to be i.i.d.~with energy $U_i$ for $i=1,2,3,\cdots$. $U_i$ has p.d.f.~$p_U(u)$. The random variables discussed here covers all the latent variables in the compound Poisson.

Photons emitted from the x-ray tube undergo attenuation when propagating through the test sample. Assuming no beam hardening, some photons are either absorbed or scattered, making them undetectable. Scattered photons may be detected but it is very rare. Attenuated photon energy remains unaffected so attenuation decreases the parameter $\lambda$ and the amount it decreases by depends on the attenuation coefficient of the material and the amount of material the x-ray attenuates. The parameters of the random variable $U_i$ remains unchanged because the energy of each photon remains the same after attenuation, assuming no beam hardening.

When the photons interact with the scintillator in the detector, they are converted into visible light. The visible light photons are then detected and converted into a a grey value. A quantum counter set the digital signal to be linear with to the number of photons detected \citep{whiting2006properties}. Let $X$ be the grey value observed, then
\begin{equation}
X = bY + \epsilon
\end{equation}
where $\epsilon\sim\normal(a,\kappa)$, $b$ and $a$ are some constant and $\kappa$ is the variance of electronic noise. In a quantum counter, the mean and variance of the grey value are
\begin{equation}
\expectation\left[X\right] = b\lambda + a
\end{equation}
and
\begin{equation}
\variance\left[X\right] = b^2\lambda + \kappa
\end{equation}
respectively. By eliminating $\lambda$
\begin{equation}
\variance\left[X\right] = b\expectation\left[X\right]+\kappa-ab \ ,
\end{equation}
a linear relationship between the variance and expectation of the grey value is obtained.

An energy integrating detector records the grey value which is linear to the energy detected \citep{whiting2006properties}. The grey value $X$ is
\begin{equation}
X|Y = \sum_{i=1}^Y U_i + \epsilon \ .
\end{equation}
This is the compound Poisson with Normal noise added to it. The scale factor $b$ is not included as this can be absorbed into $U$. Using the result that $\expectation\left[X\right]=\expectation\expectation\left[X|Y\right]$ and $\variance\left[X\right] = \variance\expectation\left[X|Y\right] + \expectation\variance\left[X|Y\right]$, the mean and variance of the grey value are
\begin{equation}
\expectation\left[X\right] = \lambda\expectation\left[U\right]+a
\end{equation}
and
\begin{equation}
\variance\left[X\right] = \lambda \expectation\left[U^2\right]+\kappa
\end{equation}
respectively. Eliminating $\lambda$ obtains
\begin{equation}
\variance\left[X\right] = \dfrac{\expectation\left[U^2\right]}{\expectation\left[U\right]} \expectation\left[X\right] + \kappa - a\dfrac{\expectation\left[U^2\right]}{\expectation\left[U\right]} \ .
\end{equation}
By assuming no beam hardening, $\expectation\left[U\right]$ and $\expectation\left[U^2\right]$ remains constant, thus there is a linear relationship between the variance and expectation of the grey value. There are other types of detection schemes \citep{whiting2006properties} but it shall be not be considered here.

Experiments have been done to verify the compound Poisson nature of the detector. This was done by investigating the variance of radiographs of air \citep{hsieh2015compound} and a polyethylene cylinder \citep{yang2009evaluation} \citep{yang2010noise} at different voltages and powers. It was found there were 2 components in the noise, one was signal dependent and comes from the compound Poisson, the other was signal independent and may be electronic noise. The electronic noise can be modelled as Normally distributed \citep{xu2009electronic}.

\subsection{Moment Generating Function}

Returning back to the compound Poisson with no electronic noise $X|Y = \sum_{i=1}^{Y}U_i$. Let the moment generating function (m.g.f.) of $X$ be
\begin{equation}
  M_X(\theta)=\expectation\left[\euler^{X\theta}\right]
  \ .
\end{equation}
This can be computed using the result for conditional expectations $M_X(\theta)=\expectation\expectation\left[\euler^{X\theta}|Y\right]$. Using the definition of $X|Y$ in Equation \eqref{eq:compoundPoisson_X|Y}, then
\begin{align}
  M_X(\theta)&=\expectation\expectation\left[\exp\left(\theta U_1 + \theta U_2 + \dotdotdot + \theta U_Y\right)|Y\right]
  \\
  &=\expectation\expectation\left[\euler^{\theta U_1}\cdot\euler^{\theta U_2}\cdot\dotdotdot\cdot\euler^{\theta U_Y}|Y\right]
  \ .
\end{align}
But because $U_i$ for $i=1,2,3,\dotdotdot$ are i.i.d., then each $U_i$ has a common m.g.f.~$M_U(\theta)=\expectation\left[\euler^{U\theta}\right]$, then
\begin{align}
  M_X(\theta)&=\expectation\left(
    \expectation\left[\euler^{\theta U_1}|Y\right]\cdot
    \expectation\left[\euler^{\theta U_2}|Y\right]\cdot
    \dotdotdot \cdot
    \expectation\left[\euler^{\theta U_Y}|Y\right]
  \right)
  \\
  &=\expectation\left[\left(M_U(\theta)\right)^Y\right]
  \\
  &=\expectation\left[\euler^{Y\ln(M_U(\theta))}\right]
  \\
  & = M_Y\left(\ln(M_U(\theta)\right)
\end{align}
where $M_Y(\theta)$ is the m.g.f.~of $Y$. It can be shown that the m.g.f.~of $Y$ is
$
  M_Y(\theta)=\expectation\left[\euler^{Y\theta}\right]=
  \exp
  \left[
    \lambda
    \left(
      \euler^\theta-1
    \right)
  \right]
$
then
\begin{equation}
  M_X(\theta)=
  \exp\left[
    \lambda
    \left(
      M_U(\theta)-1
    \right)
  \right]
  \ .
\end{equation}

Moments of $X$ can be obtained from the m.g.f.~by differentiating it and setting it to zero
\begin{align}
  M_X'(\theta)&=\exp\left[\lambda\left(M_U(\theta)-1\right)\right]\cdot\lambda M_U'(\theta) \\
  &=M_X(\theta)\lambda M_U'(\theta)
\end{align}
then
\begin{equation}
  \expectation\left[X\right]=\lambda\expectation\left[U\right]
  \ .
\end{equation}
Conducting the same procedure
\begin{equation}
  M_X''(\theta)=M_X'(\theta)\lambda M_U'(\theta)+M_X(\theta)\lambda M_U''(\theta)
  \ ,
\end{equation}
the variance can be obtained
\begin{align}
  \variance\left[X\right]&=M_X''(0)-\left[M_X'(0)\right]^2
  \\
  &=M_X'(0)\lambda M_U'(0)+M_X(0)\lambda M_U''(0)-\left[\lambda\expectation\left[U\right]\right]^2
  \\
  &=\lambda^2 \left(\expectation\left[U\right]\right)^2+\lambda \expectation\left[U^2\right]-\left[\lambda\expectation\left[U\right]\right]^2
\end{align}
resulting in
\begin{equation}
  \variance\left[X\right] = \lambda\expectation\left[U^2\right]
  \ .
\end{equation}
This can be extended for higher moments.

Because the m.g.f.~can be written down in closed form, so is the characterisation function or Fourier transform of the p.d.f.~of $X$. By obtaining an empirical version of $p_U(u)$, moments of $X$ can be estimated using the m.g.f.~and the p.d.f.~can be estimated by using fast Fourier transform on the empirical characteristic function \citep{whiting2006properties}.

\subsection{Generalised Linear Model}

A special case of the compound Poisson is when $U\sim\gammaDist\left(\alpha,\beta\right)$ where $\alpha>0$ is the gamma shape parameter and $\beta>0$ is the gamma rate parameter. The special case is used for example in \cite{xu2009electronic}. This is known as the compound Poisson-gamma distribution and denoted as $X\sim\CPoisson(\lambda,\alpha,\beta)$ and has p.d.f.
\begin{equation}
  p_X(x) = 
  \begin{cases}
    \delta(x) \euler^{-\lambda} & \text{ for } x=0 \\ 
    \sum_{y=1}^{\infty}\frac{\beta^{y\alpha}}{\Gamma(y\alpha)}x^{y\alpha-1}\euler^{-\beta x}\euler^{-\lambda}\frac{\lambda^y}{y!} & \text{ for } x>0
  \end{cases}
  \ .
  \label{eq:compoundPoisson_pdf}
\end{equation}
The conditional distribution can be shown to be $X|Y\sim\gammaDist\left(Y\alpha,\beta\right)$.

It can be shown that the compound Poisson-gamma distribution is in the exponential family for fixed $\alpha$ \citep{jorgensen1987exponential}. To show this, it is useful to reparametrize the compound Poisson-gamma distribution using the following:
\begin{equation}
  p=\frac{2+\alpha}{1+\alpha}
  \ ,
\end{equation}
\begin{equation}
  \mu=\frac{\lambda\alpha}{\beta}
  \ ,
\end{equation}
\begin{equation}
  \phi = \frac{\alpha+1}{\beta^{2-p}(\lambda\alpha)^{p-1}}
  \ .
\end{equation}
The parameters $p$, $\mu$ and $\phi$ are called the index, mean and dispersion parameters respectively. It can be shown that $1<p<2$, $\mu>0$ and $\phi>0$.

By rearranging the parameters to get
\begin{equation}
  \lambda=\frac{\mu^{2-p}}{\phi(2-p)}
\end{equation}
\begin{equation}
  \alpha=\frac{2-p}{p-1}
\end{equation}
\begin{equation}
  \beta=\frac{1}{\phi(p-1)\mu^{p-1}}
\end{equation}
and substituting it into Equation \eqref{eq:compoundPoisson_pdf}, the p.m.f.~at zero can be shown to be
\begin{equation}
  \prob(X=0) = \exp
  \left[
      -\frac{\mu^{2-p}}{\phi(2-p)}
  \right]
\end{equation}
and the p.d.f.~for $x>0$ is
\begin{multline}
  p_X(x) = \sum_{y=1}^{\infty}
  \left[
    \frac{1}{\phi(p-1)\mu^{p-1}}
  \right]^{y\alpha}
  \frac{1}{\Gamma(y\alpha)}
  x^{y\alpha-1}
  \exp\left[
      -\frac{x}{\phi(p-1)\mu^{p-1}}
  \right]
  \\
  \exp\left[
      -\frac{\mu^{2-p}}{\phi(2-p)}
  \right]
  \left[
    \frac{\mu^{2-p}}{\phi(2-p)}
  \right]^y
  \frac{1}{y!}
  \ .
\end{multline}
Tidying up the equation
\begin{multline}
  p_X(x) = 
  \exp\left[
    \frac{1}{\phi}\left(x\frac{\mu^{1-p}}{1-p}-\frac{\mu^{2-p}}{2-p}\right)
  \right]
  \frac{1}{x}
  \\
  \sum_{y=1}^{\infty}\frac{x^{y\alpha}\mu^{y[2-p-\alpha(p-1)]}}{\phi^{y(1+\alpha)}(p-1)^{y\alpha}(2-p)^yy!\Gamma(y\alpha)}
  \ .
\end{multline}
To simplify further, it should be noted that $2-p-\alpha(p-1) = 2-p - \frac{2-p}{p-1}(p-1) =0$ so that
\begin{equation}
  p_X(x) = 
  \exp\left[
    \frac{1}{\phi}
    \left(
      x\frac{\mu^{1-p}}{1-p}-\frac{\mu^{2-p}}{2-p}
    \right)
  \right]
  \frac{1}{x}
  \sum_{y=1}^{\infty}W_y(x,p,\phi)
\end{equation}
where
\begin{equation}
  W_y(x,p,\phi)=\frac{x^{y\alpha}}{\phi^{y(1+\alpha)}(p-1)^{y\alpha}(2-p)^yy!\Gamma(y\alpha)}
  \ .
\end{equation}
This is in the form of a generalised linear model \citep{nelder1972generalized, nelder1972generalized_2, mccullagh1984generalized} for fixed $p$ or $\alpha$ because the above is in the form of a distribution in the dispersive exponential family. Parameter estimation then can be done via the generalised linear model framework and can be extended to include linear mixed models \citep{zhang2013likelihood}. These has applications in for example insurance claim data \citep{jorgensen1994fitting, smyth2002fitting}.

Estimating $p$ is difficult and various methods were discussed by \cite{zhang2013likelihood}. One easy way is to estimate $\mu$ and $\phi$ on a grid of $p$'s and then select the $p$ which maximises the likelihood \citep{dunn2005series}.

One special property of the compound Poisson-gamma distribution is that it is in the Tweedie dispersion exponential family \citep{jorgensen1987exponential}. It has a special variance mean relationship
\begin{equation}
  \variance[X] = \phi \mu^p
\end{equation}
where, as a reminder, $1<p<2$. This can be derived using the partition function $Z$. Let
\begin{equation}
  \theta = \frac{\mu^{1-p}}{1-p}
\end{equation}
and
\begin{equation}
  \ln Z = \frac{1}{\phi(2-p)\theta^\alpha(1-p)^\alpha} \ ,
\end{equation}
then
\begin{equation}
  \variance[X] = \phi^2 \frac{\partial^2\ln Z}{\partial\theta^2} \ .
\end{equation}

\subsection{Method of Moments}

The method of moments is a simpler method to estimate the parameters of a compound Poisson-gamma random variable. The m.g.f.~of the of $X$ is
\begin{equation}
  M_X(\theta)=\exp\left[\lambda\left(\left(\frac{\beta}{\beta-\theta}\right)^{\alpha}-1\right)\right]
\end{equation}
and moments can be obtained from it such as
\begin{equation}
  \expectation\left[X\right]=\frac{\alpha\lambda}{\beta}
\end{equation}
\begin{equation}
  \variance\left[X\right]=\frac{\alpha(\alpha+1)\lambda}{\beta^2}
  \label{eq:compoundPoisson_variance}
\end{equation}
and
\begin{equation}
  \expectation\left[(X-\expectation[X])^3\right] = \frac{\alpha(\alpha+1)(\alpha+2)\lambda}{\beta^3}
  \ .
\end{equation}

Suppose $\widehat{\mu}$ is an estimator of $\expectation[X]$ and $\widehat{\mu}_j$ is an estimator of $\expectation\left[\left(X-\expectation[X]\right)^j\right]$ for $j=2,3$. Then the estimators
\begin{equation}
  \widehat{\lambda}=\frac{\widehat{\mu}^2\widehat{\mu}_2}{\left(2\widehat{\mu}_2^2-\widehat{\mu}_3\widehat{\mu}\right)}
\end{equation}
\begin{equation}
  \widehat{\alpha}=\frac{\widehat{\mu}_3\widehat{\mu}-2\widehat{\mu}_2^2}{\widehat{\mu}_2^2-\widehat{\mu}\widehat{\mu}_3}
\end{equation}
\begin{equation}
  \widehat{\beta}=\frac{\widehat{\mu}\widehat{\mu}_2}{\widehat{\mu}\widehat{\mu}_3-\widehat{\mu}_2^2}
\end{equation}
are method of moments estimators of $\lambda$, $\alpha$ and $\beta$ respectively \citep{withers2011compound}. These estimators suffer because estimation is not done through the sufficient statistics and can be negative, this is a problem because the parameters do not take non-positive values.

\subsection{Normal Approximation}

The evaluation of the density of a compound Poisson-gamma distribution is useful so that the likelihood can be obtained. The likelihood then can be used to find, for example, maximum likelihood estimators. A problem occurs when dealing with the infinite sum in the p.d.f.~because it cannot be simplified. There are a number of approximations or computational methods to evaluate the p.d.f.~such as Fourier inverting the characteristic function \citep{dunn2008evaluation}, using the saddlepoint approximation \citep{daniels1954saddlepoint} or cleverly sum over certain terms in the infinite sum \citep{dunn2005series}. Monte Carlo methods can be used to evaluate the p.d.f.~by simulating compound Poisson-gamma random variables.

The m.g.f.~of $X$ is
\begin{equation}
M(\theta)=\exp\left[\lambda\left(\left(\frac{\beta}{\beta-\theta}\right)^{\alpha}-1\right)\right]
\ .
\end{equation}
It provides a starting point to what limiting distributions the compound Poisson-gamma distribution converges to for large parameters. These limiting distributions then can be used to approximate the p.d.f.~of a compound Poisson-Gamma random variable. The m.g.f.~of $X$ itself it not useful because when considering $\lambda\rightarrow\infty$ or $\alpha\rightarrow\infty$ then $M_X(\theta)\rightarrow\infty$. Also for $\beta\rightarrow\infty$, then $M_X(\theta)\rightarrow 1$ which is not useful either.

The compound Poisson-gamma random variable $X$ can be standardised to obtain useful limiting results from the m.g.f. Let
\begin{equation}
  Z = \frac{X-\expectation[X]}{\sqrt{\variance[X]}}
  \ ,
\end{equation}
then using previous results
\begin{equation}
  Z = bX+a
\end{equation}
where
\begin{equation}
  b = \frac{\beta}{\sqrt{\alpha(\alpha+1)\lambda}}
\end{equation}
and
\begin{equation}
  a = -\sqrt{\frac{\alpha\lambda}{\alpha+1}}
  \ .
\end{equation}

The m.g.f.~of $Z$ is then
\begin{align}
  M_Z(\theta)&=\expectation\left[\euler^{Z\theta}\right]
  \\
  &=\expectation\left[\euler^{(bX+a)\theta}\right]
  \\
  &=\euler^{a\theta}M_X(b\theta)
  \ .
\end{align}
Substituting in the values
\begin{align}
  M_Z(\theta)&=
  \exp\left(
      -\theta\sqrt{\frac{\alpha\lambda}{\alpha+1}}
    \right)
  \exp\left[
    \lambda
    \left(
      \left(
        \frac{\beta}{\beta-\frac{\beta\theta}{\sqrt{\alpha(\alpha+1)\lambda}}}
      \right)^\alpha
      -1
    \right)
  \right]
  \\
  &=
  \exp\left(
      -\theta\sqrt{\frac{\alpha\lambda}{\alpha+1}}
    \right)
  \exp\left[
    \lambda
    \left(
      \left(
        \frac{\sqrt{\alpha(\alpha+1)\lambda}}{\sqrt{\alpha(\alpha+1)\lambda}-\theta}
      \right)^\alpha
      -1
    \right)
  \right]
\end{align}
or in a different form
\begin{equation}
	M_Z(\theta)=
	\exp\left(
	    -\theta\sqrt{\frac{\alpha\lambda}{\alpha+1}}
    \right)
	\exp\left[
		\lambda
		\left(
			\left(
				1-\frac{\theta}{\sqrt{\alpha(\alpha+1)\lambda}}	
			\right)^{-\alpha}
			-1
		\right)
	\right]
	\ .
\end{equation}

The above form is useful so that the binomial expansion can be conducted, that is
\begin{multline}
	\left(
		1-\frac{\theta}{\sqrt{\alpha(\alpha+1)\lambda}}
	\right)^{-\alpha}
	=
	1+
	\sum_{r=1}^\infty \frac{\theta^r\prod_{s=1}^r(\alpha+s-1)}{(\alpha(\alpha+1)\lambda)^{r/2}r!}
	\\
	\text{for }\frac{\|\theta\|}{\sqrt{\alpha(\alpha+1)\lambda}}<1 \ .
\end{multline}
Substituting in the binomial expansion
\begin{equation}
	M_Z(\theta)=
	\exp\left(
	    -\theta\sqrt{\frac{\alpha\lambda}{\alpha+1}}
    \right)
	\exp\left[
		\lambda
		\sum_{r=1}^\infty \frac{\theta^r\prod_{s=1}^r(\alpha+s-1)}{(\alpha(\alpha+1)\lambda)^{r/2}r!}
	\right]
	\ .
\end{equation}
Writing in full the $r=1,2$ terms
\begin{multline}
	M_Z(\theta)=
	\exp\left(
	    -\theta\sqrt{\frac{\alpha\lambda}{\alpha+1}}
    \right)
	\exp\left[
		\theta\sqrt{\frac{\alpha\lambda}{\alpha+1}}
		+\frac{\theta^2}{2}
	\right.
	\\
	\left.
		+\lambda\sum_{r=3}^\infty \frac{\theta^r\prod_{s=1}^r(\alpha+s-1)}{(\alpha(\alpha+1)\lambda)^{r/2}r!}
	\right]
\end{multline}
and a term cancels out to get
\begin{equation}
	M_Z(\theta)=
	\exp\left[
		\frac{\theta^2}{2}
		+\sum_{r=3}^\infty
		\frac
			{\theta^r\prod_{s=1}^r(\alpha+s-1)}
			{(\alpha(\alpha+1))^{r/2}r!}
		\lambda^{1-r/2}
	\right]
	\ .
\end{equation}

For large $\lambda$
\begin{equation}
	\lim_{\lambda\rightarrow\infty}M_Z(\theta)=\exp\left[\frac{\theta^2}{2}\right]
\end{equation}
which is the same as the m.g.f.~of a standard Normal variable, therefore
\begin{equation}
	\lim_{\lambda\rightarrow\infty}Z\sim\normal(0,1) \ .
\end{equation}
This should make sense as for high $\lambda$, the Poisson random variable has a high expectation, increasing the number of gamma random variables in a summation. Increasing the number of terms in a summation will trigger the central limit theorem.

Next for high $\alpha$, it should be noted that
\begin{equation}
  \lim_{\alpha\rightarrow\infty}
  \frac{
    \prod_{s=1}^r(\alpha+s-1)
  }
  {
    \alpha(\alpha+1))^{r/2}
  }
  =
  \frac{
    \prod_{s=1}^r\alpha
  }
  {
    \alpha^{r}
  }
\end{equation}
so that
\begin{equation}
  \lim_{\alpha\rightarrow\infty}
  \frac{
    \prod_{s=1}^r(\alpha+s-1)
  }
  {
    (\alpha(\alpha+1))^{r/2}
  }
  = 1
  \ .
\end{equation}
As a result
\begin{equation}
  \lim_{\alpha\rightarrow\infty}
  M_Z(\theta)=
  \exp\left[
    \frac{\theta^2}{2}
    +\sum_{r=3}^\infty
    \frac{\theta^r}{r!}
    \lambda^{1-r/2}
  \right]
  \ .
\end{equation}
However this shows that taking the limit $\alpha\rightarrow\infty$ is not enough to get a Normal limiting distribution. For a Normal limiting distribution, the limit must be accompanied with the limit $\lambda\rightarrow\infty$, that is
\begin{equation}
  \lim_{\lambda\rightarrow\infty}\lim_{\alpha\rightarrow\infty}Z\sim\normal(0,1)
  \ .
\end{equation}

Finally it should be noted that $M_Z(\theta)$ is independent of $\beta$. Thus $\beta$ will have no effect on the convergence to a Normal limiting distribution.

The above results justify the use of the approximation
\begin{equation}
	X\sim\normal\left(\frac{\lambda\alpha}{\beta},\frac{\lambda\alpha(\alpha+1)}{\beta^2}\right)
\end{equation}
for large $\lambda$. The limiting case where $\lambda\rightarrow 0$, $\alpha\rightarrow 0$ and $\beta\rightarrow 0$ will not be discussed here.

\subsection{Saddlepoint Approximation}

The saddlepoint approximation \citep{daniels1954saddlepoint} gives an approximate solution to inverting the Laplace transformation of the m.g.f.~giving the p.d.f. Inverting the Fourier transformation of the characteristic function also gives the p.d.f.~using computational methods \citep{dunn2008evaluation}. The saddlepoint approximation will be studied here.

The m.g.f.~of a random variable $X$ is defined to be
\begin{equation}
  M_X(\theta)=\expectation[\euler^{\theta X}]
  =
  \int_{-\infty}^{\infty}\euler^{x\theta} p_X(x) \diff x
  \ .
\end{equation}
For a given m.g.f., the saddlepoint approximation \citep{daniels1954saddlepoint, butler2007saddlepoint} finds an approximate $p_X(x)$. The saddlepoint approximation is given as
\begin{equation}
  p_X(x)\approx\left(2\pi K_X''(s)\right)^{-1/2}\exp\left[K_X(s)-sx\right]
  \label{eq:saddlePoint:generalSaddlePoint}
\end{equation}
where $K_X(\theta) = \ln\left(M_X(\theta)\right)$, and $s=s(x)$ is the solution to the saddle point equation $K_X'(s)=x$.

For the compound Poisson-gamma distribution, the saddle point approximation is given as 
\begin{multline}
  p_X(x)\approx
  \frac{\left(\lambda\alpha\beta^\alpha\right)^{\frac{1}{2(\alpha+1)}}\euler^{-\lambda}}{\sqrt{2\pi(\alpha+1)}}x^{-\frac{\alpha+2}{2(\alpha+1)}}
  \euler^{-x\beta}
  \exp\left[x^{\frac{\alpha}{\alpha+1}}
    \frac{(\lambda\beta^\alpha)^{\frac{1}{\alpha+1}}(\alpha+1)}{\alpha^{\frac{\alpha}{\alpha+1}}}
  \right]
  \\
  \text{for }x>0 \ .
  \label{eq:saddle_point_approx}
\end{multline}
The approximation is not well defined for $x=0$.

The integral of the density approximation over the support may not equal to one, it can be numerically re-normalised if necessary. Thus it may be more sensible to write the approximation up to a constant
\begin{equation}
  p_X(x)\propto x^{-\frac{\alpha+2}{2(\alpha+1)}}\euler^{-x\beta}
  \exp\left[
    x^{\frac{\alpha}{\alpha+1}}
    \frac{
      (\lambda\beta^\alpha)^{\frac{1}{\alpha+1}}(\alpha+1)
    }
    {
      \alpha^{\frac{\alpha}{\alpha+1}}
    }
  \right]
  \ .
\end{equation}

For the remainder of this section, it will be shown how the saddlepoint approximation can be algebraically derived. First of all the cumulant generating function $K_X(\theta)$ can be obtained from the m.g.f.
\begin{equation}
  K_X(\theta) = \lambda
  \left[
    \left(\frac{\beta}{\beta-\theta}\right)^\alpha-1
  \right]
  \ .
\end{equation}
Taking the derivative with respect to $\theta$
\begin{equation}
  K_X'(\theta)=\frac{\lambda\alpha\beta^\alpha}{(\beta-\theta)^{\alpha+1}}
\end{equation}
and this is known as the saddlepoint equation. The quantity $s=s(x)$ is the solution to the equation $K_X'(s)=x$, that is
\begin{equation*}
  \frac{\lambda\alpha\beta^\alpha}{(\beta-s)^{\alpha+1}} = x
\end{equation*}
with solution
\begin{equation}
  s = \beta - \left(\frac{\lambda\alpha\beta^\alpha}{x}\right)^{\frac{1}{\alpha+1}}
  \ .
\end{equation}

The second derivative of the cumulant generating function is
\begin{equation}
  K_X''(\theta)=\frac{\lambda\alpha(\alpha+1)\beta^\alpha}{(\beta-\theta)^{\alpha+2}} \ .
\end{equation}
Substituting this and $K_X(\theta)$ into Equation \eqref{eq:saddlePoint:generalSaddlePoint}
\begin{equation*}
  p_X(x)\approx
  \frac{1}{\sqrt{2\pi}}
  \left[
    \frac{
      (\beta-s)^{\alpha+2}
    }
    {
      \lambda\alpha(\alpha+1)\beta^\alpha
    }
  \right]^{1/2}
  \exp\left[
    \lambda\left(\left(\frac{\beta}{\beta-s}\right)^\alpha-1\right)-sx
  \right]
  \ .
\end{equation*}
Substituting in $s=s(x)$
\begin{multline*}
  p_X(x)\approx
  \frac{1}{\sqrt{2\pi}}
  \left[
    \frac{\left(\beta-\beta+\left(\frac{\lambda\alpha\beta^\alpha}{x}\right)^{\frac{1}{\alpha+1}}\right)^{\alpha+2}}{\lambda\alpha(\alpha+1)\beta^\alpha}
  \right]^{1/2}
  \\
  \exp\left[
    \lambda
    \left(
      \left(
        \frac{\beta}{\beta-\beta+\left(\frac{\lambda\alpha\beta^\alpha}{x}\right)^{\frac{1}{\alpha+1}}}
      \right)^\alpha
      -1
    \right)
    -x\left(
      \beta-\left(\frac{\lambda\alpha\beta^\alpha}{x}\right)^{\frac{1}{\alpha+1}}
    \right)
  \right]
\end{multline*}
simplifying further
\begin{multline*}
  p_X(x)\approx
  \frac{1}{\sqrt{2\pi}}
  \left[
    \frac{
      \left(\frac{\lambda\alpha\beta^\alpha}{x}\right)^{\frac{\alpha+2}{\alpha+1}}
    }
    {
      \lambda\alpha(\alpha+1)\beta^\alpha
    }
  \right]^{1/2}
  \\
  \exp\left[
    \lambda
    \left(
      \beta^\alpha
      \left(\frac{x}{\lambda\alpha\beta^\alpha}\right)^{\frac{\alpha}{\alpha+1}}
      -1
    \right)
    -x\beta
    +(\lambda\alpha\beta^\alpha)^{\frac{1}{\alpha+1}}x^{1-\frac{1}{\alpha+1}}
  \right]
\end{multline*}
\begin{multline*}
  p_X(x)\approx
  \frac{1}{\sqrt{2\pi(\alpha+1)}}x^{-\frac{\alpha+2}{2(\alpha+1)}}
  \left(\lambda\alpha\beta^\alpha\right)^{\left(\frac{\alpha+2}{\alpha+1}-1\right)/2}
  \\
  \exp\left[
    \lambda\left(
      \beta^\alpha
      \left(\frac{x}{\lambda\alpha\beta^\alpha}\right)^{\frac{\alpha}{\alpha+1}}
      -1
    \right)
    -x\beta
    +(\lambda\alpha\beta^\alpha)^{\frac{1}{\alpha+1}}x^{\frac{\alpha}{\alpha+1}}
  \right]
\end{multline*}
\begin{multline*}
  p_X(x)\approx
  \frac{
    \left(
      \lambda\alpha\beta^\alpha
    \right)^{\frac{1}{2(\alpha+1)}}
  }
  {
    \sqrt{2\pi(\alpha+1)}}x^{-\frac{\alpha+2}{2(\alpha+1)}
  } 
  \\
  \exp\left[
    -\lambda
    -x\beta
    +x^{\frac{\alpha}{\alpha+1}}
    \left(
      \frac{
        \lambda\beta^\alpha
      }
      {
        \left(\lambda\alpha\beta^\alpha\right)^{\frac{\alpha}{\alpha+1}}
      }
      +
      \left(
        \lambda\alpha\beta^\alpha
      \right)^{\frac{1}{\alpha+1}}
    \right)
  \right]
\end{multline*}
\begin{multline*}
  p_X(x)\approx
  \frac{
    \left(
      \lambda\alpha\beta^\alpha
    \right)^{\frac{1}{2(\alpha+1)}}\euler^{-\lambda}
  }
  {
    \sqrt{2\pi(\alpha+1)}}x^{-\frac{\alpha+2}{2(\alpha+1)}
  }
  \euler^{-x\beta}
  \\
  \exp\left[
    x^{\frac{\alpha}{\alpha+1}}
    \left(
      (\lambda\beta^\alpha)^{1-\frac{\alpha}{\alpha+1}}\alpha^{-\frac{\alpha}{\alpha+1}}
      +
      \left(
        \lambda\alpha\beta^\alpha
      \right)^{\frac{1}{\alpha+1}}
    \right)
  \right]
\end{multline*}
\begin{multline*}
  p_X(x)\approx
  \frac{
    \left(\lambda\alpha\beta^\alpha\right)^{\frac{1}{2(\alpha+1)}}\euler^{-\lambda}
  }
  {
    \sqrt{2\pi(\alpha+1)}
  }
  x^{-\frac{\alpha+2}{2(\alpha+1)}}\euler^{-x\beta}
  \\
  \exp\left[
    x^{\frac{\alpha}{\alpha+1}}
    \left(
      (\lambda\beta^\alpha)^{\frac{1}{\alpha+1}}
      \alpha^{-\frac{\alpha}{\alpha+1}}
      +
      \left(\lambda\alpha\beta^\alpha\right)^{\frac{1}{\alpha+1}}
    \right)
  \right]
\end{multline*}
\begin{multline*}
  p_X(x)\approx
  \frac{
    \left(\lambda\alpha\beta^\alpha\right)^{\frac{1}{2(\alpha+1)}}\euler^{-\lambda}
  }
  {
    \sqrt{2\pi(\alpha+1)}
  }
  x^{-\frac{\alpha+2}{2(\alpha+1)}}\euler^{-x\beta}
  \\
  \exp\left[
    x^{\frac{\alpha}{\alpha+1}}(\lambda\beta^\alpha)^{\frac{1}{\alpha+1}}
    \left(
      \alpha^{-\frac{\alpha}{\alpha+1}}+\alpha^{\frac{1}{\alpha+1}}
    \right)
  \right]
  \ .
\end{multline*}

The expression $\alpha^{-\frac{\alpha}{\alpha+1}}+\alpha^{\frac{1}{\alpha+1}}$ can be simplified by putting the two terms over a common denominator
\begin{align}
  \alpha^{-\frac{\alpha}{\alpha+1}}+\alpha^{\frac{1}{\alpha+1}}
  & = \alpha^{\frac{1}{\alpha+1}}+\frac{1}{\alpha^{\frac{\alpha}{\alpha+1}}}
  \nonumber\\
  & = \frac{\alpha^{\frac{1}{\alpha+1}}\alpha^{\frac{\alpha}{\alpha+1}}+1}{\alpha^{\frac{\alpha}{\alpha+1}}}
  \nonumber\\
  & = \frac{\alpha+1}{\alpha^{\frac{\alpha}{\alpha+1}}}
\end{align}
so that
\begin{equation*}
p_X(x)\approx
  \frac{
    \left(\lambda\alpha\beta^\alpha\right)^{\frac{1}{2(\alpha+1)}}\euler^{-\lambda}
  }
  {
    \sqrt{2\pi(\alpha+1)}
  }
  x^{-\frac{\alpha+2}{2(\alpha+1)}}
  \euler^{-x\beta}
  \exp\left[
    x^{\frac{\alpha}{\alpha+1}}
    \frac{(\lambda\beta^\alpha)^{\frac{1}{\alpha+1}}(\alpha+1)}{\alpha^{\frac{\alpha}{\alpha+1}}}
  \right]
  \ .
\end{equation*}

\subsection{Exact Method using Series Evaluation}

The infinite sum can be computationally summed in a clever way. This was done by summing only large terms in the sum and ignores small terms. By using Stirling's approximation, the largest term in the sum can be approximately found \citep{dunn2005series}. The p.d.f.~of the compound Poisson-gamma distribution is difficult because of the infinite sum
\begin{equation}
p_X(x) = 
  \delta(x) \euler^{-\lambda}
  +
  \euler^{-\beta x-\lambda}\frac{1}{x}\sum_{y=1}^{\infty}W_y
  \quad\text{for }x\geqslant 0
\end{equation}
where
\begin{equation}
  W_y = \frac{\beta^{y\alpha}\lambda^yx^{y\alpha}}{\Gamma(y\alpha)y!}
  \ .
  \label{eq:compoundPoisson_Wy}
\end{equation}
$W_y$ can be expressed in a different way using different parameters
\begin{equation}
  W_y(x,p,\phi)=\frac{x^{y\alpha}}{\phi^{y(1+\alpha)}(p-1)^{y\alpha}(2-p)^yy!\Gamma(y\alpha)}
\end{equation}
where
\begin{equation}
  p=\frac{2+\alpha}{1+\alpha}
\end{equation}
and
\begin{equation}
  \phi = \frac{\alpha+1}{\beta^{2-p}(\lambda\alpha)^{p-1}} \ .
\end{equation}

\cite{dunn2005series} approximated the sum by truncation
\begin{equation}
  \sum_{y=1}^\infty W_y \approx \sum_{y=y_\text{l}}^{y_\text{u}}W_y
\end{equation}
where $y_\text{l}<y_{\text{max}}<y_\text{u}$ and $y_{\text{max}}$ is the value of $y$ which maximises $W_y$. \cite{dunn2005series} used Stirling's approximation to find $y_{\text{max}}$. This was done by treating $W_y$ as a continuous function of $y$ and is differentiable with respect to $y$.

It is easier to differentiate $\ln(W_y)$ where
\begin{multline*}
  \ln(W_y) = y\alpha\ln(x)-y(1+\alpha)\ln(\phi)-y\alpha\ln(p-1)\\-y\ln(2-p)-\ln(y!)-\ln\Gamma(y\alpha)
\end{multline*}
\begin{equation}
  \ln(W_y) =
  y
  \ln\left(
    \frac{x^\alpha}{\phi^{1+\alpha}(p-1)^\alpha(2-p)}
  \right)
  -\ln(y!)-\ln\Gamma(y\alpha)
  \ .
\end{equation}
Using Stirling's approximation $\ln(n!)\approx\ln\Gamma(n)\approx n\ln(n!)-n$, then
\begin{equation}
  \ln(W_y) \approx
  y\ln\left(
    \frac{x^\alpha}{\phi^{1+\alpha}(p-1)^\alpha(2-p)}
  \right)
  -y\ln(y)+y-y\alpha\ln(y\alpha) + y\alpha
  \ .
\end{equation}
Taking the derivative with respect to $y$
\begin{multline*}
  \frac{\partial \ln(W_y)}{\partial y} \approx
  \ln\left(
    \frac{x^\alpha}{\phi^{1+\alpha}(p-1)^\alpha(2-p)}
  \right)
  -\ln(y)-1+1
  \\
  -\alpha\ln(y\alpha)-\alpha+\alpha
\end{multline*}
\begin{equation}
  \frac{\partial \ln(W_y)}{\partial y} \approx
  \ln\left(
    \frac{x^\alpha}{\phi^{1+\alpha}(p-1)^\alpha(2-p)}
  \right)
  -\ln(y)
  -\alpha\ln(y\alpha)
  \ .
\end{equation}

Setting the derivative to zero
\begin{equation*}
  0 \approx \ln\left(
    \frac{
      x^\alpha
    }
    {
      \phi^{1+\alpha}(p-1)^\alpha(2-p)y_{\text{max}}^{1+\alpha}\alpha^\alpha
    }
  \right)
\end{equation*}
\begin{equation*}
  1 \approx 
  \frac{x^\alpha}{\phi^{1+\alpha}(p-1)^\alpha(2-p)y_{\text{max}}^{1+\alpha}\alpha^\alpha}
\end{equation*}
\begin{equation*}
  y_{\text{max}}^{1+\alpha} \approx 
  \frac{x^\alpha}{\phi^{1+\alpha}(p-1)^\alpha(2-p)\alpha^\alpha}
\end{equation*}
\begin{equation*}
  y_{\text{max}} \approx 
  \frac{1}{\phi}
  \left(
    \frac{x}{(p-1)\alpha}
  \right)^{\frac{\alpha}{1+\alpha}}
  (2-p)^{\frac{-1}{1+\alpha}}
  \ .
\end{equation*}
This can be simplified using the fact that 
\begin{equation*}
  \alpha=\frac{2-p}{p-1}
  \ ,
\end{equation*}
\begin{equation*}
  \frac{1}{1+\alpha} = p-1
\end{equation*}
and
\begin{equation*}
  \frac{\alpha}{1+\alpha} = 2-p
\end{equation*}
then
\begin{equation*}
  y_{\text{max}} \approx 
  \frac{1}{\phi}
  \left(
    \frac{x}{2-p}
  \right)^{2-p}
  (2-p)^{1-p}
\end{equation*}
and finally
\begin{equation}
  y_{\text{max}} \approx \frac{x^{2-p}}{\phi(2-p)}
  \ .
\end{equation}
Because values of $y$ should be a positive integer, it would be appropriate to round the solution to $y_\text{max}$ accordingly
\begin{equation}
  y_{\text{max}} = \text{max}\left[
    1,\text{round}\left(\frac{x^{2-p}}{\phi(2-p)}\right)
  \right]
  \ .
\end{equation}

To verify that $y_\text{max}$ is a maximum, the second derivative can be investigated
\begin{equation}
  \frac{\partial^2\ln(W_y)}{\partial y^2}
  \approx
  -\frac{1}{y}(\alpha+1)
\end{equation}
to see that
\begin{equation}
  \frac{\partial^2\ln(W_y)}{\partial y^2} < 0 \quad \text{for }y=1,2,3,\dotdotdot
\end{equation}
therefore $y_\text{max}$ is a maximum.

Returning back to the truncation of the infinite sum
\begin{equation}
  \sum_{y=1}^\infty W_y \approx \sum_{y=y_\text{l}}^{y_\text{u}}W_y
  \ ,
\end{equation}
$y_\text{l}$ and $y_\text{u}$ can be chosen such that $W_{y_\text{l}}$ and $W_{y_\text{u}}$ are less than $\epsilon W_{y_\text{max}}$ where $\epsilon$ is some small constant, for example $\epsilon=\euler^{-37}$ will be better than machine precision in 64 bits \citep{dunn2005series}. To prevent overflow problems, it is advised to calculate each term in the summation in log scale \citep{dunn2005series} and using the following equation
\begin{equation}
  \ln\left[
    \sum_{y=y_\text{l}}^{y_\text{u}}W_y
  \right]
  = 
  \ln\left(
    W_{y_\text{max}}
  \right)
  +\ln\sum_{y=y_\text{l}}^{y_\text{u}}
  \exp\left[
    \ln\left(W_y\right)-\ln\left(W_{y_\text{max}}\right)
  \right]
  \ .
\end{equation}

\section{Simulation Studies on Density Evaluation}

Simulations of a compound Poisson-gamma random variable were conducted with the aim to compare how well these density evaluations methods perform. This was done by comparing the evaluated densities with the histogram of simulations and a Q-Q plot. The following compound Poisson-gamma distributions were used in the simulations to capture the variety in the compound Poisson-gamma family: $\CPoisson(1,1,1)$, $\CPoisson(10,1,1)$, $\CPoisson(1,100,1)$, $\CPoisson(100,100,1)$.

A few technical problems with the histogram because the compound Poisson has probability mass at zero and probability density for positive numbers. To correctly represent the empirical distribution of a compound Poisson random variable, a bar chart was used to show the frequency of a zero and a histogram to show the frequency density of positive numbers. However, the Normal approximation and the saddlepoint approximation does not have support at zero. Therefore to compare these approximate densities to the empirical distribution fairly, a histogram containing both zero and positive samples was used when appropriate.

The evaluation of the p.d.f.~using the saddlepoint approximate required a bit of caution to avoid over/underflow problems. Suppose realizations of $X$ were simulated $\left\{x_1,x_2,x_3,\dotdotdot,x_n\right\}$. The saddlepoint approximation was computed up to a constant using
\begin{equation}
  p_X(x) \propto
  \exp\left[
    -\frac{\alpha+2}{2(\alpha+1)}
    \ln(x)
    -x\beta
    +\left(
    \frac{x\beta}{\alpha}
    \right)^{\frac{\alpha}{\alpha+1}}\lambda^{\frac{1}{\alpha+1}}(\alpha+1) - k
  \right]
\end{equation}
for 500 equally spaced points from and including the minimum non-zero simulated value to the maximum simulated value. $k$ was chosen to be
\begin{equation}
  k =
  \max_{i\in\left\{1,2,3,\dotdotdot,n\right\}}
  \left[
    -\frac{\alpha+2}{2(\alpha+1)}
    \ln(x_i)
    -x_i\beta
    +\left(
      \frac{x_i\beta}{\alpha}
    \right)^{\frac{\alpha}{\alpha+1}}\lambda^{\frac{1}{\alpha+1}}(\alpha+1)
  \right]
  \ .
\end{equation}
The density was then was normalised by numerically integrating it using the trapezium rule using the 500 evaluated points.

A Q-Q plot is a plot which compares the empirical c.d.f.~with the theoretical c.d.f. Let
\begin{equation}
  F_X(x) = \prob(X\leqslant x)
\end{equation}
and
\begin{equation}
  \widehat{F}_X(x) = \frac{1}{n}\cdot\text{max}
  \left[
    \left(\sum_{i=1}^n\mathbb{I}(x_i\leqslant x)\right)-0.5,0
  \right]
  \ ,
\end{equation}
then a Q-Q plot is a parametric plot which plots $\widehat{F}_X^{-1}(p)$ against $F_X^{-1}(p)$ for $p=\frac{0.5}{n},\frac{1.5}{n},\frac{2.5}{n},\dotdotdot,\frac{n-0.5}{n}$. If $F_X(x)$ and $\widehat{F}_X(x)$ are similar, a Q-Q plot should be a straight line with gradient 1, intercepting the origin. For the exact method and the saddlepoint approximate, $F_X(x)$ was found numerically by evaluating the p.d.f.~at $10\,000$ equally spaced points from and including the minimum to the maximum of the simulated samples, and then summing the required trapeziums. $\widehat{F}_X^{-1}(p)$ was then calculated by interpolation. Because the Normal and saddlepoint approximation does not support zero, the Q-Q plot was omitted for these approximations for large $\lambda$.

When plotting the probability of obtaining a zero or the p.d.f.~for positive values, confidence intervals were plotted as well. Consider a bin in a histogram, the confidence intervals were obtained by assuming that the frequency in a bin $\sim\poisson(\text{p.d.f.~evaluated at the bin}\times 1\,000 \times \text{bin width})$.

For low $\lambda$ (Figures \ref{fig:compoundPoisson_histogram_1_1_1}, \ref{fig:compoundPoisson_histogram_1_100_1}, \ref{fig:compoundPoisson_histogram_approx_1_1_1} and \ref{fig:compoundPoisson_histogram_approx_1_100_1}) there is a chance of simulating zeros. For $\lambda = 1$, the Normal approximation failed to capture the probability mass at zero because the Normal distribution is symmetric and support negative values. The saddlepoint approximation did capture the probability mass at zero as shown by an increase in probability density towards zero. For $\CPoisson(1,100,1)$ in Figure \ref{fig:compoundPoisson_histogram_approx_1_100_1}, the compound Poisson-gamma p.d.f.~contains multiple peaks and the saddlepoint approximation was not flexible enough them. In Figure \ref{fig:compoundPoisson_histogram_1_100_1}, the Q-Q plot for the exact method was quite sensitive at the tails of each peak, perhaps there were a few inaccuracies in the exact evaluation of the p.d.f.~or in the calculation or inversion of the c.d.f.

As $\lambda$ increased, in Figures \ref{fig:compoundPoisson_histogram_10_1_1} and \ref{fig:compoundPoisson_histogram_100_1_1}, all 3 density evaluation methods performed quite well. For $\CPoisson(10,1,1)$ in Figure \ref{fig:compoundPoisson_histogram_10_1_1}, the Normal approximation did not quite capture the skewness. In Figure \ref{fig:compoundPoisson_histogram_100_1_1}, $\lambda$ was high enough where the compound-Poisson distribution starts to converge to a Normal distribution. All methods performed well in this realm.

\begin{figure}
  \centering
    \centerline{
    \begin{subfigure}[b]{\mainSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_CompoundPoissonlambda1alpha1beta1.eps}
        \caption{Left: Expected and observed frequency of a zero. Right: Histogram and p.d.f.~of non-zero values.}
    \end{subfigure}
    }
    \vspace{2em}
    \centerline{
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_qq_CompoundPoissonlambda1alpha1beta1.eps}
        \caption{Q-Q plot}
    \end{subfigure}
    }
    \caption{1\,000 simulations of a $\CPoisson(1,1,1)$ random variable were simulated and its empirical distribution is compared to the p.d.f.~evaluated using the exact method. In a), the dotted red line shows the $\Phi(\pm1)$ quantile of the expected frequency or frequency density.}
    \label{fig:compoundPoisson_histogram_1_1_1}
\end{figure}

\begin{figure}
  \centering
    \centerline{
    \begin{subfigure}[b]{\mainSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_CompoundPoissonlambda1alpha100beta1.eps}
        \caption{Left: Expected and observed frequency of a zero. Right: Histogram and p.d.f.~of non-zero values.}
    \end{subfigure}
    }
    \vspace{2em}
    \centerline{
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_qq_CompoundPoissonlambda1alpha100beta1.eps}
        \caption{Q-Q plot}
    \end{subfigure}
    }
    \caption{1\,000 simulations of a $\CPoisson(1,100,1)$ random variable were simulated and its empirical distribution is compared to the p.d.f.~evaluated using the exact method. In a), the dotted red line shows the $\Phi(\pm1)$ quantile of the expected frequency or frequency density.}
    \label{fig:compoundPoisson_histogram_1_100_1}
\end{figure}

\begin{figure}
  \centering
    \centerline{
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_CompoundPoissonNormlambda1alpha1beta1.eps}
        \caption{Normal approximation}
    \end{subfigure}
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_CompoundPoissonSaddlelambda1alpha1beta1.eps}
        \caption{Saddlepoint approximation}
    \end{subfigure}
    }
    \caption{1\,000 simulations of a $\CPoisson(1,1,1)$ random variable were simulated and its empirical distribution is compared to the p.d.f.~evaluated using approximate methods. In a), the dotted red line shows the $\Phi(\pm1)$ quantile of the expected frequency density.}
    \label{fig:compoundPoisson_histogram_approx_1_1_1}
\end{figure}

\begin{figure}
  \centering
    \centerline{
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_CompoundPoissonNormlambda1alpha100beta1.eps}
        \caption{Normal approximation}
    \end{subfigure}
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_CompoundPoissonSaddlelambda1alpha100beta1.eps}
        \caption{Saddlepoint approximation}
    \end{subfigure}
    }
    \caption{1\,000 simulations of a $\CPoisson(1,100,1)$ random variable were simulated and its empirical distribution is compared to the p.d.f.~evaluated using approximate methods. In a), the dotted red line shows the $\Phi(\pm1)$ quantile of the expected frequency density.}
    \label{fig:compoundPoisson_histogram_approx_1_100_1}
\end{figure}

\begin{figure}
  \centering
    \centerline{
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_CompoundPoissonlambda10alpha1beta1.eps}
        \caption{Histogram - Exact method}
    \end{subfigure}
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_qq_CompoundPoissonlambda10alpha1beta1.eps}
        \caption{Q-Q plot - Exact method}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_CompoundPoissonNormlambda10alpha1beta1.eps}
        \caption{Histogram - Normal approx.}
    \end{subfigure}
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_qq_CompoundPoissonNormlambda10alpha1beta1.eps}
        \caption{Q-Q plot - Normal approx.}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_CompoundPoissonSaddlelambda10alpha1beta1.eps}
        \caption{Histogram - Saddlepoint approx.}
    \end{subfigure}
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_qq_CompoundPoissonSaddlelambda10alpha1beta1.eps}
        \caption{Q-Q plot - Saddlepoint approx.}
    \end{subfigure}
    }
    \caption{1\,000 simulations of a $\CPoisson(10,1,1)$ random variable were simulated and its empirical distribution is compared to the p.d.f. On the left, the dotted red line shows the $\Phi(\pm1)$ quantile of the expected frequency density.}
    \label{fig:compoundPoisson_histogram_10_1_1}
\end{figure}

\begin{figure}
  \centering
    \centerline{
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_CompoundPoissonlambda100alpha100beta1.eps}
        \caption{Histogram - Exact method}
    \end{subfigure}
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_qq_CompoundPoissonlambda100alpha100beta1.eps}
        \caption{Q-Q plot - Exact method}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_CompoundPoissonNormlambda100alpha100beta1.eps}
        \caption{Histogram - Normal approx.}
    \end{subfigure}
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_qq_CompoundPoissonNormlambda100alpha100beta1.eps}
        \caption{Q-Q plot - Normal approx.}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_CompoundPoissonSaddlelambda100alpha100beta1.eps}
        \caption{Histogram - Saddlepoint approx.}
    \end{subfigure}
    \begin{subfigure}[b]{\subSize}
        \includegraphics[width=\textwidth]{../figures/compoundpoisson/cpHistogram_qq_CompoundPoissonSaddlelambda100alpha100beta1.eps}
        \caption{Q-Q plot - Saddlepoint approx.}
    \end{subfigure}
    }
    \caption{1\,000 simulations of a $\CPoisson(100,100,1)$ random variable were simulated and its empirical distribution is compared to the p.d.f. On the left, the dotted red line shows the $\Phi(\pm1)$ quantile of the expected frequency density.}
    \label{fig:compoundPoisson_histogram_100_1_1}
\end{figure}

\section{Proposed Model}

The compound Poisson-gamma distribution can be used to model the grey values of each pixel in a projection. Suppose a projection has $m$ pixels and $n$ replicate projections were obtained. Let $X_{i,j}$ and $Y_{i,j}$ be the grey value and photon count, respectively, of the $i$th pixel in the $j$th replicate projection.

By assuming no beam hardening, the distribution of the photon does not change with attenuation. As a result all pixels will detect photons with identical energy distributions thus $\alpha$ and $\beta$ are the same for all pixels. Attenuation does affect the photon count, the more material a photon has to attenuate, the lower the number of detectable photons. The amount of attenuation depends on the specific path from the source to a pixel in a detector, so $\lambda_i$ varies from pixel to pixel. Electronic Normal noise $\epsilon_{i,j}\sim\normal(a,\kappa)$ may be added as well.

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{../figures/compoundpoisson/graphicalModel.eps}
  \caption{Graphical model of the grey value $X_{i,j}$ for each of the $m$ pixels in the $n$ replicate projections. $Y_{i,j}\sim\poisson(\lambda_i)$ is the photon count. The grey value has a compound Poisson gamma element $X_{i,j}|Y_{i,j}\sim\gammaDist(Y_{i,j}\alpha,\beta)$ and can be extended by adding electronic noise $\epsilon_{i,j}\sim\normal(a,\kappa)$.}
  \label{fig:compoundPoisson_graphicalModel}
\end{figure}

The graphical model in Figure \ref{fig:compoundPoisson_graphicalModel} illustrates how all of these variables link together. $a$ and $\kappa$ are unknown parameters but can be estimated beforehand using replicate black images. The energy of each photon $U\sim\gammaDist(\alpha,\beta)$ was omitted because the conditional distribution $X|Y\sim\gammaDist(Y\alpha,\beta)$ encapsulates each detected photon energy already.

The model may be simplified by omitting the electronic noise, this can be done by setting $a=0$ and $\kappa=0$. The number of pixels can be set to $m=1$ as well. The model is well set up for the EM algorithm \citep{dempster1977maximum}. If each $Y_{i,j}$ is known, then $\lambda_i$, $\alpha$ and $\beta$ can be estimated using maximum likelihood. $Y_{i,j}$ could be estimated if $\lambda_i$, $\alpha$ and $\beta$ are known. This problem can be tackled using the EM algorithm.

\section{EM Algorithm}

The EM algorithm \citep{dempster1977maximum} will be proposed here as a method to estimate the parameters of a $X\sim\CPoisson(\lambda,\alpha,\beta)$ random variable given a sample of measurements of it $\left\{x_1,x_2,x_3,\dotdotdot,x_n\right\}$.

A popular method for parameter estimation is the maximum log likelihood method. Let $\widehat{\lambda}$, $\widehat{\alpha}$ and $\widehat{\beta}$ be estimators of $\lambda$, $\alpha$ and $\beta$ respectively. The log likelihood is defined to be
\begin{equation*}
	\ln L(\lambda,\alpha,\beta;X) = \sum_{i=1}^n
	\left[
		\mathbb{I}(x_i=0)\ln \prob(X=x_i)
		+\mathbb{I}(x_i>0)\ln p_X(x_i)
	\right]
\end{equation*}
\begin{multline}
	\ln L(\lambda,\alpha,\beta;X) = \sum_{i=1}^n
	\left[
		-\mathbb{I}(x_i=0)\lambda
		\vphantom{\sum_i^i}
	\right.
	\\
	\left.
		+\mathbb{I}(x_i>0)\left(
		-1-\beta x_i-\lambda-\ln x -\ln\sum_{y=1}^\infty W_y
		\right)
	\right]
\end{multline}
where $W_y=W_y(\lambda,\alpha,\beta)$ was defined in Equation \eqref{eq:compoundPoisson_w}. $\widehat{\lambda}$, $\widehat{\alpha}$ and $\widehat{\beta}$ are values of $\lambda$, $\alpha$ and $\beta$ which jointly maximises the log likelihood. The gradient of the log likelihood can be evaluated \citep{dunn2005series} and optimisation can be done using gradient methods.

The EM algorithm \citep{dempster1977maximum} instead treats $Y\sim\poisson(\lambda)$ as a latent variable. Let $\left\{Y_1,Y_2,Y_3,\dotdotdot, Y_n\right\}$ be realizations of $Y$ and $\left\{x_1,x_2,x_3,\dotdotdot, x_n\right\}$ be measurements of $X|Y$. Define the joint log likelihood to be
\begin{multline*}
	\ln L(\lambda,\alpha,\beta;X,Y)
	=
	\sum_{i=1}^n
	\left[
		\mathbb{I}(x_i=0)
		\ln
		\prob(Y=0)
	\right.
	\\
	\left.
		+
		\mathbb{I}(x_i>0)
		\ln
		\left[
			p_{X|Y}(x_i|Y_i)\prob(Y=Y_i)
		\right]
	\right]
\end{multline*}
so that
\begin{multline}
	\ln L(\lambda,\alpha,\beta;X,Y)=
	\sum_{i=1}^n
	\left[
		-\mathbb{I}(x_i=0)
		\lambda
	\right.
	\\
	\left.+
		\mathbb{I}(x_i>0)
		\left(
			Y_i\alpha\ln\beta-\ln\Gamma(Y_i\alpha)+(Y_i\alpha-1)\ln x_i - \beta x_i
		\right.
	\right.
	\\
	\left.
		\left.	
			- \lambda + Y_i \ln \lambda - \ln(Y_i!)
		\right)
	\right]
	\ .
\end{multline}
The estimators are found by optimising the joint log likelihood. This is done iteratively by estimating the $Y$s given the $X$s and parameters (E step), followed by estimating the parameters given the $Y$s and $X$s (M step) until some convergence conditions are met.

It will be shown that using some approximations, the E step and M step can be implemented. The Cram\'er-Rao lower bound \citep{rao1945information} \citep{cramer1946mathematical} can also be found by using a few approximations. However simulations show that these estimators struggle for high $\lambda$ and it will be shown that this will be the case for all estimators.

\subsection{E Step}

In the E step, the realizations of $Y$ are estimate using
\begin{equation}
	y_i =
	\expectation\left[
		Y|X=x_i
	\right]
\end{equation}
for given parameters $\lambda$, $\alpha$ and $\beta$. The conditional expectation is calculated using
\begin{equation}
	y_i = 
	\begin{cases}
		0 & \text{ for } x=0 \\ 
		\dfrac{\sum_{y=1}^\infty y \prob(Y=y|X=x_i)}{\sum_{y=1}^\infty \prob(Y=y|X=x_i)} & \text{ for } x>0
	\end{cases}
\end{equation}
where
\begin{equation*}
	\prob(Y=y|X=x) = \frac{p_{X|Y}(x|y)\prob(Y=y)}{p_X(x)}
	\ .
\end{equation*}
Focussing on the $x>0$ case for now
\begin{equation*}
	\prob(Y=y|X=x) = \frac{1}{p_X(x)}\frac{\beta^{y\alpha}}{\Gamma(y\alpha)}x^{y\alpha-1}\euler^{-\beta x}\frac{\euler^{-\lambda}\lambda^y}{y!}
\end{equation*}
and
\begin{equation}
	\prob(Y=y|X=x) = W_y \frac{\euler^{-\lambda-\beta x}}{x}
\end{equation}
where $W_y$ is defined in Equation \eqref{eq:compoundPoisson_Wy}.
Then the conditional expectation is
\begin{equation}
	y_i = \frac{\sum_{y=1}^\infty y W_y}{\sum_{y=1}^\infty W_y}
	\ .
\end{equation}
It was already established how the denominator can be evaluated \citep{dunn2005series}. A similar method for evaluating the numerator can be obtained by truncating the sum and summing large terms.

Let
\begin{equation}
	W_y^{(r)} = y^r W_y
\end{equation}
so that
\begin{equation}
	y_i = \frac{\sum_{y=1}^\infty W^{(1)}_y}{\sum_{y=1}^\infty W_y}
\end{equation}
and
\begin{equation}
	\variance[Y|X=x_i] = \frac{\sum_{y=1}^\infty W^{(2)}_y}{\sum_{y=1}^\infty W_y} - \left(y_i\right)^2
	\ .
\end{equation}
The exponent $r=0,1,2,\dotdotdot$ will be useful in the M step where higher conditional moments are needed.
The sum can be truncated
\begin{equation}
	\sum_{y=1}^\infty W^{(r)}_y \approx \sum_{y=y_\text{l}}^{y_\text{u}} W^{(r)}_y
\end{equation}
where $y_\text{l}<y_\text{max}<y_\text{u}$ and $y_\text{max}$ is the value of $y$ which maximises $W_y^{(r)}$. By expressing $\ln(W^{(r)}_y)$ as
\begin{equation}
	\ln\left(W_y^{(r)}\right)=r\ln(y)+\ln(W_y)
\end{equation}
and then taking the derivative with respect to $y$
\begin{equation}
	\frac{\partial \ln(W_y^{(r)})}{\partial y} = \frac{r}{y } + \frac{\partial \ln(W_y)}{\partial y}
	\ .
\end{equation}
Keep in mind that $y=1,2,3,\dotdotdot$ so for large $y$, an approximation can be made $r/y\approx 0$ so that
\begin{equation}
	\frac{\partial \ln(W_y^{(r)})}{\partial y} \approx \frac{\partial \ln(W_y)}{\partial y}
	\ .
\end{equation}
By using such an approximation, the method for evaluating the sum $\sum_{y=1}^\infty W^{(r)}_y$ is almost exactly the same as evaluating the sum $\sum_{y=1}^\infty W_y$, in particular $y_\text{max}$ are the same. For a given $r=0,1,2,\dotdotdot$, the limit of the sum $\sum_{y=y_\text{l}}^{y_\text{u}} W^{(r)}_y$ are chosen such that $W_{y_\text{l}}$ and $W_{y_\text{u}}$ are less than $\epsilon W_{y_\text{max}}^{(r)}$ where $\epsilon$ is some small constant. The limits will be different for different values of $r$.

\subsection{M Step}

In the M step, the conditional expected joint log likelihood is maximised with respect to the parameters $\lambda$, $\alpha$ and $\beta$. The objective function is then
\begin{multline*}
	T(\lambda,\alpha,\beta)=
	\sum_{i=1}^n
	\expectation\left[
		-\mathbb{I}(x_i=0)
		\lambda
	\right.
	\\
	\left.+
		\mathbb{I}(x_i>0)
		\left(
			Y_i\alpha\ln\beta-\ln\Gamma(Y_i\alpha)+(Y_i\alpha-1)\ln x_i - \beta x_i
		\right.
	\right.
	\\
	\left.
		\left.	
			- \lambda + Y_i \ln \lambda - \ln(Y_i!)
		\right)
		|X_i=x_i
	\right]
\end{multline*}
\begin{multline*}
	T(\lambda,\alpha,\beta)=
	-n\lambda
	\\
	+\sum_{i=1}^n
	\mathbb{I}(x_i>0)
	\left[
		\expectation[Y_i|X_i=x_i]\alpha\ln\beta-\expectation[\ln\Gamma(Y_i\alpha)|X_i=x_i]
	\right.
	\\
	\left.
		+\expectation[Y_i|X_i=x_i]\alpha\ln x_i - \beta x_i
		+ \expectation[Y_i|X_i=x_i] \ln \lambda
	\right] + c
\end{multline*}
where $c$ is some constant not dependent on $\lambda$, $\alpha$ or $\beta$.

The conditional expectation $y_i = \expectation[Y_i|X_i=x_i]$ was calculated in the E step. The quantity $\expectation[\ln\Gamma(Y_i\alpha)|X_i=x_i]$ can be calculated using the approximation
\begin{equation}
	\expectation[\ln\Gamma(Y_i\alpha)|X_i=x_i] \approx
	\ln\Gamma(\alpha y_i) + \frac{1}{2}\zeta_i\alpha^2\psi'(y_i\alpha)
\end{equation}
where
\begin{equation}
	\zeta_i = \variance[Y_i|X_i=x_i]
\end{equation}
calculated in the E step and $\psi$ is the digamma function. The objective function is then
\begin{multline}
	T(\lambda,\alpha,\beta)=
	-n\lambda
	+\sum_{i=1}^n
	\mathbb{I}(x_i>0)
	\left[
		y_i\alpha\ln\beta-\ln\Gamma(\alpha y_i) - \frac{1}{2}\zeta_i\alpha^2\psi'(y_i\alpha)
	\right.
	\\
	\left.
		+y_i\alpha\ln x_i - \beta x_i
		+ y_i \ln \lambda
		\vphantom{\frac{1}{1}}
	\right]
	+ c
	\ .
\end{multline}

Taking the derivative with respect to $\lambda$
\begin{equation}
	\frac{\partial T}{\partial\lambda} = -n + \frac{\sum_{i=1}^ny_i}{\lambda}
\end{equation}
and setting it to zero
\begin{equation}
	\widehat{\lambda} = \frac{\sum_{i=1}^n y_i}{n}
\end{equation}
obtains a M step estimator, and a familiar one, for $\lambda$. Taking the second derivative with respect to $\lambda$
\begin{equation}
	\frac{\partial^2 T}{\partial \lambda^2} = -\frac{\sum_{i=1}^ny_i}{\lambda^2} < 0
\end{equation}
verifies $\widehat{\lambda}$ maximises T. In addition
\begin{equation}
	\frac{\partial^2 T}{\partial \alpha \partial \lambda } = 0
\end{equation}
and
\begin{equation}
	\frac{\partial^2 T}{\partial \beta \partial \lambda } = 0
	\ .
\end{equation}

Maximising $T$ with respect to $\alpha$ and $\beta$ can be done numerically using the Newton-Raphson method since derivatives up to the second order can be obtained. For the first order these are:
\begin{multline}
	\frac{\partial T}{\partial \alpha} =
	\sum_{i=1}^n\mathbb{I}(x_i>0)
	\left[
		y_i\ln\beta -\psi(\alpha y_i)y_i-\zeta_i\alpha\psi'(\alpha y_i)
		\vphantom{\frac{1}{1}}
	\right.
	\\
	\left.
		- \frac{1}{2}\zeta_i\alpha^2\psi''(\alpha y_i)y_i + y_i\ln x_i
	\right]
\end{multline}
and
\begin{equation}
	\frac{\partial T}{\partial \beta} = \sum_{i=1}^n\mathbb{I}(x_i>0)\left[
	\frac{\alpha y_i}{\beta}-x_i
	\right]
	\ .
\end{equation}
The second orders are:
\begin{equation}
	\frac{\partial^2 T}{\partial \alpha \partial \beta} =
	\sum_{i=1}^n \mathbb{I}(x_i>0)\left[\frac{y_i}{\beta}\right]
	\ ,
	\label{eq:compoundPoisson:d2tdadb}
\end{equation}
\begin{equation}
	\frac{\partial^2 T}{\partial \beta^2} = \sum_{i=1}^n\mathbb{I}(x_i>0)\left[-\frac{\alpha y_i}{\beta^2}
	\right]
	\label{eq:compoundPoisson:d2td2b}
\end{equation}
and
\begin{multline*}
	\frac{\partial^2 T}{\partial \alpha^2} = 
	\sum_{i=1}^n\mathbb{I}(x_i>0)
	\left[
		-y_i^2\psi'(\alpha y_i) - \zeta_i\psi'(\alpha y_i) - \zeta_i\alpha y_i\psi''(\alpha y_i)
		\vphantom{\frac{1}{2}}\right.\\\left.	
		- \zeta_i\alpha\psi''(\alpha y_i)y_i
		-\frac{1}{2}\zeta_i\alpha^2\psi'''(\alpha y_i)y_i^2
	\right]
\end{multline*}
simplifying to
\begin{multline}
	\frac{\partial^2 T}{\partial \alpha^2} = 
	\sum_{i=1}^n\mathbb{I}(x_i>0)
	\left[
		-(y_i^2+\zeta_i)\psi'(\alpha y_i) - 2\zeta_i\alpha y_i\psi''(\alpha y_i)
		\vphantom{\frac{1}{2}}
	\right.
	\\
	\left.	
		-\frac{1}{2}\zeta_i\alpha^2y_i^2\psi'''(\alpha y_i)
	\right] \ .
	\label{eq:compoundPoisson:d2td2a}
\end{multline}

All the derivatives can be used in the Newton-Raphson iterative update, which is
\begin{equation}
	\begin{pmatrix}
		\alpha \\ \beta
	\end{pmatrix}
	\leftarrow
	\begin{pmatrix}
		\alpha \\ \beta
	\end{pmatrix}
	\left[
		\nabla_{\alpha,\beta}\nabla_{\alpha,\beta}\T T
	\right]^{-1}
	\left[
		\nabla_{\alpha,\beta} T
	\right]
\end{equation}
where
\begin{equation}
	\nabla_{\alpha,\beta}=
	\begin{pmatrix}
		{\partial}/{\partial \alpha}
		\\
		{\partial}/{\partial \beta}
	\end{pmatrix}
	\ .
\end{equation}
Since increasing $T$ is sufficient for the EM algorithm \citep{dempster1977maximum}, one step of the Newton-Raphson iterative update was chosen in the M step to avoid implementing a convergence condition.

\subsection{Estimation Error}

The variance of the M step estimators can be obtained by using the Cram\'er-Rao lower bound \citep{rao1945information} \citep{cramer1946mathematical} from the Fisher's information matrix defined as
\begin{equation}
	\matr{I} = -\expectation\left[
		\nabla_{\lambda,\alpha,\beta}\nabla_{\lambda,\alpha,\beta}\T \ln L(\lambda,\alpha,\beta;X,Y)
	\right]
	\ .
\end{equation}
The calculation is similar to the derivatives of $T$, so using the same approximations and the fact that $\expectation[Y]=\lambda$, then
\begin{equation}
	\matr{I}=
	\begin{pmatrix}
		\dfrac{n}{\lambda} & 0 & 0 \\
		0 & (\lambda^2+\lambda)\psi'(\alpha\lambda)+2\alpha\lambda^2\psi''(\alpha\lambda)+\frac{1}{2}\alpha^2\lambda^3\psi'''(\alpha\lambda) & -\dfrac{n\lambda}{\beta}\\
		0 & -\dfrac{n\lambda}{\beta} & \dfrac{n\alpha\lambda}{\beta^2}
	\end{pmatrix}
	\ .
\end{equation}
The Cram\'er-Rao lower bound is then
\begin{equation}
	\cov\left[
		\begin{pmatrix}
			\widehat{\lambda}\\\widehat{\alpha}\\\widehat{\beta}
		\end{pmatrix}
	\right]
	=
	\matr{I}^{-1}
	\ .
\end{equation}

\subsection{Simulations}

An experiment was conducted to assess the performance on the EM algorithm. For a given set of parameters, 1\,000 samples of a $\CPoisson(\lambda,\alpha,\beta)$ random variable were simulated. The EM algorithm was initialised with its parameters at the true values to investigate the convergence in that vicinity. The log likelihood $\ln L(\lambda,\alpha,\beta;X)$ and the parameters were recorded at every EM step. The experiment was repeated 10 times using different simulated samples.

The results for $\CPoisson(1,1,1)$, $\CPoisson(10,1,1)$, $\CPoisson(1,100,1)$ and $\CPoisson(100,100,1)$ are shown in Figures \ref{fig:compoundPoisson_convergence_1}, \ref{fig:compoundPoisson_convergence_2}, \ref{fig:compoundPoisson_convergence_3} and \ref{fig:compoundPoisson_convergence_4} respectively. Good performance can be seen for the $\lambda=1$ case with convergence of all 3 parameters within a step or two. The Cram\'er-Rao lower bound captured the spread of the estimates well.

However for $\lambda=10$ and $\lambda=100$ case, the estimates of $\alpha$ and $\beta$ struggled to converge and increased/decreased without bounds without affecting the log likelihood. It appears the EM algorithm failed for $\lambda>10$ looking at these particular examples.

\begin{figure}[p]
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_1_lnL.eps}
        \caption{Log likelihood}
    \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_1_lambda.eps}
        \caption{$\lambda$}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_1_alpha.eps}
        \caption{$\alpha$}
    \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_1_beta.eps}
        \caption{$\beta$}
    \end{subfigure}
    }
    \caption{EM algorithm was used to estimate the parameters of a $\CPoisson(1,1,1)$ random variable using 1\,000 simulated samples, repeated 10 times. The graphs show the log likelihood and the estimated parameters at each EM step for each repeat of the experiment. The dashed lines show the standard deviation using the Cram\'er-Rao lower bound.}
    \label{fig:compoundPoisson_convergence_1}
\end{figure}

\begin{figure}[p]
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_2_lnL.eps}
        \caption{Log likelihood}
    \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_2_lambda.eps}
        \caption{$\lambda$}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_2_alpha.eps}
        \caption{$\alpha$}
    \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_2_beta.eps}
        \caption{$\beta$}
    \end{subfigure}
    }
    \caption{EM algorithm was used to estimate the parameters of a $\CPoisson(10,1,1)$ random variable using 1\,000 simulated samples, repeated 10 times. The graphs show the log likelihood and the estimated parameters at each EM step for each repeat of the experiment. The dashed lines show the standard deviation using the Cram\'er-Rao lower bound.}
    \label{fig:compoundPoisson_convergence_2}
\end{figure}

\begin{figure}[p]
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_3_lnL.eps}
        \caption{Log likelihood}
    \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_3_lambda.eps}
        \caption{$\lambda$}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_3_alpha.eps}
        \caption{$\alpha$}
    \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_3_beta.eps}
        \caption{$\beta$}
    \end{subfigure}
    }
    \caption{EM algorithm was used to estimate the parameters of a $\CPoisson(1,100,1)$ random variable using 1\,000 simulated samples, repeated 10 times. The graphs show the log likelihood and the estimated parameters at each EM step for each repeat of the experiment. The dashed lines show the standard deviation using the Cram\'er-Rao lower bound.}
    \label{fig:compoundPoisson_convergence_3}
\end{figure}

\begin{figure}[p]
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_4_lnL.eps}
        \caption{Log likelihood}
    \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_4_lambda.eps}
        \caption{$\lambda$}
    \end{subfigure}
    }
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_4_alpha.eps}
        \caption{$\alpha$}
    \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/compoundPoisson/convergence_4_beta.eps}
        \caption{$\beta$}
    \end{subfigure}
    }
    \caption{EM algorithm was used to estimate the parameters of a $\CPoisson(100,100,1)$ random variable using 1\,000 simulated samples, repeated 10 times. The graphs show the log likelihood and the estimated parameters at each EM step for each repeat of the experiment. The dashed lines show the standard deviation using the Cram\'er-Rao lower bound.}
    \label{fig:compoundPoisson_convergence_4}
\end{figure}

\section{Failure Evaluation}
It should be convincing that the EM algorithm failed when the compound-Poisson Gamma random variable starts behaving Normally. In particular in Figure \ref{fig:compoundPoisson_convergence_4}, estimates of $\lambda$ are stable while estimates of $\alpha$ and $\beta$ struggled to converge. This could be because as the compound-Poisson Gamma random variable approach being Normal, the parameters $(\lambda,\alpha,\beta)$ becomes degenerate and there is more than one way to represent a two parameter random variable $\normal(\mu,\sigma^2)$.

One way to see this is to look at the Newton-Raphson step, in the M step, in particular the Hessian matrix $\nabla_{\alpha,\beta}\nabla_{\alpha,\beta}\T T$ for high $\lambda$. The elements of the Hessian matrix can be found in Equations \eqref{eq:compoundPoisson:d2tdadb}, \eqref{eq:compoundPoisson:d2td2b} and \eqref{eq:compoundPoisson:d2td2a}. The $\dfrac{\partial^2T}{\partial\alpha^2}$ element is 
\begin{multline*}
	\frac{\partial^2 T}{\partial \alpha^2} = 
	\sum_{i=1}^n\mathbb{I}(x_i>0)\left[
		-(y_i^2+\zeta_i)\psi'(\alpha y_i) - 2\zeta_i\alpha y_i\psi''(\alpha y_i)
		\vphantom{\frac{1}{2}}
	\right.
	\\
	\left.	
		-\frac{1}{2}\zeta_i\alpha^2y_i^2\psi'''(\alpha y_i)
	\right]
	\ .
\end{multline*}
For high $\alpha$ and high $\lambda$, and hence high $y_i$'s, an approximation can be used for the polygamma functions $\psi^{(k)}(n)$. Firstly using Stirling's approximation
\begin{equation}
	\ln\Gamma(n)\approx\ln(n!)\approx n\ln n-n
\end{equation}
then
\begin{equation}
	\psi(n) = \frac{\partial\ln\Gamma(n)}{\partial n} \approx \ln n
	\ .
\end{equation}
Differentiating further
\begin{align}
	\psi'(n) &\approx 1/n \\
	\psi''(n) & \approx -1/n^2 \\
	\psi'''(n) & \approx 2/n^3
	\ .
\end{align}
Using the approximations, $\dfrac{\partial^2T}{\partial\alpha^2}$ can be approximated
\begin{equation*}
	\frac{\partial^2 T}{\partial \alpha^2} \approx 
	\sum_{i=1}^n\mathbb{I}(x_i>0)
	\left[
		-\frac{y_i^2+\zeta_i}{\alpha y_i} + 2\frac{\zeta_i\alpha y_i}{\alpha^2 y_i^2}
		-\frac{1}{2}\frac{2\zeta_i\alpha^2y_i^2}{\alpha^3 y_i^3}
	\right]
\end{equation*}
to get
\begin{equation}
	\frac{\partial^2 T}{\partial \alpha^2} \approx 
	\sum_{i=1}^n\mathbb{I}(x_i>0)
	\left[
		-\frac{y_i}{\alpha}
	\right]
	\ .
\end{equation}

The Hessian matrix is then
\begin{equation}
	\nabla_{\alpha,\beta}\nabla_{\alpha,\beta}\T T \approx
	\sum_{i=1}^n
	\mathbb{I}(x_i>0)
	\begin{pmatrix}
		-y_i/\alpha  & y_i/\beta \\
		y_i/\beta & -\alpha y_i/\beta^2
	\end{pmatrix}
	\ . 
\end{equation}
The determinant of the Hessian matrix is then
\begin{equation*}
	\|\nabla_{\alpha,\beta}\nabla_{\alpha,\beta}\T T\|
	\approx
	\left(\sum_{i=1}^n-\frac{\alpha y_i}{\beta^2}\right)
	\left(\sum_{i=1}^n-\frac{y_i}{\alpha}\right) - \left(\sum_{i=1}^n\frac{y_i}{\beta}\right)^2
\end{equation*}
\begin{equation*}
	\|\nabla_{\alpha,\beta}\nabla_{\alpha,\beta}\T T\|
	\approx
	\left(\sum_{i=1}^n\sum_{j=1}^n\frac{y_iy_j}{\beta^2}\right)
	 - \left(\sum_{i=1}^n\frac{y_i}{\beta}\right)\left(\sum_{j=1}^n\frac{y_j}{\beta}\right)
\end{equation*}
\begin{equation*}
	\|\nabla_{\alpha,\beta}\nabla_{\alpha,\beta}\T T\|
	\approx
	\left(\sum_{i=1}^n\sum_{j=1}^n\frac{y_iy_j}{\beta^2}\right)
	 - \left(\sum_{i=1}^n\sum_{j=1}^n\frac{y_iy_j}{\beta^2}\right)
\end{equation*}
to get
\begin{equation}
	\|\nabla_{\alpha,\beta}\nabla_{\alpha,\beta}\T T\|
	\approx
	0
	\ .
\end{equation}

This results in a few things. Firstly $\nabla_{\alpha,\beta}\nabla_{\alpha,\beta}\T T$ is singular thus its inverse cannot be evaluated, needed for the Newton-Raphson method. Secondly, the sufficient conditions to classify a stationary point as a maximum are that the diagonal elements of the Hessian matrix are negative and the determinate is positive. For high $\lambda$ and $\alpha$, the second condition is not met.

Another argument which can be made is to restrict the parameter $\beta$ for a given mean to be
\begin{equation}
	\beta = \frac{\lambda\alpha}{\widehat{\mu}}
	\label{eq:compoundPoisson:beta_restrict}
\end{equation}
where
\begin{equation}
	\widehat{\mu} = \frac{1}{n}\sum_{i=1}^n x_i
\end{equation}
and investigate whenever a unique solution to $\dfrac{\partial T}{\partial \alpha} = 0$ can be found for high $\lambda$ and $\alpha$. The constrained objective function is
\begin{multline*}
	T(\lambda,\alpha)
	=
	-n\lambda+
	\sum_{i=1}^n
	\mathbb{I}(x_i>0)
	\left[
	y_i\alpha
	\ln\left(\frac{\lambda\alpha}{\widehat{\mu}}\right)-\ln\Gamma(\alpha y_i) - \frac{1}{2}\zeta_i\alpha^2\psi'(y_i\alpha)
	\right.
	\\
	\left.
		+y_i\alpha\ln x_i - \frac{\lambda\alpha x_i}{\widehat{\mu}}
		+ y_i \ln \lambda
		\vphantom{\frac{1}{1}}
	\right]
	+ c
\end{multline*}
which can be approximated to
\begin{multline*}
	T(\lambda,\alpha)=
	-n\lambda+
	\sum_{i=1}^n
	\mathbb{I}(x_i>0)
	\left[
		y_i\alpha\ln\left(\frac{\lambda\alpha}{\widehat{\mu}}\right)-\alpha y_i\ln(\alpha y_i) + \alpha y_i - \frac{1}{2}\frac{\zeta_i\alpha}{y_i}
	\right.
	\\
	\left.
		+y_i\alpha\ln x_i - \frac{\lambda\alpha x_i}{\widehat{\mu}}
		+ y_i \ln \lambda
		\vphantom{\frac{1}{1}}
	\right]
	+ c
\end{multline*}
\begin{multline*}
	T(\lambda,\alpha)=
	-n\lambda+
	\sum_{i=1}^n
	\mathbb{I}(x_i>0)
	\left[
		y_i\alpha\ln\left(\frac{\lambda}{y_i\widehat{\mu}}\right) + \alpha y_i - \frac{1}{2}\frac{\zeta_i\alpha}{y_i}
	\right.
	\\
	\left.
		+y_i\alpha\ln x_i - \frac{\lambda\alpha x_i}{\widehat{\mu}}
		+ y_i \ln \lambda
		\vphantom{\frac{1}{1}}
	\right]
	+ c
	\ .
\end{multline*}
Taking the derivative with respect to $\alpha$
\begin{equation*}
	\frac{\partial T}{\partial \alpha} =
	\sum_{i=1}^n
	\mathbb{I}(x_i>0)
	\left[
		y_i\ln\left(\frac{\lambda}{y_i\widehat{\mu}}\right)
		+ y_i - \frac{1}{2}\frac{\zeta_i}{y_i}
		+y_i\ln x_i - \frac{\lambda x_i}{\widehat{\mu}}
		\vphantom{\frac{1}{1}}
	\right]
\end{equation*}
and this is not $\alpha$ dependent, thus there is no solution to $\alpha$.