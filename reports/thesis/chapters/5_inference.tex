The aim of this project is to obtain an x-ray image of a 3D printed sample and compare it with a simulation of that scan using software called \texttt{aRTist}. \texttt{aRTist} can simulate x-ray scans of the 3D printed sample given the specifications of the scan, such as the x-ray source, the x-ray detector and the blueprint of the 3D printed sample. Users of \texttt{aRTist} can align the simulated x-ray image to the real x-ray image using numerical methods, however this is outside the scope of this thesis.

Disagreement between the scan and \texttt{aRTist} can be found by simply subtracting one image from the other, any values too big in magnitude can be considered as a defect. However in the previous chapters, it was found that x-ray photons behave randomly and large differences in the comparison can be due to chance. Thus the comparisons should be done under the face of uncertainty.

A pixel by pixel inference was proposed to do defect detection. A statistic $z_{i,j}$ for the $(i,j)$ positioned pixel can be calculated for all pixels. This statistic is
\begin{equation}
    z_{i,j} = 
    \dfrac{
        \text{scan}_{i,j} - \text{aRTist}_{i,j}
    }
    {
        \sqrt{\widehat{\variance}\left[\text{scan}_{i,j}\right]}
    }
\end{equation}
where $\widehat{\variance}\left[\text{scan}_{i,j}\right]$ is the estimated grey value variance of pixel $(i,j)$ in the scan. Under mild assumptions, it was shown in the previous chapters that the grey values in the scan are Normal. Thus by treating the simulated image from \texttt{aRTist} as known, then
\begin{equation}
z_{i,j}\sim \normal(0,1) \ .
\end{equation}
This means the randomness of the $z_{i,j}$ statistics can be quantified.

The estimation of the grey value variance, that is $\widehat{\variance}\left[\text{scan}_{i,j}\right]$ will be explained here. The variance model can be calibrated, or trained, by holding out a number replicated x-ray scans of a 3D printed sample. These replicated x-ray scans provide variance-mean data which then can be used to train the variance model, such as a Gamma distributed GLM. The method on doing so was described in the previous chapter and it was found a linear relationship between the variance and the mean was a good model. The variance was then predicted using the grey value in the \texttt{aRTist} simulation.

This method for inference can be applied to the dataset \texttt{Sep16 120deg}. The 20 x-ray images were spilt into 2. 19 images were used to train the variance-mean model. One image, called the test image, was used to compare with \texttt{aRTist}, as shown in Figure \ref{fig:inference_scan_aRTist}.

\begin{figure}
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/inference/scan.eps}
        \caption{X-ray image}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/inference/aRTist.eps}
        \caption{\texttt{aRTist} simulation}
    \end{subfigure}
    }
    \caption{An x-ray scan of a 3D printed cuboid, from the \texttt{Sep16 120deg} dataset. This can be compared directly to the \texttt{aRTist} simulation for defects.}
    \label{fig:inference_scan_aRTist}
\end{figure}

For each pixel, a $z_{i,j}$ statistic was calculated. A $z_{i,j}$ statistic too large in magnitude can be considered to be evidence of a positive result. Another way to represent the $z_{i,j}$ statistic is the $p$-value which is given as
\begin{equation}
    p_{i,j} = 2(1-\Phi(\|z_{i,j}\|))
\end{equation}
which can takes values $0\leqslant p_{i,j} \leqslant 1$. A $p$-value too small is considered to be evidence of a positive result. The result $z$ statistics and $p$-values are shown in Figure \ref{fig:logp_z}.

\begin{figure}
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/inference/logp.eps}
        \caption{$-\log p\text{-values}$}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/inference/z_image.eps}
        \caption{$z$ statistics}
    \end{subfigure}
    }
    \caption{The resulting z statistics and $p$ values comparing an x-ray scan with the \texttt{aRTist} simulation.}
    \label{fig:logp_z}
\end{figure}

The resulting $z$ statistics and $p$-values are concerning. This is because the $p$-values are not very smooth on the surfaces of the sample. It should be expected that small $p$-values are in areas of the defects. Significant pixels were chosen for $\|z\|>\input{../figures/inference/z_critical.txt}$, this value was chosen by using the \cite{benjamini1995controlling} algorithm at the $z_\alpha = 2$ significance level. The significant pixels are shown in Figure \ref{fig:sig_pixels}.

This proposed method for defect detection failed because too many false positives were detected. These false positives appear to have some structure, for example clustering in the corners or on surfaces. In addition, false negatives were detected because not all of the defects were detected.

Model misspecification appears to be the main source of error. The $z$ statistics can be inspected using a histogram, as shown in Figure \ref{fig:z_histo}. The histogram of the $z$ statistics do not look Normal which seems to suggest that the assumption of $z_{i,j}\sim \normal(0,1)$ is incorrect. However this assumption can relaxed and can be done using the empirical null \citep{efron2004large}.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{../figures/inference/sig_pixels.eps}
    \caption{Signficiant pixels highlighted at the $z_\alpha = 2$ significance level.}
    \label{fig:sig_pixels}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{../figures/inference/z_histo.eps}
    \caption{Histogram of the $z$ statistics}
    \label{fig:z_histo}
\end{figure}

This chapter will cover hypothesis testing, for a single test and then for multiple tests. The empirical null will then be reviewed which then can be extended to an image filter, called the empirical null filter. This filter will adjust each $z$ statistic according to its neighbours, ironing out false positive results. Simulations and actual results will be shown at the end of the chapter.

\section{Hypothesis Testing}

Hypothesis testing dates back to \cite{pearson1900on}, \cite{neyman1933on} and \cite{fisher1970statistical}. It is so fundamental in science that it is now taught in schools.

The focus on this section is on the single hypothesis test. Here the random variable $Z\sim\normal(\mu,1)$ will be studied here as this is of interest in this project. It also appears in other tests such as the difference in sample means.

In the context of this project, an example of hypothesis testing will be given here. Here, it was assumed that $Z\sim\normal(0,1)$. A null hypothesis can be written down to describe this
\begin{equation}
    H_0:\mu=0 \ .
\end{equation}
It specifies the assumptions made on the random variable $Z$. Any data behaving as assumed is treated as a negative result.

A positive result is obtained when unlikely data is obtained and this happens if $Z$ deviates too much from 0. This can be described as testing $H_0$ against an alternative hypothesis $H_1$ where
\begin{equation}
    H_1:\mu\neq0 \ .
\end{equation}
How much $Z$ deviates from 0 to be considered a positive is up to the user but typically $\|Z\|>z_\alpha$ where $z_\alpha =2$ is considered significant.

A way to quantify the threshold for a positive result is to use something called the size of the test, otherwise known as the significance level, denoted as $\alpha$. This is the probability of a false positive result, which can be denoted as
\begin{equation}
    \prob(\|Z\|>z_\alpha|H_0) = \alpha \ .
\end{equation}
Using the fact the Normal distribution is symmetric then
\begin{equation}
    2(1 - \Phi(z_\alpha)) = \alpha \ .
    \label{eq:inference_single_alpha}
\end{equation}
The choice of $z_\alpha=2$ will set the size of the test to be $\alpha\approx 4.55\%$. This sort of choice of threshold and size is sensible.

The $p$-value is a way to represent an observation of the data $Z=z$. Similar to the size, the $p$-value is
\begin{equation}
    p=2(1-\Phi(\|z\|)) \ .
\end{equation}
As a result, the $p$-value can be compared directly to the size of the test. That is there is a positive result if $p<\alpha$, otherwise it is a negative result.

The power of the test is defined as the probability of a true positive result. For unknown $\mu$, it is useful to investigate the power for a range of $\mu$.

\section{Multiple Hypothesis Testing}

In this section, the single hypothesis test will be extended and the multiple hypothesis testing \citep{shaffer1995multiple, dudoit2003multiple} shall be reviewed.

Suppose $n$ test statistics were obtained $Z_1,Z_2,\dotdotdot,Z_n$ where $Z_i\sim\normal(\mu,1)$ for $i=1,2,\dotdotdot,n$ and i.i.d. One can test each of these statistics for the following hypotheses $H_0:\mu=0$ and $H_1:\mu\neq 0$. Using the method from the single hypothesis test at the $z_\alpha=2$ level, any statistic where $\|Z_i\|>2$ for $i=1,2,\dotdotdot,n$ would be considered a positive result.

Bringing in methods from single hypothesis testing to the multiple version has drawbacks. Assuming that the null hypothesis is true for all $Z_i$ for $i=1,2,\dotdotdot,n$, then by definition $\alpha\approx 4.55\%$ of the data will be tested positive, and falsely positive. Still under the same assumption, the probability of at least one false positive result, known as the family-wise error rate (FWER) \citep{shaffer1995multiple}, is
\begin{align}
    \text{FWER}&=1-\prob(\text{no false positives}) \nonumber\\
    \text{FWER}&=1-\prob(\text{no positive results}|H_0) \nonumber\\
    \text{FWER}&=1-\left[(1-\alpha)^n\right] \label{eq:inference_fwer}\ .
\end{align}
To put into context, for $\alpha\approx 4.55\%$ and $n=20$ then remarkably $\text{FWER}\approx60.6\%$. This method for multiple hypothesis testing will have too many false positive results.

While the uncorrected method for multiple hypothesis testing has it flaws, it does control the per comparison error rate (PCER) \citep{benjamini1995controlling}. PCER is the proportion of false positives out of all tests. There are corrections for multiple hypothesis testing which controls different things. Conventionally they are expressed in terms of the number of errors made in testing, these are defined in Table \ref{table:inference_randomvariables}. Using the notation, formally PCER is defined as
\begin{equation}
    \text{PCER}=
    \dfrac{
        1
    }
    {
        n
    }
    \expectation[V]
    \ .
\end{equation}
It can be seen that if the uncorrected test controls the false positive rate such that $\expectation[V]/n_0 = \alpha$, then it controls the PCER such that
\begin{equation}
    \text{PCER}\leqslant\alpha \ .
\end{equation}

\begin{table}
    \centering
    \begin{tabular}{c|c|c|c}
        &Negative&Positive&Total\\\hline
        True null & $U$ & $V$ & $n_0$\\
        Non-true null & $T$ & $S$ & $n-n_0$\\\hline
        &$n-R$&$R$&$n$
    \end{tabular}
    \caption{Random variable definitions for the different types of error made in multiple hypothesis testing}
    \label{table:inference_randomvariables}
\end{table}

The Bonferroni correction \citep{shaffer1995multiple, bland1995multiple, perneger1998what} controls the FWER. This is done by adjusting the size of the test to be $\alpha/n$. By using the adjusted size, Equation \eqref{eq:inference_fwer} becomes
\begin{equation}
    \text{FWER}=1-\left[(1-\alpha/n)^n\right]\ .
\end{equation}
Using the approximation $(1-\alpha/n)^n\approx = 1-\alpha$ then
\begin{equation}
    \text{FWER}\approx \alpha \ .
\end{equation}
This shows that the Bonferroni correction controls the family-wise error rate such that
\begin{equation}
    \text{FWER} \leqslant \alpha \ .
\end{equation}
However in practice the Bonferroni correction is not very powerful \citep{perneger1998what}, meaning it does give too many false negative results. This is because the correction traded too many false positives for false negatives.

The \cite{benjamini1995controlling} (BH) procedure controls the false discovery rate (FDR) \citep{benjamini2010discovering} rather than the false positive or family-wise rate. The FDR is the proportion of false positives out of all positive results, that is
\begin{equation}
    \text{FDR} = \expectation\left[
        \dfrac{V}{R}
    \right]
    \ .
\end{equation}
It is defined that $V/R=0$ when $R=0$.

In practice the BH procedure adjusts the size of the test between the Bonferroni correction and the uncorrected test. The BH procedure also adapts to the data, meaning the procedure chooses different sizes for different data.

The procedure is as follows. Suppose $n$ test statistics were obtained $Z_1,Z_2,\dotdotdot,Z_n$. They will need to be converted to $p$-values and ordered such that $p_{(1)}\leqslant p_{(2)}\leqslant \dotdotdot \leqslant p_{(n)}$. The adjusted size $\alpha_{\text{BH}}$ is
\begin{equation}
    \alpha_{\text{BH}} = \frac{\alpha k}{n}
\end{equation}
where
\begin{equation}
    k\text{ is the largest }i\text{ for which }p_{(i)}\leqslant\frac{i}{n}\alpha
    \ .
\end{equation}
This essentially declare $p_{(1)},p_{(2)},\dotdotdot,p_{(k)}$ as positive results. For the case where $p_{(1)}>\alpha/n$, then $k=1$ shall be chosen so that the Bonferroni correction is used instead. The BH procedure comes from the fact that assuming the null hypothesis is true for all $Z_i$ and all tests are independent, then the $p$-values are uniformly distributed \citep{simes1986improved}. However it can be shown that the BH procedure works for many scenarios of dependencies \citep{benjamini2001control}. 

It can be shown that the BH procedure controls the FDR such that
\begin{equation}
    \text{FDR}\leqslant\pi_0\alpha\leqslant\alpha
\end{equation}
where $\pi_0=n_0/n$ \citep{benjamini1995controlling}, this is usually unknown in practice.

In summary, the uncorrected, Bonferroni and BH correction controls for different error rates according to the threshold $\alpha$, summarised in Table \ref{table:inference_corrections}.

\begin{table}
    \centering
    \begin{tabular}{c|c}
        Correction&Controls for\\\hline
        No correction&$\text{PCER}\leqslant\alpha$\\
        Bonferroni&$\text{FWER}\leqslant\alpha$\\
        BH&$\text{FDR}\leqslant\alpha$
    \end{tabular}
    \caption{Different types of corrections for multiple hypothesis testing are listed here, along with what they control for.}
    \label{table:inference_corrections}
\end{table}

\subsection{Examples}

A few numerical examples will be shown here to explore the properties of the 3 different types of method to do multiple hypothesis testing. Two scenarios will be shown here. In the first case. 1\,000 $\normal(0,1)$ test statistics are simulated, all test statistics are all null. In the second case, 800 null $\normal(0,1)$ and 200 alternative $\normal(2,1)$ test statistics were simulated. 

\begin{figure}
	\centering
	\includegraphics[width=0.7\textwidth]{../figures/inference/nullhisto.eps}
	\caption{A histogram of simulated of 1\,000 null test statistics. Also provided are the critical boundaries at the $z_\alpha=2$ level, one uncorrected for multiple testing, another corrected using the Bonferroni correction.}
	\label{fig:inference_nullhisto}
\end{figure}

A histogram of the test statistics in the all null case is shown in Figure \ref{fig:inference_nullhisto}. The critical boundaries are also shown. In the uncorrected case, the critical region is $\|Z\|>2$. Using the Bonferroni correction, the critical region becomes $\|Z\|>4.08$. By inspection of the histogram, the uncorrected method tested a number of test statistics to be positive and falsely so. The Bonferroni correction performs well because it tested all the test statistics as negative correctly.

\begin{figure}
	\centering
	\includegraphics[width=0.7\textwidth]{../figures/inference/nullpvalues.eps}
	\caption{Ordered $p$-values of simulated 1\,000 null test statistics. Any values below the critical line are declared positive using the BH procedure.}
	\label{fig:inference_nullpvalues}
\end{figure} 

The ordered $p$-values are shown in Figure \ref{fig:inference_nullpvalues}. The $p$-values form a straight line as a result of being uniformly distributed. The critical line is $0.0455\times\text{order}/1\,000$. This corresponds to the BH procedure and $p$-values below where the 2 curves intersect are considered positive results. In the all null case, the $p$-values are all larger than the critical line, thus no test statistic were tested positive using the BH procedure. As a result, the Bonferroni and BH procedure tested all test statistics to be negative correctly.

\begin{figure}
	\centering
	\includegraphics[width = 0.7\textwidth]{../figures/inference/althisto.eps}
	\caption{A histogram of simulated of 800 null $\normal(0,1)$ and 200 non-null $\normal(2,1)$ test statistics. Also provided are the critical boundaries at the $z_\alpha=2$ level using no correction, Bonferroni correction and the BH procedure.}
	\label{fig:inference_althisto}
\end{figure}

\begin{figure}
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/inference/altpvalues.eps}
        \caption{All $p$-values}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/inference/altpvalues_zoom.eps}
        \caption{Zoomed in}
    \end{subfigure}
    }
    \caption{Ordered $p$-values of simulated 800 null $\normal(0,1)$ and 200 non-null $\normal(2,1)$ test statistics. Any values below the critical line are declared positive using the BH procedure.}
	\label{fig:altpvalues}
\end{figure}

In the second scenario, 800 $\normal(0,1)$ and 200 $\normal(2,1)$ test statistics were simulated. The histogram of the simulated test statistics, along with the critical regions, is shown in Figure \ref{fig:inference_althisto}. The uncorrected and Bonferroni corrected critical regions are still the same as the first scenario. However this time the critical region using the BH procedure is different, it is $\|Z\|\geqslant\input{../figures/inference/alt_z_critical.txt}$.

The ordered $p$-values are shown in Figure \ref{fig:altpvalues}. There are $p$-values below the critical line according to the BH procedure, testing $\input{../figures/inference/alt_n_positive_bh.txt}$ test statistics as positive. The uncorrected test tested $\input{../figures/inference/alt_n_positive_uncorrected.txt}$ test statistics as positive.

This example shows a number of things. The BH procedure adapts to the data to control for the FDR. The uncorrected method did produce more positive results but at a cost of more false positives. It may seem unimpressive that the BH procedure only tested $\input{../figures/inference/alt_n_positive_bh.txt}$ positive results, but this is to control the FDR or the number of false positives out of all positive results.

Unfortunately not all 200 non-null test statistics were tested as positive, statisticians would describe it as not powerful. This is however unavoidable as a number of null and non-null test statistics will be similar in value. The aim of hypothesis testing is not to classify positive and negative results correctly, but rather highlight a number of test statistics which are worth investigating.

The error rates can be estimated as a result of knowing whenever each test statistic is truly null or non-null. Suppose the simulation was repeated $N$ times and $r_1,r_2,\dotdotdot,r_N$ test statistics were tested positive. From these positive results, suppose in addition that $v_1,v_2,\dotdotdot,v_N$ false positives were observed, following the notation in Table \ref{table:inference_randomvariables}. The error rates can be estimated
\begin{equation}
	\widehat{\text{PCER}} = \frac{1}{Nn}\sum_{i=1}^N v_i
\end{equation}
\begin{equation}
	\widehat{\text{FWER}} = \frac{1}{N} \sum_{i=1}^N \mathbb{I}(v_i\neq 0)
\end{equation}
\begin{equation}
	\widehat{\text{FDR}} = \frac{1}{N}\sum_{i=1}^N\dfrac{v_i}{r_i}\times\mathbb{I}(r_i\neq0)
	\ .
\end{equation}
The estimates of PCER and FDR is esentially the sample mean, so the standard error can be used to quantify the error. For the FWER, the variance of the Beta distribution is used to quantify the error
\begin{equation}
	\text{standard error of }\widehat{\text{FWER}} = 
	\sqrt{\dfrac{ab}{(a+b)^2(a+b+1)}}
\end{equation}
where $a = \sum_{i=1}^N \mathbb{I}(v_i\neq 0)$ and $b = N - a$.

\begin{table}
    \centering
    \input{../tables/inference_error_rate1.tex_table}
    \caption{Various error rates when using different types of corrections for multiple hypothesis testing at the $z_\alpha=2$ level. 1\,000 test statistics were simulated, all standard Normal. Error bars represent the standard errors after 1\,000 repeats of the experiment.}
    \label{table:inference_error_rate1}
\end{table}

\begin{table}
    \centering
    \input{../tables/inference_error_rate2.tex_table}
    \caption{Various error rates when using different types of corrections for multiple hypothesis testing at the $z_\alpha=2$ level. 1\,000 test statistics were simulated, 800 from the standard Normal, 200 from $\normal(2,1)$. Error bars represent the standard errors after 1\,000 repeats of the experiment.}
     \label{table:inference_error_rate1}
\end{table}

Error rates estimations for the two scenarios are shown in Tables \ref{table:inference_error_rate1} and \ref{table:inference_error_rate1} for $N=1\,000$. These estimations show that the 3 different types of multiple hypothesis controls for different rates. The uncorrected, Bonferroni and BH controls the PCER, FWER and FDR to be equal or below $\alpha\approx 0.455$ respectively, equality is met when all the test statistics are all truly null.

\section{Empirical Null}

By treating each pixel as a hypothesis test, methods for multiple hypothesis can be deployed. At the start of the chapter, a scan from the \texttt{Sep16 120deg} dataset was compared to the \texttt{aRTist} simulation, obtaining a $z$ image.

\begin{figure}
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/inference/sub_z_image.eps}
        \caption{$z$ image with section highlighted}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/inference/sub_z_histo.eps}
        \caption{Histogram of section}
    \end{subfigure}
    }
    \caption{The resulting $z$ image. A histogram of a section of the $z$ image is shown.}
    \label{fig:inference_sub_z}
\end{figure}

A $200\times200$ section of the $z$ image was investigated by plotting the histogram as shown in Figure \ref{fig:inference_sub_z}. One could simply use the BH procedure at the $z_\alpha=2$ level to get the critical region of $\|Z\|>\input{../figures/inference/sub_boundary.txt}$. However a quick look at the histogram will show that almost half of the pixels will be tested as positive. It is questionable whenever such number of positive results is sensible, in particular in an area where defects are not expected.

The histogram showed that the majority of the $z$ statistics are centred around 2 and has a bell shaped curve, a characteristic of the Normal distribution. If the data shows that the majority is centred elsewhere then expected, is it right to then to test the majority of the data as positive? This is an example of model misspecification because it was assumed that the $z$ statistics are distributed as $\normal(0,1)$ when in reality they are distributed as $\normal(\mu_0,\sigma_0^2)$.

The empirical null \citep{efron2004large} adjusts the assumptions made on the $z$ statistics according to the majority of the data and treating it as the norm, relaxing the assumptions needed. It requires the following assumptions: the majority of the data are truly null, non-null test statistics are rare and the null test statistics are Normal distributed.

The method will be described here. Suppose the following test statistics were obtained $z_1,z_2,\dotdotdot,z_n$. First the histogram needs to be smoothed. \cite{efron2004large} recommended using splines but a kernel density estimate \citep{parzen1962on, friedman2001elements} will be used here because it has simpler mathematical properties. The kernel density estimate is
\begin{equation}
	\widehat{p}(z)=
	\frac{1}{nh}
	\sum_{i=1}^n\phi\left(
		\dfrac{z_i-z}{h}
	\right)
\end{equation}
where $h$ is the bandwidth. The bandwidth was chosen such that
\begin{equation}
	h = 0.9n^{-1/5}\times\text{min}\left(s_z,\text{iqr}_z/1.34\right) + 0.16
\end{equation}
where $s_z$ and $\text{iqr}_z$ are the standard deviation and interquartile range. The parameters of the empirical null are estimated using the kernel density estimate. The resulting estimated density is shown in Figure \ref{fig:inference_sub_density_estimate}.

\begin{figure}
	\centering
    \centerline{
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/inference/sub_z_histo_nocritical.eps}
        \caption{Histogram}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../figures/inference/sub_z_parzen.eps}
        \caption{Kernel density estimate}
    \end{subfigure}
    }
    \caption{The density of the test statistics can be estimated using a histogram or a kernel density estimate.}
    \label{fig:inference_sub_density_estimate}
\end{figure}

 Next, the mode of the kernel density estimate, denoted as $\widehat{\mu}_0$. is to be found. This is then used as the mean of the empirical null. The calculation of $\widehat{\mu}_0$ can be numerically using the Newton-Raphson method. Using the mode, the standard deviation of the empirical null is estimated using
 \begin{equation}
 	\widehat{\sigma}_0 = \left[
 		\left.
 			-\dfrac{\partial^2}{\partial z^2}\ln\widehat{p}(z)
 		\right|_{z=\widehat{\mu}_0}
 	\right]^{-1/2}
 \end{equation}

\subsection{Statistical Details}

\subsection{Mode Finding}

\subsection{Bandwidth Selection}

\subsection{Results}

\section{Empirical Null Filter}

\section{Results}