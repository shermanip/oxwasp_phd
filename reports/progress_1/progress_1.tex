\documentclass[a4paper]{proc}

\title{18 month progress report}
\author{Sherman Ip \quad Statistics \quad University of Warwick \quad March 31, 2017}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{graphicx}
\usepackage[outdir=./]{epstopdf}
\usepackage{natbib}
\usepackage{url}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{hyphenat}

\DeclareMathOperator{\expfamily}{ExpFamily}
\DeclareMathOperator{\expectation}{\mathbb{E}}
\DeclareMathOperator{\variance}{\mathbb{V}ar}
\DeclareMathOperator{\cov}{\mathbb{C}ov}
\DeclareMathOperator{\corr}{\mathbb{C}orr}
\DeclareMathOperator{\bernoulli}{Bernoulli}
\DeclareMathOperator{\betaDist}{Beta}
\DeclareMathOperator{\dirichlet}{Dir}
\DeclareMathOperator{\bin}{Bin}
\DeclareMathOperator{\MN}{Multinomial}
\DeclareMathOperator{\prob}{\mathbb{P}}
\DeclareMathOperator{\trace}{Tr}
\DeclareMathOperator{\normal}{N}
\DeclareMathOperator{\gammaDist}{Gamma}
\DeclareMathOperator{\poisson}{Poisson}

\newcommand{\RSS}{\mathrm{RSS}}
\newcommand{\euler}{\mathrm{e}}
\newcommand{\diff}{\mathrm{d}}
\newcommand{\T}{^\textup{T}}
\newcommand{\dotdotdot}{_{\phantom{.}\cdots}}
\newcommand{\BIC}{\textup{BIC}}

\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vectGreek}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\mathsf{#1}}

\begin{document}
\maketitle

\begin{abstract}
X-ray CT can be used for defect detection and quality control of 3D printing. This report will outline statistical methods for comparing x-ray scans of a 3D printed sample with its computer model under the face of uncertainty. Shading correction methods and the mean and variance relationship were investigated. Lastly the compound Poisson was reviewed.
\end{abstract}

\section{Introduction}
3D printing, or additive layer manufacturing \cite{kodama1981automatic}\cite{wong2012review}, can be used to create objects with complicated shapes and geometry from a blueprint, or CAM model, on a computer.  Recently it has been commercialised and has served purposes in the medical field \cite{kang20163d} and engineering \cite{wong2012review}. The need for detecting defects from 3D printing is required in order to do quality control, especially if the 3D printed object is used for something critical for example body parts. 

X-ray computed tomography (CT) \cite{hounsfield1980computed}\cite{michael2001x}\cite{cantatore2011introduction} scans can be used to take images of a rotating 3D printed sample to obtain images of the sample at multiple angles. These x-ray images then can be combined to form a 3D projection and the detection of defects can start. However due to the random nature of x-ray photons in its production in an x-ray tube and attenuation from the x-ray tube to the detector, sources of error or noise is introduced. In order to do defect detection in a statistical manner, sources of error must be considered.

A mini-project on this topic was completed at 12 months of the PhD \cite{ip2016inside}. The main aim of that project was to conduct exploratory data analysis on a dataset of 100 images of an x-ray CT scan of a stationary 3D printed cuboid. Linear regressions were used to model the relationship between the variance and the mean grey value of each pixel. Separately latent variable models were fitted (or at least attempted) on the scans, such as PCA, factor analysis and the compound Poisson, to find sources of variance.

After the mini-project, the main aims during the PhD was to extend the mini-project by developing a statistic to aid in defect detection while considering the randomness of photons. Another aim was to build a model for the noise.

This report will outline a pre-process method called shading correction. The aim of shading correction is to remove shading effects before any statistical analysis is conducted. It was then investigated how different types of shading correction affected the mean and variance grey value of each pixel in a scan. Afterwards, the mean and variance relationship was modelled using a GLM with different link functions. After finding a good model for the mean and variance, a statistic was developed to compare a scan with a simulation in the face of uncertainty. Lastly the compound Poisson was studied with the hope it can aid in variance modelling and prediction.

\section{About the Data}
Data was collected prior to the mini-project. 20 black/grey/white (b/g/w) images were collected on 14/03/16 at around 1250. The b/g/w images are images obtained from the x-ray detector when exposed to x-rays at different settings. They are useful for calibrating the detector, thus b/g/w images can be called reference images. About 2 hours later, 100 images of a 3D printed cuboid (or sample) were taken. These images were obtained from the x-ray detector when exposed to x-rays with the sample between the x-ray source and detector. The 3D printed sample was stationary and not rotating throughout the scans. The sample was held at the bottom by foam. The detector used was the Perkin Elmer XRD 1621 digital x-ray detector, with settings shown in Table \ref{table:settings}.

\begin{table*}
	\centering
	\begin{tabular}{c|c|c|c}
		& Voltage (kV) & Power (W) & Exposure time (ms) \\
		\hline
		Black & 0 & 0 & 1000\\
		Grey & 85 & 1.7 & 1000\\
		White & 85 & 6.8 & 1000 \\
		Sample & 100 & 33 & 500
	\end{tabular}
	\caption{Setting of the x-ray CT scan when collecting images from the x-ray detector. Error bars were not given and the number of significant figures shown are as given.}
	\label{table:settings}
\end{table*}

The images were $(1\,996\times1\,996)$ pixels in size and are in greyscale with 16 bits, in other words each pixel can have grey values which take integer values from 0 to $2^{16}-1$ inclusive. The grey values are in arbitrary units. A sample of images are shown in Figure \ref{fig:image} and the histogram of grey values are shown in Figure \ref{fig:hist}.

\begin{figure*}
	\centering
	\includegraphics[width = \textwidth]{../figures/data/140316_image.png}
	\caption{A black, grey, white and sample image obtained from the x-ray detector.}
	\label{fig:image}
\end{figure*}

\begin{figure}
	\centering
	\includegraphics[width = 0.45\textwidth]{../figures/data/140316_histo.eps}
	\caption{Histogram of grey values from a black, grey, white and sample image obtained from the x-ray detector.}
	\label{fig:hist}
\end{figure}

\section{Shading Correction}
The x-ray detector was made up 32 panels, arranged in 2 rows and 16 columns. Unfortunately these panels can be observed in the images which can be seen by the panel-wise effects, as shown in Figure \ref{fig:image}. The effects can be described as introducing a gradient to the grey values within panels and perhaps between panels and globally. The aim of shading correction is to remove these effects to estimate the image of the scan if it were to have none of these effects.

Let $N(x,y)$ be the grey value of the pixel at position $(x,y)$ in the obtained image. Similarly let $U(x,y)$ be they grey value of pixels of the shading free image. The shading effects can be modelled using a linear relationship
\begin{equation}
N(x,y) = U(x,y)b(x,y) + a(x,y)
\end{equation} 
where $b(x,y)$ is the multiplicative effect and $a(x,y)$ is the additive effect \cite{munzenmayer2003enhancing}. If it is possible to estimate the functions $b(x,y)$ and $a(x,y)$ using the estimators $\widehat{b}(x,y)$ and $\widehat{a}(x,y)$ respectively, then the shading-free image can be estimated using the estimator
\begin{equation}
\widehat{U}(x,y) = \frac{N(x,y)-\widehat{a}(x,y)}{\widehat{b}(x,y)} \times c_1 + c_2
\end{equation}
where $c_1$ and $c_2$ are user defined constants.
One method to estimate the images $b(x,y)$ and $a(x,y)$ is to use the black and white (b/w) images. Let $\overline{B}(x,y)$ and $\overline{W}(x,y)$ be the sample mean over the 20 black and white images respectively. The estimators for $b(x,y)$ and $a(x,y)$ are
\begin{equation}
\widehat{b}(x,y) = \overline{W}(x,y) - \overline{B}(x,y)
\end{equation}
and
\begin{equation}
\widehat{a}(x,y) = \overline{B}(x,y)
\end{equation}
respectively \cite{young2000shading}.

\subsection{Methods}
A new shading correction will be proposed here which extends the shading correction method described in the previous section by considering the grey images. The objective of shading correction is to predict the shading free image given the scan image, this can be done using linear regression of the form
\begin{equation}
U(x,y) = N(x,y)\beta(x,y)+\alpha(x,y) \ .
\end{equation}
The shading free image is not truly known, but one should expect the b/g/w shading free images should be completely flat.

Let $R_\text{b}(x,y)$, $R_\text{g}(x,y)$ and $R_\text{w}(x,y)$ be the grey value of the pixel, at position $(x,y)$, of the sample mean b/g/w images respectively. Let these images have $h$ rows and $w$ columns. The shading free b/g/w images should be completely flat, in other words all the pixels within a reference image should have the same grey value. A good guess what that shading corrected grey value should be would be the sample mean of the shading uncorrected grey values within the b/g/w images $\overline{R}_\text{b}$, $\overline{R}_\text{g}$ and $\overline{R}_\text{w}$ where
\begin{equation}
\overline{R}_j=\frac{\sum_{x=1}^w\sum_{y=1}^hR_j(x,y)}{wh}
\end{equation}
for $j\in\{\text{b},\text{g},\text{w}\}$. By treating these within image sample mean grey values as the true shading free grey value, then the problem becomes a linear regression with 3 data points.

Let the global sample mean be
\begin{equation}
\overline{R}=\frac{\overline{R}_\text{b} + \overline{R}_\text{g} + \overline{R}_\text{w}}{3}
\end{equation}
and the between image sample mean to be
\begin{equation}
\overline{R}_B(x,y)=\frac{R_\text{b}(x,y) + R_\text{g}(x,y) + R_\text{w}(x,y)}{3} \ ,
\end{equation}
then the estimators for $\beta(x,y)$ and $\alpha(x,y)$ are
\begin{multline}
\widehat{\beta}(x,y)=
\\
\frac{\sum_{j\in\{\text{b},\text{g},\text{w}\}}\left(R_j(x,y)-\overline{R}_B(x,y)\right)\left(\overline{R}_j-\overline{R}\right)}{\sum_{j\in\{\text{b},\text{g},\text{w}\}}\left(R_j(x,y)-\overline{R}_B(x,y)\right)^2}
\end{multline}
and
\begin{equation}
\widehat{\alpha}(x,y) = \overline{R}-\widehat{\beta}(x,y)\overline{R}_B(x,y) 
\end{equation}
respectively.

Suppose now an image to be shading corrected is obtained $N(x,y)$. The shading free image $U(x,y)$ can be estimated by interpolating the linear regression
\begin{equation}
\widehat{U}(x,y) = \widehat{\beta}(x,y)N(x,y)+\widehat{\alpha}(x,y) \ .
\end{equation}
This can be extended to fewer or more than 3 reference images. The disadvantage of having more reference images is that it does cost time to take more scans. Shading correction which uses b/g/w images shall be referred to as b/g/w shading correction. Similarly shading correction which uses only the b/w images shall be referred to as b/w shading correction.

Figure \ref{fig:shadingCorrection} shows an example of a scan before and after b/g/w shading correction. By inspection, the post shading correction background is smoother because the panel effects appeared to be gone, meeting the objective of shading correction. Figure \ref{fig:shadingCorrection_grad} shows the values of $\widehat{\beta}(x,y)$. By inspection there is some curvature in the $\widehat{\beta}$ image, with high gradients in the corners. Problems occur if the reference images behave differently from expected. For example some pixels may be dead and do not vary at all, this resulted in infinite gradients, shown by red circles in the figure. In order to combat this, any pixels with undefined grey values post shading correction were replaced with the sample median of its 8 nearest neighbours.

\begin{figure*}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/shadingCorrection/block.png}
		\caption{Shading uncorrected}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/shadingCorrection/block_shadingCorrected.png}
		\caption{b/g/w shading correction}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width = \textwidth]{../figures/shadingCorrection/polynomial_shadingCorrection.png}
		\caption{b/g/w 2nd order polynomial shading correction}
	\end{subfigure}
	\caption{A x-ray CT scan image before and after b/g/w shading correction.}
	\label{fig:shadingCorrection}
\end{figure*}

\begin{figure*}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{../figures/shadingCorrection/gradient.png}
		\caption{b/g/w shading correction}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{../figures/shadingCorrection/polynomial_gradient.png}
		\caption{b/g/w 2nd order polynomial shading correction}
	\end{subfigure}
	\caption{The gradient used in b/g/w shading correction. Red circles indicate infinite gradient, green circles indicate negative gradient.}
	\label{fig:shadingCorrection_grad}
\end{figure*}

To avoid dealing with dead pixels \cite{brettschneider2014spatial}, a method was considered which smoothed the reference images prior to any shading correction parameter estimation. The purpose of this was to iron out any misbehaving pixels in the reference images and to capture the shading effects parametrically. Second order panel-wise surface polynomials were fitted to the $R_\text{b}(x,y)$, $R_\text{g}(x,y)$ and $R_\text{w}(x,y)$ images, this is shown in Figure \ref{fig:polynomial_shadingCorrection}. The standardised residuals from the panel-wise fit are shown in Figure \ref{fig:polynomial_residual}. A quick inspection of the residuals shows any characteristics not captured by the panel-wise fit. In the black and grey images, the residuals showed a global ring which was very unusual for the black image because it is detecting no x-rays at all. The b/g/w residuals all showed vertical stripes. The panel-wise fit can be improved by including a global polynomial fit to capture between panel effects. Higher order polynomials can be considered but overfitting may be possible as it may attempt to capture all the characteristics of the b/g/w images.

Shading correction which uses the fitted polynomials as reference images will be referred to as b/g/w 2nd order polynomial shading correction. The gradient parameter for the b/g/w 2nd order polynomial shading correction is shown in Figure \ref{fig:shadingCorrection_grad}. It is smoother and robust because the gradients are of sensible values, for example no infinite values. The resulting shading correction is shown in Figure \ref{fig:shadingCorrection}. This method can be advantageous as the panel-wise 2nd order polynomial only captured panel effects parametrically and smoothed out clusters of bad behaving pixels as shown in the residuals. As a result, the shading correction is smoother between pixels because the bad behaving pixels do not impact as much in the interpolation of the linear regression.

\begin{figure*}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/shadingCorrection/polynomial_black.png}
		\caption{Black}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/shadingCorrection/polynomial_grey.png}
		\caption{Grey}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/shadingCorrection/polynomial_white.png}
		\caption{White}
	\end{subfigure}
	\caption{Second order panel-wise surface polynomials fit on the reference images.}
	\label{fig:polynomial_shadingCorrection}
\end{figure*}

\begin{figure*}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/shadingCorrection/residual_black.png}
		\caption{Black}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/shadingCorrection/residual_grey.png}
		\caption{Grey}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/shadingCorrection/residual_white.png}
		\caption{White}
	\end{subfigure}
	\caption{Standardised residuals from the second order panel-wise surface polynomials fit on the reference images.}
	\label{fig:polynomial_residual}
\end{figure*}

\subsection{Experiments}

An experiment was conducted to assess the performance of different types of shading correction. The methodology behind it is that shading correction applied on the reference images should remove shading effects and produce an image which is noisy but all the pixels should have the same mean grey value.

The 20 b/g/w images were spilt into a training set and test set of equal size randomly. The training set was used to estimate the parameters for shading correction. Afterwards, shading correction was applied to each image in the test set producing 10 shading corrected reference images of each grey scale. The post shading correction within pixel variance and between pixel variance was recorded, exactly the same procedure as ANOVA treating each pixel as a group. For each pixel to have the same mean grey value, the within and between pixel variance should be similar. The experiment was repeated 20 times by changing randomly the assignment of the training and test set.

Figure \ref{fig:shadingCorr_bgw} shows a sample of reference images post shading correction. Without shading correction, the shading effects were clear. With b/w and b/g/w shading correction, the reference images appeared flatter. The flatness of the black and grey image depended on if the grey images were used when training the shading correction. The b/g/w 2nd order polynomial shading correction retained some effects not captured by the panel wise polynomial fits. This was shown because the shading corrected images were very similar to the residual plots in Figure \ref{fig:polynomial_residual}.

Table \ref{table:bgw_anova} shows the within and between pixel variance of the grey values. The table shown that b/w and b/g/w shading correction does make an improvement because it brings the within and between pixel variances closer together. The performance of shading correction on the black and grey images appeared to depend on if the grey images were used in training the shading correction. The b/g/w 2nd order polynomial shading correction does perform by bringing the within and between pixel variances closer together but not as much as b/w and b/g/w shading correction.

\begin{figure*}
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/shadingCorrection/shadingCorrection_no_shad.png}
		\caption{No shading correction}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/shadingCorrection/shadingCorrection_bw.png}
		\caption{b/w shading correction}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/shadingCorrection/shadingCorrection_bgw.png}
		\caption{b/g/w shading correction}
	\end{subfigure}
		\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/shadingCorrection/shadingCorrection_polynomial.png}
		\caption{b/g/w 2nd order polynomial shading correction}
	\end{subfigure}
	\caption{Shading correction applied on reference images from a test set. A training set was used to train the shading correction.}
	\label{fig:shadingCorr_bgw}
\end{figure*}

\begin{table*}
	\centering
	\begin{tabular}{c|c|c}
		\input{../tables/bgwShadingANOVA_no_shad.tex_table}
	\end{tabular}
	\\
	\vspace{.25cm}
	a) No shading correction
	\\\vspace{.5cm}
	\begin{tabular}{c|c|c}
		\input{../tables/bgwShadingANOVA_bw.tex_table}
	\end{tabular}
	\\
	\vspace{.25cm}
	b) b/w shading correction
	\\\vspace{.5cm}
	\begin{tabular}{c|c|c}
		\input{../tables/bgwShadingANOVA_bgw.tex_table}
	\end{tabular}
	\\
	\vspace{.25cm}
	c) b/g/w shading correction
	\\\vspace{.5cm}
	\begin{tabular}{c|c|c}
		\input{../tables/bgwShadingANOVA_polynomial.tex_table}
	\end{tabular}
	\\
	\vspace{.25cm}
	d) b/g/w 2nd order polynomial shading correction
	\caption{Within pixel and between pixel variance of the shading corrected reference images. Values stated are the quartiles from 20 repeats of the experiment.}
	\label{table:bgw_anova}
\end{table*}

In conclusion, b/w and b/g/w shading correction did perform well because there was evidence they are attempting to iron out all the pixel grey values to have the same mean. The b/g/w 2nd order polynomial shading correction does its job. However because the panel wise polynomial under fitted and did not capture all the shading effects, the shading correction also did not capture all the shading effects.

The use of grey images in training the shading correction can be useful. It appeared that by considering the grey images, the shading correction performance on certain grey values was traded for other grey values. Additional methods for shading correction could be considered such as fitting a global polynomial in addition to the panel wise polynomials to the reference images. There exist methods which do not require reference images at all, such as entropy minimisation \cite{likar2000retrospective}, which could be studied further and may be useful here.

\section{Mean Variance Relationship}
The variance can be used to quantify the uncertainty of a grey value in a pixel. The purpose of modelling the mean and variance relationship of the grey value of a pixel is to be able to predict the grey value variance of each pixel for a single scan. This then can be used for inference when comparing a scan with the CAM model. A basic linear regression was conducted in the mini-project \cite{ip2016inside}, here generalised linear models (GLM) \cite{nelder1972generalized} \cite{mccullagh1984generalized} was used to model the mean and variance relationship.

In the mini-project, it was found the background and the foam holding the 3D printed sample have different mean variance relationships \cite{ip2016inside}. The object of interest is the 3D printed sample. Thus to model the mean variance relationship of the 3D printed sample, the 3D printed sample was segmented from the background and the foam. This was done by cropping the scans of the 3D printed sample in half, removing the bottom half containing the foam. Next a threshold was conducted using the following procedure: 
\begin{itemize}
	\item For each of the 20 b/g/w images, apply a panel wise median filter of size $3\times3$. Pixels outside the panels were symmetrically extended.
	\item Use these filtered reference images for shading correction.
	\item Take the mean over all 100 shading corrected scans.
	\item Keep pixels with grey values less than $4.7\times 10^4$.
\end{itemize}
This method of segmentation was developed by informal experimentation. The resulting segmented image is shown in Figure \ref{fig:segment}. Other segmentation methods could be considered but further study of other methods may not be necessary here.

\begin{figure}
	\centering
	\centerline{
	\includegraphics[width = 0.5 \textwidth]{../figures/meanVar/segment.png}
	}
	\caption{Resulting segmentation of the 3D printed sample}
	\label{fig:segment}
\end{figure}

The within pixel mean and variance grey value were estimated using the standard unbiased estimators. A few assumptions were made to justify the use of GLM to model the mean variance relationship. Firstly, it was assumed the standard error from estimating the mean was negligible. Secondly, it was assumed for a given pixel, the grey value from each scan were Normal i.i.d. Following from these assumptions, the sample variance is Gamma distributed with shape parameter $(n-1)/2$ where $n$ is the number of scans used to estimate the variance. Furthermore the standard error of the sample variance is proportional to $(n-1)^{-1/2}$, meaning the sample variance is more precise when more and more images were used to estimate the variance. This was experimentally shown in Figure \ref{fig:meanVar_sampleSize}, the spread of the sample variance decreased as $n$ increased.

GLM can model the sample variance as a Gamma distributed response variable, being explained by the sample mean through a link function $g(\text{variance})$. Fitting of the model was done using iterative re-weighted least squares. The dispersion parameter need not to be estimated because the shape parameter $(n-1)/2$ is known. A linear fit using the identity link did show that the Gamma response captured the spread of the sample variance for different sample means, this is shown in Figure \ref{fig:meanVar_sampleSize}.

\begin{figure*}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/meanVar/sample_size_25.png}
		\caption{$n=25$}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/meanVar/sample_size_50.png}
		\caption{$n=50$}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/meanVar/sample_size_75.png}
		\caption{$n=75$}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/meanVar/sample_size_100.png}
		\caption{$n=100$}
	\end{subfigure}
	\caption{Within pixel mean and variance grey value frequency density plot of each pixel in the segmented image of the 3D printed sample. The sample variance and sample mean were estimated using $n$ scans selected at random without replacement. A Gamma GLM was fitted with the identity link. The solid and dotted lines are the mean response and the $\Phi(\pm 1)$ quantiles respectively.}
	\label{fig:meanVar_sampleSize}
\end{figure*}

The systematic component is defined as a linear combination of the sample mean features, for example $\beta_0+\beta_1\text{mean}^p$ where $\beta_0$ and $\beta_1$ are constants to be estimated and $p$ is the polynomial order. In GLM, the link function equates to the systematic function to form a relationship between the mean and variance. 3 link functions were considered, identity, log and inverse, with the resulting mean and variance relationship shown in Table \ref{table:link_functions} and some fits in Figure \ref{fig:meanVar_link}.

\begin{table}
	\centering
	\begin{tabular}{c|c}
	Link function & Relationship \\
	\hline
	Identity & $\text{variance} = \beta_0+\beta_1\text{mean}^p$ \\
	Log & $\text{variance} = \exp{\left(\beta_0+\beta_1\text{mean}^p\right)}$ \\
	Inverse & $\text{variance} = \left(\beta_0+\beta_1\text{mean}^p\right)^{-1}$
	\end{tabular}
	\caption{Link functions and the resulting mean and variance relationship when using polynomial features.}
	\label{table:link_functions}
\end{table}

\begin{figure*}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/meanVar/meanVar_identity.png}
		\caption{Identity link, order 1}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/meanVar/meanVar_log.png}
		\caption{Log link, order -1}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/meanVar/meanVar_canonical_1.png}
		\caption{Inverse link, order -1}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/meanVar/meanVar_canonical_2.png}
		\caption{Inverse link, order -2}
	\end{subfigure}
	\caption{Shading uncorrected within pixel mean and variance grey value frequency density plot of each pixel in the segmented image of the 3D printed sample. A Gamma GLM was fitted with different link functions and polynomial orders. The solid and dotted lines are the mean response and the $\Phi(\pm 1)$ quantiles respectively.}
	\label{fig:meanVar_link}
\end{figure*}

Different shading corrections had small effects on the mean variance relationship, as shown in Figure \ref{fig:meanVar_shadingCorrection}. Due to some misbehaving pixels in the reference images, grey value outliers were possible such as negative values. Pixels with grey values less than $5.8\times 10^3$ or more than $6.6\times 10^4$ were considered outliers. Outliers were replaced by the median of its 8 post shading correction nearest neighbours.

\begin{figure*}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/meanVar/shadingCorrection_no_shad.png}
		\caption{No shading correction}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/meanVar/shadingCorrection_bw.png}
		\caption{b/w shading correction}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/meanVar/shadingCorrection_bgw.png}
		\caption{b/g/w shading correction}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/meanVar/shadingCorrection_polynomial.png}
		\caption{b/g/w 2nd order polynomial shading correction}
	\end{subfigure}
	\caption{Post shading correction within pixel mean and variance grey value frequency density plot of each pixel in the segmented image of the 3D printed sample. A Gamma GLM was fitted with the identity link. The solid and dotted lines are the mean response and the $\Phi(\pm 1)$ quantiles respectively.}
	\label{fig:meanVar_shadingCorrection}
\end{figure*}

\subsection{Experiments}

An experiment was conducted to assess the GLM fit on the mean and variance data for different link functions, polynomial orders and shading corrections.

For a given link function, polynomial order and shading correction, the 100 images of the 3D printed sample were spilt into two equally sized sets: the training set and the test set. The within pixel sample mean and sample variance grey value were estimated for both the training set and test set separately. The GLM was fitted onto the mean variance data obtained from the training set. The fitted model was then used to predict the variance given the mean. The predicted variance given the mean then can be compared with the sample variance.

There is sampling error in the sample variance and the magnitude of the error varies for different means. In order to take the varying sampling error into account, the mean standardised squared error (MSSE) was used to assess the performance of the fitted GLM predicting the variance given the mean. The errors were standardised by dividing by the standard deviation of the GLM response.

The experiment was repeated 100 times by reassigning the training and test set randomly.

The training and test MSSE are shown in Table \ref{table:training}. NaN values were obtained when the iterative reweighted least squares failed to converge. By using b/g/w shading correction, the test MSSE improved which suggests that b/g/w shading correction aids in variance prediction. b/w shading correction increased the MSSE by a magnitude of $\sim\times10^3$. A quick residual plot in Figure \ref{fig:glm_residual} showed that residuals were more centred at zero for b/g/w shading correction compared to b/w shading correction. b/w shading correction struggled to correct the grey pixels correctly, creating bias in the model.

The performance of the different link functions were very similar and no performance was gained by using higher order polynomials.

\begin{table*}
\centering
\centerline{
\begin{tabular}{c|c|c|c|c}
\input{../tables/GLM_meanVar_trainingTest_training.tex_table}
\end{tabular}}
\vspace{.25cm}
a) Training error
\\\vspace{.5cm}
\centerline{
\begin{tabular}{c|c|c|c|c}
\input{../tables/GLM_meanVar_trainingTest_test.tex_table}
\end{tabular}}
\vspace{.25cm}
b) Test error
\\\vspace{.5cm}
\caption{Mean squared standardised error using GLM with identity link to predict the variance given the mean. Values stated are the quartiles from 100 repeats.}
\label{table:training}
\end{table*}

\begin{figure*}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/meanVar/residual_no_shad.png}
		\caption{No shading correction}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/meanVar/residual_bw.png}
		\caption{b/w shading correction}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width = \textwidth]{../figures/meanVar/residual_bgw.png}
		\caption{b/g/w shading correction}
	\end{subfigure}
	\caption{Frequency density plot of the standardised residuals from predicting the variance given the mean using data in a test set. The model used is a GLM with identity link function.}
	\label{fig:glm_residual}
\end{figure*}

In conclusion it was found that b/g/w shading correction aid in variance prediction by making the mean variance data more linear. A simple identity link performed just as well as other link functions which suggests a linear relationship between the variance and the mean could be sufficient enough for the purpose of variance prediction.

\section{Inference}

A statistic was developed which compares each pixel of a single scan of a 3D printed sample with the true image obtained from a computer model or simulation. Any outlying pixels then can be considered as a possible defect in the 3D printed sample. The statistic is based on the $Z$ statistic and is given as
\begin{equation}
Z(x,y) = \frac{\text{scan}(x,y) - \text{model}(x,y)}{\sqrt{\widehat{\variance}[\text{scan}(x,y)]}}
\end{equation}
%$Z(x,y)=\frac{\text{scan}(x,y)-\text{model}(x,y)}{\sqrt{\widehat{\mathbb{V}\text{ar}}[\text{scan}(x,y)]}}$
where $\widehat{\variance}[\text{scan}(x,y)]$ is the estimated grey value variance of a pixel at $(x,y)$ of the scan. The estimated variance can be obtained using a GLM model as discussed in the previous section. In addition, the scan image should be shading corrected to remove as much shading effects as possible. Assuming the $Z$ statistics are standard Normal distributed, multiple hypothesis testing can be used to test each pixel to find significant or outlying pixels, such as the Bonferroni correction.

\subsection{Experiments}
An experiment was conducted to investigate the behaviour of the $Z$ statistics. b/g/w shading correction was applied prior to the experiment and any outlying pixels were interpolated as discussed in the previous section. The 100 images were spilt into 3 parts:
\begin{itemize}
	\item 50 images in the training set to train the mean variance relationship model using GLM, as described in the previous section, with the identity link;
	\item 25 images in the truth set, the mean over these images were used as the true image;
	\item 25 images in the test set, each of these images will be compared to the true image.
\end{itemize}
Each image in the test set was compared to the true image by calculating the $Z$ statistic for each pixel. Thus for each pixel, a sample of 25 $Z$ statistics was obtained.

A sample of $Z$ statistics, along with significant pixels, are shown in Figure \ref{fig:p_values}. The Bonferroni correction was used to correct for multiple testing. The $Z$ statistics within image varied scan to scan, suggesting images in the test set were not quite as similar to each other as hoped for. Common significant pixels were picked up by the statistic, especially on the left edge. Fortunately only a few significant pixels were picked up from the 3D printed sample, implying only a few false positives were detected.

\begin{figure*}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width = \textwidth]{../figures/meanVar/p_values.png}
		\caption{$\Phi(Z)$}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width = \textwidth]{../figures/meanVar/critical_values.png}
		\caption{Significant pixels}
	\end{subfigure}
	\caption{A sample of $Z$ statistics obtained from comparing a scan from the test set to the true image. In a) the $Z$ statistics were transformed using the standard Normal c.d.f. In b) highlighted in red are significant pixels at the $5\sigma$ significance level corrected for multiple tests using the Bonferroni correction.}
	\label{fig:p_values}
\end{figure*}

The $Z$ statistics within pixel were tested for Normality  using the $\chi^2$ goodness of fit test. To make the test more powerful, 75 more $Z$ statistics were collected by reassigning randomly the training, truth and test set and repeating the experiment 3 more times. Figure \ref{fig:chi_squared} showed that the $p$ values from the $\chi^2$ test were particularly low, especially on the bottom half of the image. In particular there is a vertical line of significant pixels as well. Normality may be a valid assumption but the bottom of the image where the foam is may break such an assumption.

\begin{figure*}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width = \textwidth]{../figures/meanVar/chi_squared_critical.png}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width = \textwidth]{../figures/meanVar/chi_squared_p_values.eps}
	\end{subfigure}
	\caption{$p$ values from the $\chi^2$ goodness of fit test within pixel. Highlighted in red are significant pixels at the $2\sigma$ significant level corrected for multiply testing using the Bonferroni correction.}
	\label{fig:chi_squared}
\end{figure*}

\subsection{Conclusion}
The $Z$ statistics looked like a good candidate to make inference from a single scan of a 3D printed sample. There is further work to be done. Rather than using a subset of the data as the true image, simulations can be treated as the true image to investigate how well the $Z$ statistic perform in real life defect detection. Further multiple hypothesis testing should be investigated and inference can be extended to the 3D reconstructed data of the 3D printed sample.

\section{Compound Poisson}
In the mini-project, a compound Poisson model was used to model the distribution of grey values in a pixel \cite{ip2016inside}. The model was built by assuming that photon arrivals are a Poisson process and that each photon has random energy, distributed according to an energy spectrum. The grey value was then proportional to the sum of energies of each photon detected in some exposure time, with some Normal noise added.

Consider a pixel. Let $Y$ be the number of photons detected for that particular pixel. Let $U$ be the energy of a photon post-attenuation and is typically random, here it was assumed to be Normal distributed. Let $X$ be the grey value of that pixel. The model is hierarchical such that at the top layer
\begin{equation}
Y\sim\poisson\left(\nu\tau\right)
\end{equation}
\begin{equation}
U_i\sim\normal\left(\mu,\sigma^2\right) \quad \text{ for i.i.d. }i=1,2,3,\dotdotdot
\end{equation}
where $\nu$ is the rate of photon arrivals, $\tau$ is the time exposure, $\mu$ and $\sigma^2$ is the mean and variance of photon energy respectively. Then
\begin{equation}
X = \alpha\sum_{i=1}^Y U_i + e
\end{equation}
where $e\sim\normal(0,\beta^2)$, $\beta^2$ is the thermal noise variance and $\alpha$ is some constant and can be interpreted as the gain. It was shown that this model can be written down as a latent variable model such that 
\begin{equation}
Y\sim\poisson\left(\nu\tau\right)
\end{equation}
\begin{equation}
X|Y\sim\normal\left(\alpha\mu Y, \alpha^2\sigma^2 Y + \beta^2\right) \ .
\end{equation}

The model can be simplified by setting $\beta=0$. By doing so, the constant $\alpha$ becomes redundant as it can be simply absorbed by $\mu$ and $\sigma^2$. It was attempted to estimate the parameters $\nu$, $\mu$ and $\sigma^2$ for a given CT scan. An EM algorithm \cite{dempster1977maximum} was implemented but it suffered in the E step because the conditional expectation, that is $\expectation\left[Y|X\right]$ given the 3 parameters, was expressed as an infinite sum. Truncating the sum was not fesible because in some cases, a very large number of terms were needed to produce accurate results.

The main aims were to investigate the compound Poisson, approximations and methods for fitting it to data. The compound Possion then could be used in variance modelling or prediction in future work.

\subsection{Literature Review}
Compound Poisson was proposed to improve noise models in CT scans \cite{whiting2002signal}. Simple models have been used which assumed the greyvalues were Poisson distributed or Normal \cite{lu2002analytical} distributed. The compound Poisson improves on these models by considering the energy spectrum of x-ray quanta and integrating it into the x-ray detector \cite{whiting2006properties}. This has been useful in image reconstruction \cite{elbakri2003efficient}\cite{elbakri2002statistical}\cite{elbakri2001statistical}\cite{lasio2007statistical} and tested in experiments \cite{wang2008experimental}. The p.d.f.~of the grey values can be numerically obtained using the saddle point approximation for a given general energy spectrum \cite{elbakri2003efficient}.

A special case of the compound Poisson is the compound Poisson-Gamma model. The model is as follows:
\begin{equation}
Y\sim\poisson\left(\nu\tau\right)
\end{equation}
\begin{equation}
U\sim\gammaDist\left(\alpha,\lambda\right)
\end{equation}
\begin{equation}
X = \sum_{i=1}^Y U_i
\end{equation}
where $\alpha$ and $\lambda$ are the shape and rate parameters, $\nu$ is the photon rate and $\tau$ is the time exposure. Then it can be shown that
\begin{equation}
X|Y\sim\gammaDist\left(Y\alpha,\lambda\right) \ .
\end{equation}
This is special because the marginal $X$ is in the exponential family, more specifically in the Tweedie dispersion exponential family \cite{jorgensen1987exponential}.

It is very well known that the marginal p.d.f.~of $X$ is intractable. However the m.g.f.~can be written in closed form. The m.g.f.~is defined as
\begin{equation}
M(\theta)=\expectation\left[\euler^{\theta X}\right] \ .
\end{equation}
Using the property of conditional expectations
\begin{equation*}
M(\theta)=\expectation\expectation\left[\euler^{\theta X}\right|Y]
\end{equation*}
and then the properties of the Gamma distribution
\begin{equation*}
M(\theta)=\expectation\left[\left(\frac{\lambda}{\lambda-\theta}\right)^{Y\alpha}\right] \ .
\end{equation*}
Finally using the m.g.f.~of the Poisson distribution
\begin{equation}
M(\theta)=\exp\left[\nu\tau\left(\left(\frac{\lambda}{\lambda-\theta}\right)^{\alpha}-1\right)\right] \ .
\end{equation}
Moments up to a number of orders then can be obtained. For example
\begin{equation}
\expectation\left[X\right]=\frac{\alpha\nu\tau}{\lambda}
\end{equation}
\begin{equation}
\variance\left[X\right]=\frac{\alpha(\alpha+1)\nu\tau}{\lambda^2}
\end{equation}
\begin{equation}
\expectation\left[(X-\mu)^3\right] = \frac{\alpha(\alpha+1)(\alpha+2)\nu\tau}{\lambda^3}
\end{equation}
where $\expectation\left[X\right]=\mu$.

For a given i.i.d.~random sample of a compound Poisson-Gamma random variable, the unknown parameters $\nu$, $\alpha$ and $\lambda$, can be estimated using method of moments. Suppose $\widehat{\mu}$ is an estimator of $\expectation[X]$ and $\widehat{\mu}_j$ is an estimator of $\expectation\left[\left(X-\mu\right)^j\right]$ for $j=2,3$. Then the estimators
\begin{equation}
\widehat{\nu}=\frac{\widehat{\mu}^2\widehat{\mu}_2}{\tau\left(2\widehat{\mu}_2^2-\widehat{\mu}_3\widehat{\mu}\right)}
\end{equation}
\begin{equation}
\widehat{\alpha}=\frac{\widehat{\mu}_3\widehat{\mu}-2\widehat{\mu}_2^2}{\widehat{\mu}_2^2-\widehat{\mu}\widehat{\mu}_3}
\end{equation}
\begin{equation}
\widehat{\lambda}=\frac{\widehat{\mu}\widehat{\mu}_2}{\widehat{\mu}\widehat{\mu}_3-\widehat{\mu}_2^2}
\end{equation}
are method of moments estimators of $\nu$, $\alpha$ and $\lambda$ respectively \cite{withers2011compound}. These estimators suffer because estimation is not done through the sufficient statistics and can be negative, this is a problem because the parameters do not take non-positive values.

Estimation can also be done using the EM algorithm \cite{dempster1977maximum}, treating the random variable $Y$ to be latent. The problem with the EM algorithm is that the E step is not tractable. This problem can be tackled by truncating the infinite sum in a clever way so that the sum is over a number of high contributing terms to save computational cost \cite{dunn2005series}. Another way is to use MCMC in the E step \cite{booth1999maximizing}.

There exist approximate maximum log likelihood estimators. Under certain conditions, the random variable $X$ can be approximated using the Normal distribution and the likelihood can be maximised numerically. Another approximation is to use the saddle point approximation \cite{daniels1954saddlepoint} to approximate the p.d.f.~of $X$, thus an approximate likelihood can be obtained.

Lastly, because $X$ is in the dispersion exponential family, parameter estimation can be done through the GLM framework \cite{zhang2013likelihood}. The GLM framework and the saddlepoint approximation were used extensively for insurance claim data \cite{jorgensen1994fitting} \cite{jensen1991saddlepoint}.

\subsection{Probability Density Function}
The joint p.d.f.~$p_{X,Y}(x,y)$ has probability mass at $X=0$ and $Y=0$ with probability
\begin{equation}
\prob\left(X=0,Y=0\right)=\euler^{-\nu\tau} \ ,
\end{equation}
and continuous elsewhere
\begin{equation}
p_{X,Y}(x,y)=\frac{\lambda^{y\alpha}}{\Gamma(y\alpha)}x^{y\alpha-1}\euler^{-\lambda x}\euler^{-\nu\tau}\frac{(\nu\tau)^y}{y!}
\end{equation}
for $y=1,2,\dotdotdot$ and $x>0$. The marginal can be obtained by summing over all possible values of $y$. The marginal p.d.f.~has probability mass at $X=0$ with probability
\begin{equation}
\prob(X=0)=\euler^{-\nu\tau}
\end{equation}
and continuous elsewhere
\begin{equation}
p_{X}(x)=\sum_{y=1}^{\infty}\frac{\lambda^{y\alpha}}{\Gamma(y\alpha)}x^{y\alpha-1}\euler^{-\lambda x}\euler^{-\nu\tau}\frac{(\nu\tau)^y}{y!}
\end{equation}
for $x>0$. It is a mixture with one continuous component at $x>0$ and discrete mass at $x=0$. This makes the compound Poisson useful for modelling non-negative variables which can take values of 0. By inspection the marginal p.d.f.~cannot be written down in closed form.

\subsection{Normal Approximation}
The marginal p.d.f.~of $X$ can be approximated using the Normal distribution. Using the results of the mean and variance of $X$, the Normal approximation is
\begin{equation}
X\sim\normal\left(\frac{\alpha\nu\tau}{\lambda},\frac{\alpha(\alpha+1)\nu\tau}{\lambda^2}\right) \ .
\end{equation}
To assess when the Normal approximation is sensible to use can be done by investigating the m.g.f.~of $X$. The m.g.f.~can be written as
\begin{equation*}
M(\theta)=\exp\left[\nu\tau\left(\left(1-\frac{\theta}{\lambda}\right)^{-\alpha}-1\right)\right] \ .
\end{equation*}
Assuming $|\theta/\lambda|<1$ and using the binomial expansion
\begin{equation}
M(\theta)=\exp\left[\nu\tau\left(
\sum_{r=1}^{\infty}\frac{\theta^r}{r!\lambda^r}\prod_{t=1}^{r}(\alpha+t-1)
\right)\right] \ .
\end{equation}
This is similar to the m.g.f.~of the Normal distribution by considering the first 2 order terms
\begin{multline}
M(\theta)=\exp\left[\theta\frac{\alpha\nu\tau}{\lambda}+\frac{\theta^2}{2}\frac{\alpha(\alpha+1)\nu\tau}{\lambda^2}
\right.\\\left.
+\sum_{r=3}^\infty\nu\tau\frac{\theta^r}{r!\lambda^r}\prod_{t=1}^{r}(\alpha+t-1)
\right] \ .
\end{multline}
For the Normal approximation to be a good approximation, the third and higher order terms must be small. This can happen for large $\lambda$.

A more general result can be obtained. This can be done by taking a Taylor expansion of the m.g.f.~of the Gamma component in $M(\theta)$. That is let
\begin{equation}
M(\theta)=\exp\left[
\nu\tau\left(M_U(\theta)-1\right)
\right]
\end{equation}
where $M_U(\theta)$ is the m.g.f.~of $\gammaDist(\alpha,\lambda)$. Then taking the Taylor expansion
\begin{equation*}
M(\theta)=\exp\left[
\nu\tau\left(
\sum_{r=0}^\infty M_U^{(r)}(0)\frac{\theta^r}{r!}
-1
\right)
\right]
\end{equation*}
but because $M_U(0)=1$
\begin{equation}
M(\theta)=\exp\left[
\nu\tau
\sum_{r=1}^\infty M_U^{(r)}(0)\frac{\theta^r}{r!}
\right] \ .
\end{equation}
As before, this is similar to the m.g.f.~of the Normal distribution when considering the first 2 terms
\begin{multline}
M(\theta)=\exp\left[
\theta M_U'(0)\nu\tau+\frac{\theta^2}{2} M_U''(0)\nu\tau
\right.\\\left.+
\sum_{r=3}^\infty M_U^{(r)}(0)\frac{\theta^r}{r!}\nu\tau\right] \ .
\end{multline}
Thus for the Normal approximation to be good, the parameters $\alpha$ and $\lambda$ must be such that the third and higher moments of $\gammaDist(\alpha,\lambda)$ are much smaller than the first two moments.

\subsection{Saddle Point Approximation}
For a given m.g.f., that is
\begin{equation}
M(\theta)=\int_{-\infty}^{\infty}\euler^{x\theta} p_X(x) \diff x \ ,
\end{equation}
the saddle point approximation \cite{daniels1954saddlepoint} \cite{butler2007saddlepoint} finds an approximate $p_X(x)$. The saddle point approximation is given as
\begin{equation}
p_X(x)\approx\left(2\pi K''(s)\right)^{-1/2}\exp\left[K(s)-sx\right]
\end{equation}
where $K(\theta) = \ln\left(M(\theta)\right)$, and $s=s(x)$ is the solution to the saddle point equation $K'(s)=x$.

For the compound Poisson-Gamma distribution, the saddle point approximation is given in Equation \eqref{eq:saddle_point_approx} and is valid only for $x>0$. The integral of the density approximation over the support may not equal to one, it can be numerically re-normalised if necessary.

\begin{figure*}[htb]
\begin{multline}
p_X(x)\approx
\left(2\pi(\alpha+1)\right)^{-1/2}\left(\nu\tau\lambda^\alpha\alpha\right)^{1/(2(\alpha+1))}\euler^{-\nu\tau}
\\
\exp\left[
-\frac{\alpha+2}{2(\alpha+1)}\ln(x)-x\lambda+\left(\left(\frac{x\lambda}{\alpha}\right)^{\alpha}\nu\tau\right)^{1/(\alpha+1)}(\alpha+1)
\right]
\label{eq:saddle_point_approx}
\end{multline}
\end{figure*}

\subsection{Simulations}
Exact simulation of the compound Poisson distribution is easy, this is done by simulating the latent variable $Y$ and then simulating the observable given that latent variable, $X|Y$. An experiment was conducted to assess how good the density approximations are. This was done by simulating $10^4$ samples of a compound Poisson random variable, and then comparing the histogram of the samples with the density approximation. Three sets of parameters were chosen and the results are shown in Figure \ref{fig:histogram}.

For low $\nu\tau$, there is a chance of simulating zeros. The Normal approximation failed to capture the probability mass at zero because the Normal distribution was constrained to be symmetric. The saddlepoint approximation did capture probability mass at zero as shown by an increase in probability density towards zero. However for a distinct mixture looking compound Poisson, Figure \ref{fig:histogram} (d), the saddlepoint approximation was not flexible enough to capture each individual peak.

For the Normal looking compound Poisson, both approximations performed well.
\begin{figure*}
\centerline{
	\centering
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=1\textwidth]{../figures/compoundPoisson/normal_1.eps}
        \caption{Normal approximation: $\nu\tau=1$, $\alpha=1$, $\lambda=1$}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=1\textwidth]{../figures/compoundPoisson/saddle_1.eps}
        \caption{Saddlepoint approximation: $\nu\tau=1$, $\alpha=1$, $\lambda=1$}
    \end{subfigure}
}
\centerline{
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=1\textwidth]{../figures/compoundPoisson/normal_2.eps}
        \caption{Normal approximation: $\nu\tau=3$, $\alpha=400$, $\lambda=100$}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=1\textwidth]{../figures/compoundPoisson/saddle_2.eps}
        \caption{Saddlepoint approximation: $\nu\tau=3$, $\alpha=400$, $\lambda=100$}
    \end{subfigure}
}
\centerline{
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=1\textwidth]{../figures/compoundPoisson/normal_3.eps}
        \caption{Normal approximation: $\nu\tau=500$, $\alpha=80$, $\lambda=2$}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=1\textwidth]{../figures/compoundPoisson/saddle_3.eps}
        \caption{Saddlepoint approximation: $\nu\tau=500$, $\alpha=80$, $\lambda=2$}
    \end{subfigure}
}
\caption{A sample of $10^4$ compound Poisson random variables was simulated. The histogram of the samples was plotted along with the density approxmation.}
\label{fig:histogram}
\end{figure*}

\subsection{Parameter estimation}
The saddlepoint approximation did capture the probability mass at zero, however the formula for the density approximation does not support zero. As a result, the saddlepoint approxmation may not be suited to approximate the likelihood when there is a good chance of obtaining zeros in the data. A way around it is to adjust the data by adding a very small constant to all zeros in the data.

An experiment was conducted to optimise the saddle point likelihood approximation numerically using the quasi-Newton method. For a given set of compound Poisson parameters, this was done by simulating 100 i.i.d.~samples. Any zeros in the sample were converted to a small constant, the constant being 1/8th of the minimum of all non-zero data in the sample, this was done to make the approximated likelihood evaluable. The likelihood was then optimised using \texttt{MATLAB}'s \texttt{fminunc} function with the default settings. The initial value was set at the true value to see how much the optimiser diverges from the true value. This was repeated 1\,000 times to obtain an empirical sampling distribution of the 3 estimators.

2 sets of parameters, carried forward from the previous section were used: $\{\nu\tau=3,\alpha=400,\lambda=100\}$ and $\{\nu\tau=500,\alpha=80,\lambda=2\}$. Figure \ref{fig:sampling_dist} shows the empirical sampling distribution from the experiment. For these particular examples, the estimators appeared to be unbiased with nice behaving distributions and finite variance. However for the $\{\nu\tau=500,\alpha=80,\lambda=2\}$ case, the variance looked too small to be true. It was found that the estimator was extremely sensitive to the initial value and the optimiser may have not moved much from the initial value, this seems to be the case here.

\begin{figure*}
\centerline{
	\centering
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=1\textwidth]{../figures/compoundPoisson/sampling_dist_1_nu.eps}
        \caption{$\nu\tau$ estimates for $\nu\tau=3$, $\alpha=400$, $\lambda=100$}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=1\textwidth]{../figures/compoundPoisson/sampling_dist_2_nu.eps}
        \caption{$\nu\tau$ estimates for  $\nu\tau=500$, $\alpha=80$, $\lambda=2$}
    \end{subfigure}
}
\centerline{
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=1\textwidth]{../figures/compoundPoisson/sampling_dist_1_alpha.eps}
        \caption{$\alpha$ estimates for  $\nu\tau=3$, $\alpha=400$, $\lambda=100$}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=1\textwidth]{../figures/compoundPoisson/sampling_dist_2_alpha.eps}
        \caption{$\alpha$ estimates for $\nu\tau=500$, $\alpha=80$, $\lambda=2$}
    \end{subfigure}
}
\centerline{
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=1\textwidth]{../figures/compoundPoisson/sampling_dist_1_lambda.eps}
        \caption{$\lambda$ estimates for $\nu\tau=3$, $\alpha=400$, $\lambda=100$}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=1\textwidth]{../figures/compoundPoisson/sampling_dist_2_lambda.eps}
        \caption{$\lambda$ estimates for $\nu\tau=500$, $\alpha=80$, $\lambda=2$}
    \end{subfigure}
}
\caption{Sampling distribution of the maximum saddlepoint approximated likelihood estimators. Dotted line represent the true value.}
\label{fig:sampling_dist}
\end{figure*}

Figure \ref{fig:lnL} shows a plot of a typical log likelihood in the 3 parameter space. The log likelihood looks shallow and perhaps the optimiser in \texttt{MATLAB} may have struggled to improve estimations near the true value.

\begin{figure}
\centerline{\centering
\includegraphics[width=0.5\textwidth]{../figures/compoundPoisson/log_likelihood.eps}}
\caption{An example of the log likelihood of a sample of compound Poisson random variables with parameters: $\nu\tau=500$, $\alpha=80$, $\lambda=2$}
\label{fig:lnL}
\end{figure}

\subsection{Future work}
The \texttt{R} package \texttt{cplm} \cite{zhang2013likelihood} numerically estimate parameters for the compound Poisson in the GLM framework. However for large parameters, there were numerical problems and the estimators' performance were hit and miss. Future work will rely on this package and the saddlepoint approximation to estimate parameters for the compound Poisson. Numerical problems can come from ill-conditioned variables and the data need to be scaled. One useful property of the compound Poisson is that the parameter $\lambda$ can absorb any scaling by a constant. That is if one can express a compound Poisson random variable as
\begin{equation}
X\sim\text{CompoundPoisson}\left(\nu\tau,\alpha,\lambda\right)
\end{equation}
then for a positive constant $k$
\begin{equation}
kX\sim\text{CompoundPoisson}\left(\nu\tau,\alpha,\lambda/k\right) \ .
\end{equation}

\bibliographystyle{unsrt}
\bibliography{../bib}

\end{document}
